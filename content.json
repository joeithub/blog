{"pages":[],"posts":[{"title":"Git合并特定commits到另一个分支","text":"Git合并特定commits 到另一个分支 ybdesire 2014-12-25 14:13:09 77825 收藏 7展开经常被问到如何从一个分支合并特定的commits到另一个分支。有时候你需要这样做，只合并你需要的那些commits，不需要的commits就不合并进去了。 合并某个分支上的单个commit首先，用git log或GitX工具查看一下你想选择哪些commits进行合并，例如： dd2e86 - 946992 -9143a9 - a6fd86 - 5a6057 [master] \\ 76cada - 62ecb3 - b886a0 [feature]比如，feature 分支上的commit 62ecb3 非常重要，它含有一个bug的修改，或其他人想访问的内容。无论什么原因，你现在只需要将62ecb3 合并到master，而不合并feature上的其他commits，所以我们用git cherry-pick命令来做： git checkout mastergit cherry-pick 62ecb3这样就好啦。现在62ecb3 就被合并到master分支，并在master中添加了commit（作为一个新的commit）。cherry-pick 和merge比较类似，如果git不能合并代码改动（比如遇到合并冲突），git需要你自己来解决冲突并手动添加commit。 合并某个分支上的一系列commits在一些特性情况下，合并单个commit并不够，你需要合并一系列相连的commits。这种情况下就不要选择cherry-pick了，rebase 更适合。还以上例为例，假设你需要合并feature分支的commit76cada ~62ecb3 到master分支。 首先需要基于feature创建一个新的分支，并指明新分支的最后一个commit： git checkout -b newbranch 62ecb3然后，rebase这个新分支的commit到master（–ontomaster）。76cada 指明你想从哪个特定的commit开始。 git rebase --onto master 76cada得到的结果就是feature分支的commit 76cada ~62ecb3 都被合并到了master分支。","link":"/2020/05/10/ Git合并/"},{"title":"Spring Boot——RabbitMQ","text":"Spring Boot——RabbitMQ [TOCM] [TOC] 使用配置1、老规矩，先在pom.xml中添加相关依赖：12345&lt;!--消息队列模块--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 2、在application.properties添加rabbitmq的相关信息：12345spring.application.name=spirng-boot-rabbitmqspring.rabbitmq.host=127.0.0.1spring.rabbitmq.port=5672spring.rabbitmq.username=guestspring.rabbitmq.password=guest 3、配置队列：12345678910111213141516import org.springframework.amqp.core.Queue;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration; /** * @Author: * @Description:队列配置，队列的名称，发送者和接受者的名称必须一致，否则接收不到消息 * @Date:2017/12/11 14:50 */@Configurationpublic class RabbitMqConfig { @Bean public Queue Queue1() { return new Queue(\"队列名\"); }} 4、发送者通过Controller类发送消息：123456789101112131415161718import org.springframework.amqp.core.AmqpTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController; import java.util.Date; @RestControllerpublic class SendController { @Autowired private AmqpTemplate amqpTemplate; @RequestMapping(\"/send\") public String send(){ String content=\"Date:\"+new Date(); amqpTemplate.convertAndSend(\"队列名\",content); return content; }} 5、创建接受者Receiver1，新建类：12345678910111213import org.springframework.amqp.rabbit.annotation.RabbitHandler;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component; @Component@RabbitListener(queues = \"队列名\")public class Receiver1 { @RabbitHandler public void receiver(String msg){ System.out.println(\"队列名 receiver1:\"+msg); }} 6、测试浏览器访问地址：http://localhost:8080/send，如下图： 查看RabbitMQ的Web客户端http://localhost:15672，需要自己安装RabbitMQ的客户端，可以自己上网查阅相关教程。帐号密码和配置文件一样，如下图：可以在列表里看到之前创建的队列。 一对多的使用配置1、一对多，一个发送者发送消息，多个接受者接受同一个消息，添加新的接收者Receiver2：12345678910111213import org.springframework.amqp.rabbit.annotation.RabbitHandler;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component; @Component@RabbitListener(queues = \"队列名\")public class Receiver2 { @RabbitHandler public void receiver(String msg){ System.out.println(\"队列名 receiver2:\"+msg); }} 2、发送者循环发送10个消息，在SendController添加一对多发送方法：12345678910@RequestMapping(\"/multiSend\") public String multiSend(){ StringBuilder times=new StringBuilder(); for(int i=0;i&lt;10;i++){ long time=System.nanoTime(); amqpTemplate.convertAndSend(\"队列名\",\"第\"+i+\"次发送的时间：\"+time); times.append(time+\"&lt;br&gt;\"); } return times.toString(); } 3、测试，浏览器访问http://localhost:8080/multiSend，如下图： 4、终端输出接收数据：12345678910Test1 receiver2:第1次发送的时间：25953655163399Test1 receiver1:第0次发送的时间：25953641137213Test1 receiver2:第2次发送的时间：25953655403734Test1 receiver1:第3次发送的时间：25953655591967Test1 receiver1:第5次发送的时间：25953655949458Test1 receiver2:第4次发送的时间：25953655772971Test1 receiver1:第6次发送的时间：25953656111790Test1 receiver1:第8次发送的时间：25953656492471Test1 receiver1:第9次发送的时间：25953656687330Test1 receiver2:第7次发送的时间：25953656277133 可以看到发送者发送一个消息被多个接收者接收，注意这里的消息只能被消费一次 多对多的使用配置1、在配置类RabbbitMqConfig添加新的队列名：1234567891011@Configurationpublic class RabbitMqConfig { @Bean public Queue Queue1() { return new Queue(\"队列名\"); } @Bean public Queue Queue2() { return new Queue(\"队列名2\"); }} 2、修改Receiver2接收队列名为队列名2： 123456789 @Component@RabbitListener(queues = \"队列名2\")//这里的lyhTest2是多对多，如果要测试一对多改成lyhTest1public class Receiver2 { @RabbitHandler public void receiver(String msg){ System.out.println(\"队列名2 receiver2:\"+msg); }} 3、在SendController添加多对多发送消息的方法：1234567891011@RequestMapping(\"/multi2MultiSend\") public String mutil2MutilSend(){ StringBuilder times=new StringBuilder(); for(int i=0;i&lt;10;i++){ long time=System.nanoTime(); amqpTemplate.convertAndSend(\"lyhTest1\",\"第\"+i+\"次发送的时间：\"+time); amqpTemplate.convertAndSend(\"lyhTest2\",\"第\"+i+\"次发送的时间：\"+time); times.append(time+\"&lt;br&gt;\"); } return times.toString(); } 4、测试，浏览器访问：http://localhost:8080/multi2MultiSend，如下图： 5、终端输出接收数据：1234567891011121314151617181920Test1 receiver1:第0次发送的时间：27607875773748Test2 receiver2:第0次发送的时间：27607875773748Test2 receiver2:第1次发送的时间：27607882272138Test2 receiver2:第2次发送的时间：27607882429049Test1 receiver1:第1次发送的时间：27607882272138Test2 receiver2:第3次发送的时间：27607882594693Test1 receiver1:第2次发送的时间：27607882429049Test2 receiver2:第4次发送的时间：27607882897371Test1 receiver1:第3次发送的时间：27607882594693Test2 receiver2:第5次发送的时间：27607883163005Test1 receiver1:第4次发送的时间：27607882897371Test2 receiver2:第6次发送的时间：27607883319916Test2 receiver2:第7次发送的时间：27607883489777Test1 receiver1:第5次发送的时间：27607883163005Test1 receiver1:第6次发送的时间：27607883319916Test2 receiver2:第8次发送的时间：27607883957798Test2 receiver2:第9次发送的时间：27607884305953Test1 receiver1:第7次发送的时间：27607883489777Test1 receiver1:第8次发送的时间：27607883957798Test1 receiver1:第9次发送的时间：27607884305953 可以看到不同的接收者接收不同发送者发送的消息，消息也可以是实体对象，这里就不做演示。 Topic Exchange的使用配置Topic Exchange是RabbitMQ中最灵活的一种方式，它能够根据routing_key自由的绑定不同的队列，可以适用绝大部分的项目需求 1、新建RabbitMqTopicConfig配置类：123456789101112131415161718192021222324252627282930313233343536373839404142434445import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.Queue;import org.springframework.amqp.core.TopicExchange;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration; /** * @Author:linyuanhuang * @Description:Topic Exchange配置类 * @Date:2017/12/11 17:13 */@Configurationpublic class { //只接一个topic final static String message = \"topic.message\"; //接收多个topic final static String messages = \"topic.messages\"; @Bean public Queue queueMessage() { return new Queue(RabbitMqTopicConfig.message); } @Bean public Queue queueMessages() { return new Queue(RabbitMqTopicConfig.messages); } @Bean TopicExchange exchange() { return new TopicExchange(\"exchange\"); } @Bean Binding bindingExchangeMessage(Queue queueMessage, TopicExchange exchange) { return BindingBuilder.bind(queueMessage).to(exchange).with(\"topic.message\"); } @Bean Binding bindingExchangeMessages(Queue queueMessages, TopicExchange exchange) { //这里的#表示零个或多个词。 return BindingBuilder.bind(queueMessages).to(exchange).with(\"topic.#\"); }} 2、在SendController添加发送消息方法：1234567891011121314@RequestMapping(\"/topicSend1\") public String topicSend1() { String context = \"my topic 1\"; System.out.println(\"发送者说 : \" + context); this.amqpTemplate.convertAndSend(\"exchange\", \"topic.message\", context); return context; } @RequestMapping(\"/topicSend2\") public String topicSend2() { String context = \"my topic 2\"; System.out.println(\"发送者说 : \" + context); this.amqpTemplate.convertAndSend(\"exchange\", \"topic.messages\", context); return context; } 3、创建接收者的方法TopicReceiver1和TopicReceiver2：TopicReceiver1： 123456789101112import org.springframework.amqp.rabbit.annotation.RabbitHandler;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;@Component@RabbitListener(queues = \"topic.message\")public class TopicReceiver1 { @RabbitHandler public void process(String msg) { System.out.println(\"TopicReceiver1:\" + msg); } } TopicReceiver2： 123456789101112import org.springframework.amqp.rabbit.annotation.RabbitHandler;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;@Component@RabbitListener(queues = \"topic.messages\")public class TopicReceiver2 { @RabbitHandler public void process(String msg) { System.out.println(\"TopicReceiver2 :\" + msg); } } 4、测试：浏览器访问http://localhost:8080/topicSend1，终端输出： 发送者说 : my topic 1TopicReceiver1:my topic 1TopicReceiver2 :my topic 1浏览器访问http://localhost:8080/topicSend2，终端输出： 发送者说 : my topic 2TopicReceiver2 :my topic 2 5、总结：这里的Topic Exchange 转发消息主要是根据通配符，队列topic.message只能匹配topic.message的路由。而topic.messages匹配路由规则是topic.#，所以它可以匹配topic.开头的全部路由。而topic.#发送的消息也只能是topic.#的接受者才能接收。","link":"/2019/12/06/ Spring Boot——RabbitMQ入门/"},{"title":"SQLAlert 语法","text":"RDL 语法RDL 脚本汲取了其他编程语言较好的语法格式，并进行了一定的修改，以达到相同的功能使用户输入最少的内容。 语句与注释RDL 脚本以语句为单位执行，每条执行语句必需以分号（;）结束。使用花括号（{}）来指定语句块，语句块可以不以分号（;）结束，但语句块内的每条语句都必需以分号（;）结束。例如下述代码片断，都是有效的语句： .cs .numberLines}1234567&gt; print(&quot;world&quot;); // 单独的语句&gt; &gt; { // 包含两条语句的语句块&gt; print(&quot;hello&quot;);&gt; print(&quot;world&quot;);&gt; }&gt; 在上述代码片断中，第 1 行与第 5 行为注释内容。在 RDL 脚本中可以用 # 和 // 来对内容进行注释，从 # 或 // 开始直到行尾均为注释内容，SQLAlert 在解释执行脚本时会忽略所有注释内容。在 RDL 脚本中不支持类 C 语言中的块级注释。 数据类型RDL 支持五种基本的数据类型：（1）数（包括整型与浮点）；（2）字符串；（3）数组（List）；（4）字典（Dict）；（5）布尔（true、false），熟悉 JSON 的读者应该知道这四种数据均为 JSON 支持的数据类型。如下代码片断所示，为四种数据类型的示例： .cs .numberLines}1234567891011&gt; // 数（整型与浮点）, 支持 10 进制整数、16 进制整数、10 进制浮点数。&gt; 123456 0xFF 0xFF 123.456&gt; // 字符串，支持单引号（&apos;）、双引号（&quot;）、反引号（｀）。&gt; &apos;string&apos; &quot;string&quot; `string`&gt; // 数组，元素可以是任意类型数据与表达式。&gt; [ 123, &apos;string&apos;, 100 + 200, {&quot;name&quot;: &quot;zhang&quot;}, [1, 2, 3] ]&gt; // 字典，KEY 只能为字符串；VALUE 可以是任意类型数据与表达式。&gt; { &quot;value&quot;: &quot;zhang&quot;, &quot;list&quot;: [ 1, 2, 3 ], &quot;dict&quot;: { &quot;vlaue&quot;: 1 } }&gt; // 布尔类型数据&gt; true false&gt; 其中字符串、List、Dict 类型的数据均可以分多行定义，但是字符串在多行定义时，换行符也是字符串内容的一部分。 变量RDL 脚本中的变量是无类型（或弱类型）的，不需要任何声明，可以直接赋值使用。将什么类型的数据赋值给变量，则变量就是什么类型。如下述代码片断所示： .cs .numberLines}12345&gt; a = 123; // 将整型赋值给变量 a， 则变量 a 即为整型；&gt; a = &quot;a string&quot;; // 将字符串赋值给变量 a，则变量 a 即为字符串；&gt; a = [ 1, 2, 3 ]; // 将 List 赋值给变量 a，则变量 a 即为 List；&gt; a = { &quot;value&quot;: [ 1, 2, 3 ] }; // 将 Dict 赋值给变量 a，则变量 a 即为 Dict；&gt; 如果一个变量在使用前没有赋值任何值，则这个变量的值为 null，表示空。数值 null 也可以单独作为一个值来使用，其地位与基本数据类型是一样的。 需要注意，变量名不能包含特殊字符及 RDL 脚本预置的所有符号。可以包含下划线与数字，但不能以数字开头。 字符串中的变量在定义字符串时，可以使用 %(name) 来引用当前执行环境中的变量值，相当于字符串的格式化。其中，name 为变量的名，如下述代码所示： .cs .numberLines}1234&gt; a = &quot;zhang&quot;;&gt; b = 123;&gt; print(&quot;a = %(a); b= %(b); c = %(c)&quot;);&gt; 执行该示例代码后，得到如下输出内容： .cs}12&gt; a = zhang; b= 123; c = null&gt; 其中，%(a) 被变量 a 的值（字符串 zhang）替换； %(b) 被变量 b 的值 （整数 123）替换； 对于 %(c)，由于变量 c 没有定义，被替换成 null。 表达式RDL 脚本支持五种算术表达式：加（+）、减（-）、乘（*）、除（/）、取余（%）；六种比较表达式：小于（&lt;）、小于等于（&lt;=）、大于（&gt;）、大于等于（&gt;=）、等于（==）、不等于（!=）；三种逻辑表达式：且（&amp;&amp;, and）、或（||, or）、否（!, not）。 表达式之间可能使用括号 () 进行无限嵌套，括号内的操作符优先级将高于括号外的操作符。如下代码片断给出了表达的部分示例： .cs .numberLines}123456&gt; a = ((1 + 5) * (4 - 2)) / 3;&gt; print(&quot;a =&quot;, a);&gt; &gt; b = (100 &gt;= 200) &amp;&amp; (1 &gt; 2);&gt; print(&quot;b =&quot;, b);&gt; 与其它编程语言一样，操作符 否（!, not）为单目操作符，其它均为双目操作符。除此之外，RDL 还支持一种三目操作符，… ? … : … 熟悉 C 或 java 的读者应该对这种三目操作符比较熟悉。下面给出该三目操作符的使用示例： .cs .numberLines}123&gt; name = 100 &gt; 200 ? &apos;zhang&apos; : &apos;wang&apos;;&gt; print(&quot;name =&quot;, name);&gt; 上述代码中，比较表达式 100 &gt; 200 的值为假（false）所以赋值语句将字符串 ‘wang’ 赋值给变量 name。 分支语句RDL 脚本支持 if expr { } else if expr { } else {} 分支，根据条件来执行不同的语句块，语句块必需包含在花括号内，花括号后面不需要以分号结束。条件表达式可以是任务类型的数据、表达式或者函数，可以包含在括号（）内，也可以不使用括号。 分支语句示例： .cs .numberLines}1234&gt; if a &gt; 200 { do_something(); }&gt; else if a &gt; 100 { do_something(); }&gt; else { do_something(); }&gt; 分支语句中，必需以 if 分支开始，中间可以有多个 else if 分支，也可以没有，结尾可以出现 else 分支，也可以没有，但最多只能有一个。 循环语句RDL 脚本中支持 for 循环语句，包括（1）for expr { } 和（2）for init; cond; next { } 两种形式，第一种形式类似于 C 中的 while 循环。 第（1）种 for 循环示例，打印出 0 ~ 9 的 10 个数字，代码如下： .cs .numberLines}123456&gt; a = 0;&gt; for (a &gt; 10) {&gt; print(&quot;a =&quot;, a);&gt; a ++;&gt; }&gt; 第（2）种 for 循环示例，同样打印出 0 ~ 9 的 10 个数字，代码如下： .cs .numberLines}1234&gt; for a = 0; a &lt; 10; a ++ {&gt; print(&quot;a =&quot;, a);&gt; }&gt; 循环中支持 continue 和 break 关键字，同时支持循环的嵌套。 函数定义RDL 脚本中通过关键字 def 来定义函数，示例如下： .cs .numberLines}12345&gt; def my_print(value) { // value 为函数的参数&gt; print(&quot;value =&quot;, value);&gt; }&gt; my_print(&quot;zhang&quot;); // 函数调用&gt; 函数定义中支持使用 return 关键字从函数中退出，同时可以指定函数的返回值。 文件包含RDL 脚本中可以通过 include 和 import 两个关键字来包含或者引入另一个 RDL 脚本文件。示例代码如下： （1） 第一个脚本文件 test1.rule： .cs .numberLines}1234&gt; def test_print(value) {&gt; print(&quot;value =&quot;, value);&gt; }&gt; （2）在第二个脚本文件中包含第一个脚本文件（test2.rule): .cs .numberLines}123&gt; include &quot;test1.rule&quot;;&gt; test_print(&quot;zhang&quot;); // 在脚本文件 test1.rule 中定义&gt; 上述代码示例中，在脚本 test2.rule 中调用了 test1.rule 中定义的函数 test_print()，在 test2.rule 中同时还可以引用 test1.rule 中定义的任何变量。 在 RDL 脚本中，可以在任意位置使用 include 和 import 来包含另一个脚本文件，二者的唯一区别是：通过 include 包含的脚本只会被执行一次，被引用的脚本只会在第一次 include 的时候执行；而通过 import 引入的脚本每次都会被执行，后面执行的结果将会覆盖前面的执行结果。另一个代码示例： .cs .numberLines}1234567891011121314151617&gt; // 脚本文件 test1.rule 内容&gt; name = &quot;zhang&quot;;&gt; age = 18;&gt; &gt; def print_name_age() {&gt; print(&quot;name =&quot;, name, &quot;age =&quot;, age);&gt; }&gt; &gt; &gt; // 脚本文件 test2.rule 内容&gt; name = &quot;wang&quot;;&gt; &gt; include &quot;test1.rule&quot;;&gt; &gt; age = 25;&gt; print_name_age();&gt; 请读者自行验证该示例的输出内容。","link":"/2019/06/22/2000-syntax/"},{"title":"SQLAlert 概述","text":"概述SQLAlert 是一个基于 ES（Elasticsearch）的异常检测与报警输出引擎，该引擎支持用户使用脚本定义报警检测规则。用户可以在脚本中使用 SQL 语句查询 ES，通过计算及过滤等得出报警数据，再将报警数据写回 ES 或者通过邮件输出。 本文档主要介绍 SQLAlert 支持的脚本语言 RDL（Rule Description Language）的语法，及其提供的库函数的使用方法。本文档需要读者对其他至少一门编程语言的语法有一定的了解，例如：C/C++/Java/JS/PHP 等，编程语言的基础知识不在本文档的描述范围之内。 说明SQLAlert 支持的 RDL 脚本是一种函数式脚本语言，不支持面向对象，其设计目的是为了弥补静态配置的不足。RDL 脚本在设计时充分考虑了应用场景，功能强大且语法简单、灵活。支持（1）JSON 数据类型；（2）算术与逻辑运算；（3）无类型的变量；（4）函数数定义；（5）相关库函数。 RDL 脚本库函数提供了常用计算模型，来制定复杂的报警规则，同时用户还可以通过函数定义，来实现更加复杂的规则。 执行脚本SQLAlert 使用 Go 语言开发，有非常高的执行效率。由于 RDL 脚本在解释执行时不进行编译，所以不建议在脚本中定义时间复杂度很高的算法或操作。RDL 的执行依赖其解释器 SQLAlert，请确保系统中已经存在。SQLAlert 的安装及使用，不在本文档的介绍范围内。SQLAlert 在解释执行 RDL 脚本文件时，约定其需以 .rule 结尾，下面通过 “Hello World” 示例来介绍 RDL 脚本是如何执行的： 12&gt; print(&quot;hello world&quot;);&gt; 将上述内容保存到以 .rule 结尾的文件（例如 hello.rule）中，使用如下命令来执行： 12&gt; sqlalert -t hello.rule&gt; 上述脚本代码中，通过 print() 函数来打印 “Hello World” 字符串，该语句以分号（;）结束。执行完该脚本后，会在终端上打印出 Hello World 信息。 RDL 脚本支持 UTF-8 编码，但其变量、操作符等均仅支持英文格式。用户可以使用 sqlalert -h/–help 查看 sqlalert 的更多选项。","link":"/2019/06/22/1000-overview/"},{"title":"SQLAlert 库函数","text":"RDL 库函数本章节将描述 SQLAlert 为 RDL 语言提供的库函数, 后续子章节将分类对其进行描述。","link":"/2019/06/22/3000-libs/"},{"title":"SQLAlert 基础函数","text":"基础函数本小节描述 RDL 函数库中的基础函数。 print(v, …)本函数接受任意多个参数，将给定的所有参数在同一行进行打印输出，打印时每个参数之间用空格分隔。对于 List 与 Dict 数据，按标准 JSON 进行格式化输出。 该函数代码示例如下： .cs .numberLines}12345&gt; num = 100;&gt; list = [ 1, &quot;string&quot; ];&gt; dict = { &quot;name&quot;: &quot;zhang&quot;, &quot;list&quot;: [ 1, 2, 3 ] };&gt; print(&quot;values =&quot;, num, list, dict);&gt; 示例代码的输出内容如下： .cs}12&gt; values = 100 [1,&quot;string&quot;] {&quot;list&quot;:[1,2,3],&quot;name&quot;:&quot;zhang&quot;}&gt; pprint(v, …)本函数为 print() 函数的美化版本，首字母 p 为 pretty 的简写，该函数将给定的参数按 JSON 格式进行缩进、分行格式化，然后再输出。在打印输出时，同样在多个参数之间以空格分隔。 该函数代码示例如下： .cs .numberLines}1234567&gt; list = [&gt; { &quot;name&quot;: &quot;zhang&quot;, &quot;age&quot;: 18, &quot;desc&quot;: &quot;A good man.&quot; },&gt; { &quot;name&quot;: &quot;wang&quot;, &quot;age&quot;: 22, &quot;desc&quot;: &quot;A good man too.&quot;}&gt; ];&gt;&gt; pprint(list);&gt; 示例代码的输出内容如下： .cs}12345678910111213&gt; [&gt; {&gt; &quot;name&quot;: &quot;zhang&quot;,&gt; &quot;age&quot;: 18,&gt; &quot;desc&quot;: &quot;A good man.&quot;&gt; },&gt; {&gt; &quot;name&quot;: &quot;wang&quot;,&gt; &quot;age&quot;: 22,&gt; &quot;desc&quot;: &quot;A good man too.&quot;&gt; }&gt; ]&gt; print_list(list)本函数对指定的 List 进行打印输出。对 List 的每一个元素按 JSON 进行格式化，并将每个元素在同一行打印输出。在 List 元素较多，且需要对元素进行比较时，该函数非常有用。该函数接受 1 个参数： list: 数组（List）类型，数据内的元素可以是任意类型。 本函数在打印 list 时，不输出起始符与结束符 []，如果 list 为空（即元素个数为零）时，将不输出任何内容。 该函数代码示例如下： .cs .numberLines}1234567&gt; list = [&gt; { &quot;name&quot;: &quot;zhang&quot;, &quot;age&quot;: 18, &quot;desc&quot;: &quot;A good man.&quot; },&gt; { &quot;name&quot;: &quot;wang&quot;, &quot;age&quot;: 22, &quot;desc&quot;: &quot;A good man too.&quot;}&gt; ];&gt;&gt; print_list(list);&gt; 示例代码的输出内容如下： .cs}123&gt; {&quot;age&quot;:18,&quot;desc&quot;:&quot;A good man.&quot;,&quot;name&quot;:&quot;zhang&quot;}&gt; {&quot;age&quot;:22,&quot;desc&quot;:&quot;A good man too.&quot;,&quot;name&quot;:&quot;wang&quot;}&gt; print_ctx() || print_context()本函数打印出脚本执行到当前位置时，执行上下文中的所有变量的值，函数不接受任何参数。print_context() 是该函数的完整函数名，print_ctx() 是函数的别名，二者在功能及使用上是一致的，本文档后续不再对这种格式进行解释。 该函数代码示例如下： .cs .numberLines}12345&gt; name = &quot;zhang&quot;;&gt; list = [ 1, 2, 3, 4, 5 ];&gt; &gt; print_ctx();&gt; 示例代码的输出内容如下： .cs}1234567&gt; Context-Global:&gt; __etc_scripts__ = /usr/local/etc&gt; name = zhang&gt; list = [1,2,3,4,5]&gt; &gt; Context-Local: empty&gt; 输出结果中，变量 _etc_scripts_ 为 SQLAlert 执行 RDL 脚本时，自动添加的全局变量，用于指定脚本执行目录（搜索目录）。更多说明请参阅本文档附录部分。 exit()该函数结束（终止）当前脚本的执行，该函数执行后，后面的所有语句将不再执行，函数不接受任何参数。 该函数代码示例如下： .cs .numberLines}12345&gt; print(&quot;hello&quot;);&gt; print(&quot;world&quot;);&gt; exit();&gt; print(&quot;rest&quot;);&gt; 示例代码的输出内容如下： .cs}123&gt; hello&gt; world&gt; 示例中，在 exit() 函数后面的语句 print(“rest”) 没有被执行。 error(msg)与 exit() 函数类似，该函数结束（终止）当前脚本的执行，不同的是该函数会在结束脚本执行时，输出错误信息，函数接受 1 个参数： msg: 任意数据类型，需要打印输出的错误信息内容。 该函数代码示例如下： .cs .numberLines}12345&gt; def test_error() {&gt; error(&quot;An error message&quot;);&gt; }&gt; test_error(); &gt; 示例代码的输出内容如下： 1234&gt; 2018-02-03 18:52:03 SQLAlert: [ERR] An error message&gt; in error(), in line 2 in file test.rule&gt; called by test_error(), in line 5 in file test.rule in task &apos;test&apos;&gt; 如示例中所示，该函数在输出错误信息的同时会输出函数的调用栈信息。 copy(v)本函数深度拷贝给定的对象，并返回新拷贝的对象。对新对象的任何修改不会影响到旧对象，该函数接收 1 个参数： v: 任意数据类型，需要拷贝的对象。 该函数代码示例如下： .cs .numberLines}1234567&gt; a = { &quot;name&quot;: &quot;zhang&quot;, &quot;age&quot;: 18 };&gt; b = copy(a);&gt; b[&quot;age&quot;] = 22;&gt; &gt; print(&quot;a =&quot;, a)&gt; print(&quot;b =&quot;, b)&gt; 示例代码的输出内容如下： .cs}123&gt; a = {&quot;age&quot;:18,&quot;name&quot;:&quot;zhang&quot;}&gt; b = {&quot;age&quot;:22,&quot;name&quot;:&quot;zhang&quot;}&gt; len(v)本函数用于计算给定对象的长度：（1）对于字符串，返回字符串的长度；（2）对于数组，返回数组元素的个数；（3）对于字典，返回字典 KEY 的个数；（4）对于其他类型数据，则返回 0。该函数接收 1 个参数： v: 任意数据类型，需要计算长度的对象。 该函数代码示例如下： .cs .numberLines}123456789&gt; a = &quot;zhang&quot;;&gt; print(&quot;len(str) =&quot;, len(a));&gt; &gt; a = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ];&gt; print(&quot;len(list) =&quot;, len(a));&gt; &gt; a = { &quot;name&quot;: &quot;zhang&quot;, &quot;age&quot;: 10, &quot;desc&quot;: &quot;a good man&quot; };&gt; print(&quot;len(dict) =&quot;, len(a));&gt; 示例代码的输出内容如下： .cs}1234&gt; len(str) = 5&gt; len(list) = 10&gt; len(dict) = 3&gt; get_ctx(name) || get_context(name)本函数返回当前执行上下文中的指定变量值，在搜索上下文时将忽略变量名的大小写。该函数接收 1 个参数： name: 字符串类型，需要获取的变量名。 该函数代码示例如下： .cs .numberLines}123&gt; aAcC = &quot;zhang&quot;;&gt; print(get_ctx(&quot;aAcC&quot;));&gt; 示例代码的输出内容如下： .cs}12&gt; zhang&gt; set_session(name, value)本函数在脚本的 session 中设置一个名为 name 值为 value 的变量，以记录脚本执行的状态。SQLAlert 在循环调度执行脚本时，会为每一次执行创建一个新的执行上下文，同时将上次执行的 session 保存到当前的执行上下文中。脚本可以在当前执行时访问上一次执行的状态，以实现特殊的需求。 需要注意的时，session 一旦被设置以后，将不会被自动回收，需要用户在脚本中显式的删除。通过 set_session(name, null) 设置某个名为 name 的 session 值为 null 即可将其使用的资源回收。在脚本中使用 session 功能需要特别小心，否则可能会产生内存泄漏的问题。 在脚本中可以将任意类型的数据保存到 session 中，该函数接收 2 个参数： name: 字符串类型，session 的名称。 value: 任意数据类型，session 的值。 该函数代码示例如下： .cs .numberLines}1234&gt; name = &quot;test&quot;;&gt; value = [ { &quot;name&quot;: &quot;zhang&quot; }, { &quot;name&quot;: &quot;wang&quot; } ];&gt; set_session(name, value);&gt; 该示例不会输出任何内容，只是在脚本的 session 中设置了一个名为 test 的变量。请阅读 get_session() 函数以了解如果获取脚本 session 的值。 get_session(name)本函数从脚本执行上下文的 session 中获取名为 name 的 session 值。该函数接收 1 个参数： name: 字符串类型，session 的名称。 该函数代码示例如下： .cs .numberLines}1234567&gt; name = &quot;test&quot;;&gt; value = [ { &quot;name&quot;: &quot;zhang&quot; }, { &quot;name&quot;: &quot;wang&quot; } ];&gt; set_session(name, value);&gt; &gt; mySession = get_session(&quot;test&quot;);&gt; print(&quot;my session =&quot;, mySession);&gt; 示例代码的输出内容如下： .cs}12&gt; my session = [{&quot;name&quot;:&quot;zhang&quot;},{&quot;name&quot;:&quot;wang&quot;}]&gt; 用户可以通过如下指令来让 SQLAlert 循环执行指定的脚本： .cs}12&gt; sqlalert --interval 5m --from &quot;2017-10-12 08:00:00&quot; --to &quot;2017-10-12 18:00:00&quot; -t test.rule&gt; 上述指令中，sqlalert 将循环执行指定的脚本 test.rule，并且修改脚本运行时的系统时间戳，时间戳从 2017-10-12 08:00:00 按每次递增 5m 的间隔进行变化，直到时间戳等于或超过 2017-10-12 18:00:00 停止。更多说明请参阅 SQLAlert 安装使用文档。 load_json(path)本函数将指定的文件以 JSON 格式加载到脚本上下文中，如果 path 为绝对路径，则直接加载；如果 path 为相对路径，则会在 path 之前添加 ETC 前缀。在使用 SQLAlert 执行 RDL 脚本时，可以通过选项 -e/–etc 来指定 ETC 路径，如果不指定，默认为 /usr/local/etc。函数接收 1 个参数： path: 字符串类型，文件路径名。 该函数代码示例如下： .cs .numberLines}123&gt; pprint(load_json(&quot;data.json&quot;));&gt; pprint(load_json(&quot;/home/&lt;user&gt;/sqlalert/data/data.json&quot;));&gt; 上述示例代码仅作为参考，无法执行，请用户换成自己的路径进行测试。","link":"/2019/06/22/3010-base/"},{"title":"SQLAlert 类型函数","text":"类型函数本小节描述 RDL 中的数据类型相关函数。 is_num(v)本函数检测给定的值是否是一个数（包括整数和浮点数），如果给定的参数是一个数，则返回 true，否则返回 false，该函数接收 1 个参数： v: 任意类型，需要检测的值或变量。 该函数代码示例如下： .cs .numberLines}123456789&gt; a = 123;&gt; print(a, &quot;is number:&quot;, is_num(a));&gt; &gt; a = 123.456;&gt; print(a, &quot;is number:&quot;, is_num(a));&gt; &gt; a = &quot;zhang&quot;;&gt; print(a, &quot;is number:&quot;, is_num(a));&gt; 示例代码的输出内容如下： .cs}1234&gt; 123 is number: true&gt; 123.456 is number: true&gt; zhang is number: false&gt; is_int(v)本函数检测给定的值是否是一个整数，如果给定的参数是一个整数，则返回 true，否则返回 false，该函数接收 1 个参数： v: 任意类型，需要检测的值或变量。 该函数代码示例如下： .cs .numberLines}123456789&gt; a = 123;&gt; print(a, &quot;is int:&quot;, is_int(a));&gt; &gt; a = 123.456;&gt; print(a, &quot;is int:&quot;, is_int(a));&gt; &gt; a = &quot;zhang&quot;;&gt; print(a, &quot;is int:&quot;, is_int(a));&gt; 示例代码的输出内容如下： .cs}1234&gt; 123 is int: true&gt; 123.456 is int: false&gt; zhang is int: false&gt; is_float(v)本函数检测给定的值是否是一个浮点数，如果给定的参数是一个浮点数，则返回 true，否则返回 false，该函数接收 1 个参数： v: 任意类型，需要检测的值或变量。 该函数代码示例如下： .cs .numberLines}123456789&gt; a = 123;&gt; print(a, &quot;is float:&quot;, is_float(a));&gt; &gt; a = 123.456;&gt; print(a, &quot;is float:&quot;, is_float(a));&gt; &gt; a = &quot;zhang&quot;;&gt; print(a, &quot;is float:&quot;, is_float(a));&gt; 示例代码的输出内容如下： .cs}1234&gt; 123 is float: false&gt; 123.456 is float: true&gt; zhang is float: false&gt; is_str(v)本函数检测给定的值是否是一个字符串，如果给定的参数是一个字符串，则返回 true，否则返回 false，该函数接收 1 个参数： v: 任意类型，需要检测的值或变量。 该函数代码示例如下： .cs .numberLines}123456789&gt; a = 123;&gt; print(a, &quot;is str:&quot;, is_str(a));&gt; &gt; a = 123.456;&gt; print(a, &quot;is str:&quot;, is_str(a));&gt; &gt; a = &quot;zhang&quot;;&gt; print(a, &quot;is str:&quot;, is_str(a));&gt; 示例代码的输出内容如下： .cs}1234&gt; 123 is str: false&gt; 123.456 is str: false&gt; zhang is str: true&gt; is_list(v)本函数检测给定的值是否是一个数组（List），如果给定的参数是一个数组，则返回 true，否则返回 false，该函数接收 1 个参数： v: 任意类型，需要检测的值或变量。 该函数代码示例如下： .cs .numberLines}123456789&gt; a = 123;&gt; print(a, &quot;is list:&quot;, is_list(a));&gt; &gt; a = [1, 2, 3 ];&gt; print(a, &quot;is list:&quot;, is_list(a));&gt; &gt; a = { &quot;name&quot;: &quot;zhang&quot; };&gt; print(a, &quot;is list:&quot;, is_list(a));&gt; 示例代码的输出内容如下： .cs}1234&gt; 123 is list: false&gt; [1,2,3] is list: true&gt; {&quot;name&quot;:&quot;zhang&quot;} is list: false&gt; is_dict(v)本函数检测给定的值是否是一个字典（Dict），如果给定的参数是一个字典，则返回 true，否则返回 false，该函数接收 1 个参数： v: 任意类型，需要检测的值或变量。 该函数代码示例如下： .cs .numberLines}123456789&gt; a = 123;&gt; print(a, &quot;is dict:&quot;, is_dict(a));&gt; &gt; a = [1, 2, 3 ];&gt; print(a, &quot;is dict:&quot;, is_dict(a));&gt; &gt; a = { &quot;name&quot;: &quot;zhang&quot; };&gt; print(a, &quot;is dict:&quot;, is_dict(a));&gt; 示例代码的输出内容如下： .cs}1234&gt; 123 is dict: false&gt; [1,2,3] is dict: false&gt; {&quot;name&quot;:&quot;zhang&quot;} is dict: true&gt; is_func(v)本函数检测是否存在指定的函数，如果指定的函数存在，则返回 true，否则返回 false，该函数接收 1 个参数： v: 字段串类型，需要检测的函数名。 该函数代码示例如下： .cs .numberLines}123&gt; print(&quot;print is func:&quot;, is_func(&quot;print&quot;));&gt; print(&quot;xxxxx is func:&quot;, is_func(&quot;xxxxx&quot;));&gt; 示例代码的输出内容如下： .cs}123&gt; print is func: true&gt; xxxxx is func: false&gt; is_null(v)本函数检测给定的值是否为 null（未赋值的变量即为 null）如指定的参数值为 null，则返回 true，否则返回 false，该函数接收 1 个参数： v: 任意类型，需要检测的值或变量。 该函数代码示例如下： .cs .numberLines}12345678&gt; print(&quot;null is null&quot;, is_null(null));&gt; &gt; a = null;&gt; print(&quot;a %(a) is null:&quot;, is_null(a));&gt; &gt; a = 0;&gt; print(&quot;a %(a) is null:&quot;, is_null(a));&gt; 示例代码的输出内容如下： .cs}1234&gt; null is null true&gt; a null is null: true&gt; a 0 is null: false&gt;","link":"/2019/06/22/3020-types/"},{"title":"SQLAlert 字符串函数","text":"字符串函数本小节介绍 RDL 中字符串相关函数。 str(v)本函数将指定的参数转化成字符串类型，返回转化后的字符串。该函数接收 1 个参数： v: 任意类型，需要转化值或变量。 该函数代码示例如下： .cs .numberLines}123&gt; print(str(1));&gt; print(str([1, 2, 3]));&gt; 示例代码的输出内容如下： .cs}123&gt; 1&gt; [1,2,3]&gt; split(str, sep)本函数将指定的字符串分割成一个字符串数组，该函数接收 2 个参数： str: 字符串类型，需要分割的字符串。 sep: 字符串类型「可选」，字符串分割符。如果该参数不指定，则以所有空白为分隔符，即将字符串分割为若干个“域”。 该函数代码示例如下： .cs .numberLines}123&gt; print(split(&quot;zhang Wang li han&quot;));&gt; print(split(&quot;zhang Wang li han&quot;, &quot; &quot;));&gt; 示例代码的输出内容如下： .cs}123&gt; [&quot;zhang&quot;,&quot;Wang&quot;,&quot;li&quot;,&quot;han&quot;]&gt; [&quot;zhang&quot;,&quot;Wang&quot;,&quot;&quot;,&quot;li&quot;,&quot;han&quot;]&gt; 请读者自行研究该示例两处 split() 函数用法的差异。 trim(str, sep)本函数去除指定字符串左右两边的空白，返回新字符串，原字符串保持不变。该函数接收 1 个参数： str: 字符串类型，需要去除空白的字符串。 该函数代码示例如下： .cs .numberLines}12&gt; print(trim(&quot; zhang &quot;));&gt; 示例代码的输出内容如下： .cs}12&gt; zhang&gt; fmt_int(num)本函数将指定的数值格按照整型进行格式化成字符串，在格式化浮点数时，将舍去所有小数部分。该函数接收 1 个参数： num: 整型或浮点型，需要格式化的数值。 该函数代码示例如下： .cs .numberLines}1234&gt; print(fmt_int(10));&gt; print(fmt_int(123.567));&gt; print(fmt_int(123.456));&gt; 示例代码的输出内容如下： .cs}1234&gt; 10&gt; 123&gt; 123&gt; fmt_float(float, n)本函数将指定的浮点数格式成字符串，保留指定位数的小数。舍去多余的小数位数时，按“四舍五入”进行进位操作。该函数接收 1 个参数： float: 浮点数，需要格式化的浮点数； n: 需要保留的小位个数。 该函数代码示例如下： .cs .numberLines}123&gt; print(fmt_float(123.567898, 0));&gt; print(fmt_float(123.456253, 2));&gt; 示例代码的输出内容如下： .cs}123&gt; 124&gt; 123.46&gt; fmt_bytes(int) || fmt_bits(int)本函数将指定的整数格式化成带字节（B）或比特单位（b）的字符串，格式化时按 1024 进行取余操作，并保留小数点后面 1 位。该函数接收 1 个参数： int: 整数，需要格式化的整数。 该函数代码示例如下： .cs .numberLines}123&gt; print(fmt_bytes(1234859));&gt; print(fmt_bits(190933388));&gt; 示例代码的输出内容如下： .cs}123&gt; 1.2 MB&gt; 182.1 Mb&gt; fmt_pct(num) || fmt_percentage(num)本函数将指定的整数或浮点数据格式化成百分比字符串，并保留小数点后 2 位。该函数接收 1 个参数： num: 整数或浮点数，需要格式化的整数。 该函数代码示例如下： .cs .numberLines}123&gt; print(fmt_pct(0.12303));&gt; print(fmt_pct(0.12344));&gt; 示例代码的输出内容如下： .cs}123&gt; 12.30%&gt; 12.34%&gt; fmt_time(int, type)本函数将指定的整数格式化带单位的时间间隔，并保留秒后面小数点 3 位，来表示毫秒。该函数接收 2 个参数： int: 整数，需要格式化的时间间隔。 type: 字符串类型「可选」，时间间隔类型 ms（毫秒） 或 us （微秒），如果不指定，则默认为 ms。 该函数代码示例如下： .cs .numberLines}1234&gt; print(fmt_time(1234567));&gt; print(fmt_time(1234567, &quot;ms&quot;));&gt; print(fmt_time(1234567, &quot;us&quot;));&gt; 示例代码的输出内容如下： .cs}1234&gt; 20m 34.567s&gt; 20m 34.567s&gt; 1.234s&gt;","link":"/2019/06/22/3030-strings/"},{"title":"SQLAlert 字典函数","text":"字典函数本小节介绍 RDL 中字典相关的操作函数。 keys(dict)本函数返回指定字典的所有 KEY 的值，该函数返回一个数组，包含所有的 KEY。由于字典是基于动态 hash 表实现的，所以每次获取的 KEY 数组元素的顺序是不一致的，但 KEY 数组的最量是一样的。函数接收 1 个参数： dict: 字典类型，指定的字典。 该函数代码示例如下： .cs .numberLines}123&gt; dict = { &quot;name&quot;: &quot;zhang&quot;, &quot;age&quot;: 18, &quot;desc&quot;: &quot;a good man&quot; };&gt; print(keys(dict));&gt; 示例代码的输出内容如下： .cs}12&gt; [&quot;name&quot;,&quot;age&quot;,&quot;desc&quot;]&gt; values(dict)本函数返回指定字典的所有 VALUE 的值，该函数返回一个数组，包含所有的 VALUE。由于字典是基于动态 hash 表实现的，所以每次获取的 VALUE 数组元素的顺序是不一致的，但 VALUE 数组的最量是一样的。函数接收 1 个参数： dict: 字典类型，指定的字典。 该函数代码示例如下： .cs .numberLines}123&gt; dict = { &quot;name&quot;: &quot;zhang&quot;, &quot;age&quot;: 18, &quot;desc&quot;: &quot;a good man&quot; };&gt; print(values(dict));&gt; 示例代码的输出内容如下： .cs}12&gt; [&quot;a good man&quot;,&quot;zhang&quot;,18]&gt; delete(dict, key)本函数从指定的字典中删除指定的 KEY，该函数不返回任何值。函数接收 2 个参数： dict: 字典类型，需要删除元素的字典； key: 字符串类型，需要删除的元素的 KEY。 该函数代码示例如下： .cs .numberLines}1234&gt; dict = { &quot;name&quot;: &quot;zhang&quot;, &quot;age&quot;: 18, &quot;desc&quot;: &quot;a good man&quot; };&gt; delete(dict, &quot;name&quot;);&gt; print(dict); &gt; 示例代码的输出内容如下： .cs}12&gt; {&quot;age&quot;:18,&quot;desc&quot;:&quot;a good man&quot;}&gt; dict_get(dict, key, def)本函数从指定的字典中读取指定 KEY 的值，如果 KEY 不存在则返回指定的默认值 def。函数接收 3 个参数： dict: 字典类型，指定的字典； key: 字符串类型，需要获取的元素的 KEY； def: 任意类型「可选」，KEY 不存在时返回的默认值，如果不指定，则默认值为 null。 该函数代码示例如下： .cs .numberLines}12345&gt; dict = { &quot;name&quot;: &quot;zhang&quot;, &quot;age&quot;: 18, &quot;desc&quot;: &quot;a good man&quot; };&gt; print(dict_get(dict, &quot;name&quot;));&gt; print(dict_get(dict, &quot;dummy&quot;));&gt; print(dict_get(dict, &quot;dummy&quot;, &quot;hello&quot;));&gt; 示例代码的输出内容如下： .cs}1234&gt; zhang&gt; null&gt; hello&gt; join_values(dict, keys, sep)本函数将字典中指定的 KEY 对应的值连接起来，生成一个字符串并返回。函数接收 3 个参数： dict: 字典类型，指定的字典； keys: 数组类型，数组的元素必需为字符串，需连接的 KEY 列表； sep: 字符串类型，连接时元素之间的分隔符。 该函数代码示例如下： .cs .numberLines}123&gt; dict = { &quot;name&quot;: &quot;zhang&quot;, &quot;age&quot;: 18, &quot;desc&quot;: &quot;a good man&quot; };&gt; print(join_values(dict, [&quot;name&quot;, &quot;desc&quot;], &quot; = &quot;));&gt; 示例代码的输出内容如下： .cs}12&gt; zhang = a good man&gt;","link":"/2019/06/22/3050-dict/"},{"title":"SQLAlert 数组函数","text":"数组函数本小节介绍 RDL 中数组相关的操作函数。 append(list, v, …)本函数将指定的值或变量添加到 list 的尾部，本函数接收 2 个以上参数： list: 数组类型，所以其他参数将被添加到该数组的尾部； v, …: 任意数据类型，n 个（n &gt;= 1）需要添加的值或变量； 该函数代码示例如下： .cs .numberLines}1234&gt; list = [];&gt; list = append(list, 1, 2);&gt; print(list);&gt; 示例代码的输出内容如下： .cs}12&gt; 2 [1,2]&gt; append_first(list, v, …)本函数将指定的值或变量添加到 list 的头部，函数接收 2 个以上参数： list: 数组类型，所以其他参数的值将被添加到该数组的头部； v, …: 任意数据类型，n 个（n &gt;= 1）需要添加的值或变量； 该函数代码示例如下： .cs .numberLines}1234&gt; list = [1, 2];&gt; list = append_first(list, 3);&gt; print(len(list), list);&gt; 示例代码的输出内容如下： .cs}12&gt; 3 [3,1,2]&gt; append_list(listDst, listSrc)本函数将数组 listSrc 中的所有元素添加到数组 listDst 内，函数接收 2 个参数： listDst: 数组类型，listSrc 数组中的所有元素都将被添加到该参数指定的数组中； listSrc: 数组类型，需要添加的数组； 该函数代码示例如下： .cs .numberLines}1234&gt; list = [1, 2];&gt; list = append_list(list, [3, 4]);&gt; print(len(list), list);&gt; 示例代码的输出内容如下： .cs}12&gt; 4 [1,2,3,4]&gt; remove_first(list)本函数删除指定数组的第一个元素，并返回元素删除后的数组。函数接收 1 个参数： list: 数组类型，需要删除元素的数组； 该函数代码示例如下： .cs .numberLines}1234&gt; list = [1, 2， 3， 4， 5];&gt; list = remove_first(list);&gt; print(len(list), list);&gt; 示例代码的输出内容如下： .cs}12&gt; 4 [2,3,4,5]&gt; remove_first(list)本函数删除指定数组的最后一个元素，并返回元素删除后的数组。函数接收 1 个参数： list: 数组类型，需要删除元素的数组； 该函数代码示例如下： .cs .numberLines}1234&gt; list = [1, 2， 3， 4， 5];&gt; list = remove_last(list);&gt; print(len(list), list);&gt; 示例代码的输出内容如下： .cs}12&gt; 4 [1,2,3,4]&gt; join(list, sep)本函数将数组中的所有元素连接成一个字符串，函数接收 2 个参数： list: 数组类型，需要连接的数组； sep: 字符串类型，元素与元素之间的分隔符。 该函数代码示例如下： .cs .numberLines}1234&gt; list = [1, 2, 3, 4, 5];&gt; print(join(list));&gt; print(join(list, &quot;_&quot;));&gt; 示例代码的输出内容如下： .cs}123&gt; 12345&gt; 1_2_3_4_5&gt; slice(list, from, to)本函数截取指定数组的一部分作为子数组（或切片），并将子数组返回给调用者，子数组与原数组共享元素的引用。对子数组的任何修改，将直接影响到原数组的数据，所以不建议在脚本中对截取的子数组做任何修改，除非在这样的需求。函数接收 3 个参数： list: 数组类型，需要截取的数组； from: 整型数据，截取元素的起始位置； to: 整型数据「可选」，截取元素的结束位置。如果该参数不指定则返回的子数组包括从 from 直到原数组的结束位置。 该函数代码示例如下： .cs .numberLines}12345678&gt; list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];&gt; print(slice(list, 3, 5));&gt; print(slice(list, 3));&gt; &gt; list2 = slice(list, 3, 5);&gt; list2[0] = 100;&gt; print(list);&gt; 示例代码的输出内容如下： .cs}1234&gt; [4,5]&gt; [4,5,6,7,8,9,10]&gt; [1,2,3,100,5,6,7,8,9,10]&gt; 如果想对截取出一个新的、与原数组完成不相干的子数据，请使用 copy() 函数。 sort(list)本函数对指定的数组进行升序（从小到大）排序，并返回排序后的新数组，原数组保持不变。函数接收 1 个参数： list: 数组类型，需要排序的数组； 该函数代码示例如下： .cs .numberLines}123&gt; list = [2, 4, 3, 10, 1, 6, 7, 8, 5, 9];&gt; print(sort(list));&gt; 示例代码的输出内容如下： .cs}12&gt; [1,2,3,4,5,6,7,8,9,10]&gt; sort_r(list)本函数对指定的数组进行降序（从大到小）排序，并返回排序后的新数组，原数组保持不变。函数接收 1 个参数： list: 数组类型，需要排序的数组； 该函数代码示例如下： .cs .numberLines}123&gt; list = [2, 4, 3, 10, 1, 6, 7, 8, 5, 9];&gt; print(sort_r(list));&gt; 示例代码的输出内容如下： .cs}12&gt; [10,9,8,7,6,5,4,3,2,1]&gt; reverse(list)本函数将指定的数组中的元素进行反转，并返回反转后的新数组，原数组保持不变。函数接收 1 个参数： list: 数组类型，需要反转的数组； 该函数代码示例如下： .cs .numberLines}123&gt; list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];&gt; print(reverse(list));&gt; 示例代码的输出内容如下： .cs}12&gt; [10,9,8,7,6,5,4,3,2,1]&gt; list_to_dict(list, keys, sep)本函数所处理的数组中，每个元素必需是一个字典类型数据。函数将数组中每个字典中指定的 keys 连接起来作为 KEY，对应的元素作为 VALUE 生成一个新的字典，并将新生成的字典返回给调用者。新字典中的 VALUE 为一个数组，数组的元素为 KEY 相同的原数组的所有元素。 函数接收 3 个参数： list: 数组类型，需要转换的数组； keys: 数组类型，数组的元素必需为字符串。需要连接的 KEY 列表； sep: 字符串类型「可选」，连接 KEY 时，每个字之间的分隔符，如果不指定则分隔符为空。 该函数代码示例如下： .cs .numberLines}12345678&gt; list = [&gt; { &quot;name&quot;: &quot;zhang&quot;, &quot;age&quot;: 18, &quot;desc&quot;: &quot;a good man&quot; },&gt; { &quot;name&quot;: &quot;zhang&quot;, &quot;age&quot;: 22, &quot;desc&quot;: &quot;a good man too&quot; },&gt; { &quot;name&quot;: &quot;wang&quot;, &quot;age&quot;: 25, &quot;desc&quot;: &quot;a good doctor&quot; },&gt; { &quot;name&quot;: &quot;wang&quot;, &quot;age&quot;: 30, &quot;desc&quot;: &quot;a good doctor too.&quot; }&gt; ];&gt; pprint(list_to_dict(list, [&quot;name&quot;]));&gt; 示例代码的输出内容如下： .cs}123456789101112131415161718192021222324252627&gt; {&gt; &quot;zhang&quot;: [&gt; {&gt; &quot;name&quot;: &quot;zhang&quot;,&gt; &quot;age&quot;: 18,&gt; &quot;desc&quot;: &quot;a good man&quot;&gt; },&gt; {&gt; &quot;age&quot;: 22,&gt; &quot;desc&quot;: &quot;a good man too&quot;,&gt; &quot;name&quot;: &quot;zhang&quot;&gt; }&gt; ],&gt; &quot;wang&quot;: [&gt; {&gt; &quot;name&quot;: &quot;wang&quot;,&gt; &quot;age&quot;: 25,&gt; &quot;desc&quot;: &quot;a good doctor&quot;&gt; },&gt; {&gt; &quot;name&quot;: &quot;wang&quot;,&gt; &quot;age&quot;: 30,&gt; &quot;desc&quot;: &quot;a good doctor too.&quot;&gt; }&gt; ]&gt; }&gt;","link":"/2019/06/22/3040-list/"},{"title":"SQLAlert 脚本执行函数","text":"脚本执行函数本小节介绍 RDL 中脚本执行相关函数。 call(name, args, …)本函数使用指定的参数来调用指定的函数，并返回被调函数的返回值。函数接收 n（n &gt;= 1）个参数： name: 字符串类型，被调用函数名。 args, …: 任意类型，name 后面的所有参数都将被直接传递给被调函数。 该函数代码示例如下： .cs .numberLines}123456&gt; def myPrint(value) {&gt; print(&quot;value =&quot;, value);&gt; }&gt; &gt; call(&quot;myPrint&quot;, &quot;hello world&quot;);&gt; 示例代码的输出内容如下： .cs}12&gt; value = hello world&gt; call_builtin(name, args, …)本函数使用指定的参数来调用指定的函数，并返回被调函数的返回值。该函数只搜索 RDL 库提供的函数，而不调用用户自定义的函数。函数接收 n（n &gt;= 1）个参数： name: 字符串类型，被调用函数名。 args, …: 任意类型，name 后面的所有参数都将被直接传递给被调函数。 该函数代码示例如下： .cs .numberLines}123456&gt; def myPrint(value) {&gt; print(&quot;value =&quot;, value);&gt; }&gt; &gt; call_builtin(&quot;myPrint&quot;, &quot;hello world&quot;);&gt; 示例代码的输出内容如下： .cs}123&gt; 2018-02-07 12:04:20 SQLAlert: [ERR] builtin &apos;myPrint&apos; not found&gt; in call_builtin(), in line 9 in file test.rule in task &apos;test&apos;&gt; call_list(list)本函数按顺序调用指定的函数列表，将函数的返回值传递给下一个函数，并返回最后一个函数的返回值。函数列表的配置如下所示： .cs}12345&gt; __funclist__ = [ &gt; { &quot;name&quot;: &quot;func1&quot;, &quot;args&quot;: { &quot;arg1&quot;: &quot;value1&quot; } },&gt; { &quot;name&quot;: &quot;func2&quot;, &quot;args&quot;: { &quot;arg2&quot;: &quot;value2&quot; } } &gt; ];&gt; 上述函数列表配置中，name 为函数名，args 为函数的参数，参数可以是任意类型，如果多个参数的话，一般使用字典来传递。被调用的函数必需定义为如下格式： .cs}12&gt; def name(result, args); // 第一个参数为上一函数的返回值；第二个参数为配置的参数。&gt; 函数 call_list() 接收 1 个参数： list: 数组类型，其元素必需为上述配置格式。 该函数代码示例如下： .cs .numberLines}123456789101112131415161718192021&gt; def print_zhang(result, args) {&gt; print(&quot;zhang result =&quot;, result);&gt; print(&quot;zhang args =&quot;, args);&gt; return &quot;zhang&quot;;&gt; }&gt; def print_wang(result, args) {&gt; print(&quot;wang result =&quot;, result);&gt; print(&quot;wang args =&quot;, args);&gt; }&gt; def print_liang(result, args) {&gt; print(&quot;liang result =&quot;, result);&gt; print(&quot;liang args =&quot;, args);&gt; return &quot;liang&quot;;&gt; }&gt; funclist = [&gt; { &quot;name&quot;: &quot;print_zhang&quot;, args: { &quot;value&quot;: &quot;hello&quot; } },&gt; { &quot;name&quot;: &quot;print_wang&quot;, args: { &quot;value&quot;: &quot;world&quot; } },&gt; { &quot;name&quot;: &quot;print_liang&quot; }&gt; ];&gt; print(call_list(funclist));&gt; 示例代码的输出内容如下： .cs}12345678&gt; zhang result = null&gt; zhang args = {&quot;value&quot;:&quot;hello&quot;}&gt; wang result = zhang&gt; wang args = {&quot;value&quot;:&quot;world&quot;}&gt; liang result = null&gt; liang args = null&gt; liang&gt; run(name)本函数执行指定的函数，并将返回值保存到执行上下文中，如果上下文中设置了全局变量 _sub_rules_ 则断续执行该变量所配置的子脚本。变量 _sub_rules_ 必需是一个数组，数组内的元素是字符串格式，每一个元素为一个脚本文件名。通过该函数可以实现脚本关联执行或者分级执行的功能。函数接收 1 个参数： name: 字符串类型，被调用函数名。 该函数在执行子脚本时，为每个脚本执行分配一个线程，即对所配置的子脚本并行执行，并等待所有脚本的返回。限于篇幅，该函数暂不列出代码示例。","link":"/2019/06/22/3070-exec/"},{"title":"SQLAlert 查询函数","text":"查询函数本小节介绍 RDL 中查询 ES 相关函数，RDL 提供的查询函数使用 SQL 查询 ES，本文档中的示例只是对 SQL 简单的使用，SQLalert 中对 SQL 的支持请参阅 SQLAlert SQL 用户手册。 query(sql, filter)本函数使用指定的 SQL 语句查询 ES，并返回查询结果。如果指定了过滤条件，则对查询的结果进行过滤，返回过滤后的结果。函数接收 2 个参数： sql: 字符串类型，SQL 查询语句。 filter: 字符串类型，查询结果过滤表达式。 在使用 query() 函数前，需要在脚本中定义 ES 服务器地址信息，该函数搜索 _es_host_ 和 _es_host_query_ 变量作为 ES 集群的地址，地址信息为字符串类型，且包含端口号。 该函数代码示例如下： .cs .numberLines}12345&gt; __es_host__ = &quot;192.168.0.122:9222&quot;;&gt; &gt; result = query(&quot;SELECT sip, sport, dip, sport FROM &apos;tcp-*&apos; LIMIT5&quot;);&gt; pprint(result);&gt; 示例代码的输出内容如下： .cs}12345678910111213141516171819202122232425262728&gt; [&gt; {&gt; &quot;sip&quot;: &quot;192.168.0.13&quot;,&gt; &quot;sport&quot;: 56167,&gt; &quot;dip&quot;: &quot;192.30.253.124&quot;&gt; },&gt; {&gt; &quot;sip&quot;: &quot;192.168.0.17&quot;,&gt; &quot;sport&quot;: 59312,&gt; &quot;dip&quot;: &quot;180.149.132.165&quot;&gt; },&gt; {&gt; &quot;sip&quot;: &quot;192.168.0.17&quot;,&gt; &quot;sport&quot;: 59206,&gt; &quot;dip&quot;: &quot;106.39.162.37&quot;&gt; },&gt; {&gt; &quot;sip&quot;: &quot;192.168.0.12&quot;,&gt; &quot;sport&quot;: 65223,&gt; &quot;dip&quot;: &quot;64.233.189.139&quot;&gt; },&gt; {&gt; &quot;sip&quot;: &quot;192.168.0.13&quot;,&gt; &quot;sport&quot;: 56380,&gt; &quot;dip&quot;: &quot;74.125.204.113&quot;&gt; }&gt; ]&gt; 为方便阅读，后续示例将使用 print_list() 函数打印查询结果，上述示例代码的结果使用 print_list() 函数打印出来如下： .cs}123456&gt; {&quot;dip&quot;:&quot;192.30.253.124&quot;,&quot;sip&quot;:&quot;192.168.0.13&quot;,&quot;sport&quot;:56167}&gt; {&quot;dip&quot;:&quot;180.149.132.165&quot;,&quot;sip&quot;:&quot;192.168.0.17&quot;,&quot;sport&quot;:59312}&gt; {&quot;dip&quot;:&quot;106.39.162.37&quot;,&quot;sip&quot;:&quot;192.168.0.17&quot;,&quot;sport&quot;:59206}&gt; {&quot;dip&quot;:&quot;64.233.189.139&quot;,&quot;sip&quot;:&quot;192.168.0.12&quot;,&quot;sport&quot;:65223}&gt; {&quot;dip&quot;:&quot;74.125.204.113&quot;,&quot;sip&quot;:&quot;192.168.0.13&quot;,&quot;sport&quot;:56380}&gt; query() 函数支持对查询结果的二次过滤，通过参数 filter 将 RDL 支持表达式传递给该函数即可，示例代码如下： .cs .numberLines}123456&gt; __es_host__ = &quot;192.168.0.122:9222&quot;;&gt; &gt; sql = &quot;SELECT sip, sport, dip, sport FROM &apos;tcp-*&apos; LIMIT 5&quot;;&gt; result = query(sql, &quot;sip == &apos;192.168.0.13&apos;&quot;);&gt; print_list(result);&gt; 示例代码的输出内容如下： .cs}123&gt; {&quot;dip&quot;:&quot;192.30.253.124&quot;,&quot;sip&quot;:&quot;192.168.0.13&quot;,&quot;sport&quot;:56167}&gt; {&quot;dip&quot;:&quot;74.125.204.113&quot;,&quot;sip&quot;:&quot;192.168.0.13&quot;,&quot;sport&quot;:56380}&gt; 需要注意，过滤表达式中，支持 RDL 所有支持的表达式，但不支持函数调用。 query_avgby_num(sql, num, filter)本函数使用指定的 SQL 语句查询 ES，并返回查询结果。如果指定了过滤条件，则对查询的结果进行过滤，返回过滤后的结果。该函数会将查询结果中所有的数值型字段，除以指定的数，然后再返回处理后的结果。其他方面与 query() 函数是一致的，query_avgby_num() 函数接收 3 个参数： sql: 字符串类型，SQL 查询语句。 num: 数值类型，除数。 filter: 字符串类型，查询结果过滤表达式。 请用户参阅 query() 函数示例，自行验证本函数。 query_avgby_field(sql, name, filter)本函数使用指定的 SQL 语句查询 ES，并返回查询结果。如果指定了过滤条件，则对查询的结果进行过滤，返回过滤后的结果。该函数会将查询结果中所有的数值型字段，除以指定的字段值，然后再返回处理后的结果，指定的字段必需为数值。其他方面与 query() 函数是一致的，query_avgby_field() 函数接收 3 个参数： sql: 字符串类型，SQL 查询语句。 name: 字符串类型，除数字段名。 filter: 字符串类型，查询结果过滤表达式。 请用户参阅 query() 函数示例，自行验证本函数。","link":"/2019/06/22/3080-query/"},{"title":"SQLAlert 报警输出函数","text":"报警输出函数本小节介绍 RDL 中报警输出函数。 alert_es(list)本函数将指定数组中的数据写入到 ES 的索引中。脚本中通过全局变量 _es_host_ 和 _es_host_insert_ 来指定 ES 服务器的配置，通过全局变量 _alert_index_ 指定输出的索引信息，该函数在搜索 ES 服务器配置时，优先使用变量 _es_host_insert_ 的值，如果该变量不存在，再使用 _es_host_ 的值，配置项如下： .cs .numberLines}123&gt; __es_host__ = &quot;localhost:9200&quot;;&gt; __es_index_alert__ = { &quot;index&quot;: &quot;alert-%Y-%M-%D&quot;, &quot;type&quot;: &quot;your type&quot; };&gt; 输出索引配置中，index 表示索引名，可以使用 %Y、%M、%D 按 年、月、日 自动生成索引，type 为 ES 索引的数据类型。ES 索引相关内容请参阅 ES 官方相关文档。 函数 alert_es() 接收 1 个参数： list: 数组类型，数组中每个元素均为字典类型数据。需要导出的数据。 alert_email(list)alert(list)","link":"/2019/06/22/3090-alert/"},{"title":"SQLAlert 时间函数","text":"时间函数本小节介绍 RDL 中字典相关的操作函数。 time(sep)本函数获取当前时间戳，并格式化成字符串。如果脚本中配置了全局变量 _now_，则该函数返回 __now\\ 所表示的时间戳。函数接收 1 个参数： sep: 字符串类型，日期与时间之间的分隔符。 该函数代码示例如下： .cs .numberLines}123456&gt; print(time());&gt; print(time(&quot;T&quot;));&gt; &gt; __now__ = &quot;2015-03-02 08:10:00.234&quot;;&gt; print(time());&gt; 示例代码的输出内容如下： .cs}1234&gt; 2018-02-07 11:04:25.824+08:00&gt; 2018-02-07T11:04:25.824+08:00&gt; 2015-03-02 08:10:00.234+08:00&gt; sys_time(sep)本函数获取当前时间戳，并格式化成字符串。函数返回的数值不受全局变量 _now_ 的影响。函数接收 1 个参数： sep: 字符串类型，日期与时间之间的分隔符。 该函数代码示例如下： .cs .numberLines}1234567&gt; print(time());&gt; print(sys_time());&gt; &gt; __now__ = &quot;2015-03-02 08:10:00.234&quot;;&gt; print(time());&gt; print(sys_time());&gt; 示例代码的输出内容如下： .cs}12345&gt; 2018-02-07 11:08:27.865+08:00&gt; 2018-02-07 11:08:27.865+08:00&gt; 2015-03-02 08:10:00.234+08:00&gt; 2018-02-07 11:08:27.865+08:00&gt; now(sep)本函数获取当前时间戳整数值，精确到毫秒。如果脚本中配置了全局变量 _now_，则该函数返回 __now\\ 所表示的时间戳，函数不接收任何个参数。 该函数代码示例如下： .cs .numberLines}12345&gt; print(now());&gt; &gt; __now__ = &quot;2015-03-02 08:10:00.234&quot;;&gt; print(now());&gt; 示例代码的输出内容如下： .cs}123&gt; 1517972720018&gt; 1425255000234&gt; sys_now(sep)本函数获取当前时间戳整数值，精确到毫秒。函数返回的数值不受全局变量 _now_ 的影响，函数不接收任何个参数。 该函数代码示例如下： .cs .numberLines}1234567&gt; print(now());&gt; print(sys_now());&gt; &gt; __now__ = &quot;2015-03-02 08:10:00.234&quot;;&gt; print(now());&gt; print(sys_now());&gt; 示例代码的输出内容如下： .cs}12345&gt; 1517972986212&gt; 1517972986213&gt; 1425255000234&gt; 1517972986213&gt; check_datetime(list)本函数检测当前日期和时间是否被配置成 on 状态，如果是则返回 true，否则返回 false。如果日期及时间未配置，则返回 true 表示默认为 on 状态。与 on 状态相反的状态为 off 状态，可以通过如下格式对日期和时间进行配置： .cs .numberLines}123456&gt; [&gt; { &quot;type&quot;: &quot;on&quot;, &quot;months&quot;: &quot;1 TO 12&quot;, &quot;week_days&quot;: &quot;1 to 5&quot;, &quot;hours&quot;: &quot;9 to 18&quot; },&gt; { &quot;type&quot;: &quot;off&quot;, &quot;months&quot;: 10, &quot;month_days&quot;: 1 },&gt; { &quot;type&quot;: &quot;off&quot;, &quot;datetime&quot;: &quot;2017-11-02 09&quot; }&gt; ]&gt; 日期和时间配置说明： 数组中每一项为一个配置，通过 type 域指定功能 on 或者 off； 支持对 months, month_days, week_days, hours, minutes 进行配置，如果不配置则不进行检测； 支持使用 datetime 来配置具体的时间段； 所有配置支持三种格式：（1）单个整数；（2）整数的数组；（3）字符串 “m to n” 的格式，其中 m 和 n 为整数，mounths 的范围为 1 到 12，month_days 的范围为 1 到 31，week_days 的范围为 1 到 7， hours 的范围从 0 到 23， minutes 的范围从 0 到 59； 对于第一个配置项，如果 type 为 on 则其中不在当前配置项中的其他时间段均被置成 off，后续所有配置只影响对应的配置时间段，不会影响到其他时间段； 可以重复地 on 或者 off 某时间段，以最后一个配置项作为最终结果。 函数接收 1 个参数： list: 数组类型，list 内的元素必需为上述日期和时间配置格式。如果不指定配置，默认搜索全局变量 _workdays_ 作为配置。 该函数代码示例如下： .cs .numberLines}12345678910111213&gt; list = [&gt; { &quot;type&quot;: &quot;on&quot;, &quot;months&quot;: &quot;1 TO 12&quot;, &quot;week_days&quot;: &quot;1 to 5&quot;, &quot;hours&quot;: &quot;9 to 18&quot; },&gt; { &quot;type&quot;: &quot;off&quot;, &quot;months&quot;: 10, &quot;month_days&quot;: 1 },&gt; { &quot;type&quot;: &quot;off&quot;, &quot;datetime&quot;: &quot;2017-11-02 09&quot; }&gt; ];&gt; print(time(), &quot;is on ?&quot;, check_datetime(list));&gt; &gt; __now__ = &quot;2017-11-02 09:00:00&quot;;&gt; print(time(), &quot;is on ?&quot;, check_datetime(list));&gt; &gt; __now__ = &quot;2017-10-01 08:00:00&quot;;&gt; print(time(), &quot;is on ?&quot;, check_datetime(list));&gt; 示例代码的输出内容如下： .cs}1234&gt; 2018-02-07 11:40:21.919+08:00 is on ? true&gt; 2017-11-02 09:00:00+08:00 is on ? false&gt; 2017-10-01 08:00:00+08:00 is on ? false&gt; 需要注意，如果没有配置日期与时间或者配置的时期与时间非法，则该函数返回 true 表示默认为 on 状态。","link":"/2019/06/22/3060-time/"},{"title":"SQLAlert 脚本语言参考手册","text":"","link":"/2019/06/22/0000-home/"},{"title":"注解@ConfigurationProperties","text":"@ConfigurationProperties 在Idea中冒红问题 冒红不影响正常的运行，如想结果按照以下步骤。 pom.xml增加如下依赖12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233@Configuration@EnableConfigurationProperties({RedisConfig.class}) @ConfigurationProperties(prefix = \"custom.redis\")public class RedisConfig { private String host; private int port; private int maxIdle; private int maxActive; public String getHost() { return host; } public void setHost(String host) { this.host = host; } public int getPort() { return port; } public void setPort(int port) { this.port = port; } public int getMaxIdle() { return maxIdle; } public void setMaxIdle(int maxIdle) { this.maxIdle = maxIdle; } public int getMaxActive() { return maxActive; } public void setMaxActive(int maxActive) { this.maxActive = maxActive; } } 主要是增加@EnableConfigurationProperties({RedisConfig.class})","link":"/2020/04/14/@ConfigurationProperties/"},{"title":"文档标题","text":"使用@Select注解时使用in传入ids数组作为参数 展开最近用到Mybatis的注解sql方式，结果发现在传入多个id作为参数跟xml的用法不太一样，到网上搜罗了一些方法，很多都会报错，最后如下方法调通了，重点是script标签，和参数的名字： 123456789101112131415161718@Select({ \"&lt;script&gt;\", \"select\", \"c.cust_id, plat_cust_id, plat_cust_name, cust_map_id, cust_type, is_merchant, \", \"create_time, create_opt_id, p.cust_name, p.tel, p.id_type, p.id_no \", \"from personal_customer p\", \"left join customer c\", \"on c.cust_id = p.cust_id\", \"where p.cust_id in \", \"&lt;foreach item='item' index='index' collection='ids'\", \"open='(' separator=',' close=')'&gt;\", \"#{item}\", \"&lt;/foreach&gt;\", \"&lt;/script&gt;\" }) @ResultMap(\"com.cloud.customer.dao.PersonalCustomerMapper.PersonDOResultMap\") List&lt;PersonInfoDO&gt; selectRecordsByIds(@Param(\"ids\") String[] ids);} 在sql语句两头用script标签包起来，然后中间用foreach正常编辑，参数注入一个String数组，就可以完成查询了","link":"/2020/05/29/@Select注解时使用in/"},{"title":"@AutoConfigureAfter","text":"@AutoConfigureAfter 在加载配置的类之后再加载当前类它的value 是一个数组 一般配合着@import 注解使用 ，在使用import时必须要让这个类先被spring ioc 加载好所以@AutoConfigureAfter必不可少 123456789101112131415161718192021222324252627282930@Configurationpublic class ClassA { //在加载DemoConfig之前加载ClassA类}@Configuration@AutoConfigureAfter(ClassA.class)@Import(ClassA.class)public class DemoConfig {}@Retention(RetentionPolicy.RUNTIME)@Target({ ElementType.TYPE })public @interface AutoConfigureAfter { /** * The auto-configure classes that should have already been applied. * @return the classes */ Class&lt;?&gt;[] value() default {}; /** * The names of the auto-configure classes that should have already been applied. * @return the class names * @since 1.2.2 */ String[] name() default {};} 注意： spring只对spring.factory文件下的配置类进行排序","link":"/2019/08/29/AutoConfigureAfter/"},{"title":"Autowired(Value)注入与@PostConstruct调用顺序","text":"最近在项目开发中遇到这样一个需求，由于元数据在短时间内被客户端多次读取，因此希望直接将数据存储到内存，以减少网络开销，借助guava cache于是有了下面这个类 123456789101112131415161718192021222324252627282930/** * Created on 2018/10/18 */@Componentpublic class CacheUtil { @Autowired CaseGraphService caseGraphService; @Value(\"${cache.expire.duration}\") long expireDuration; private Cache&lt;Long, CaseGraphDTO&gt; metaNodeCache = CacheBuilder .newBuilder() .maximumSize(1000) .expireAfterAccess(expireDuration, TimeUnit.MINUTES) //设置过期时间 .build(); public CaseGraphDTO getMetaNode(long caseId) throws ExecutionException { return metaNodeCache.get(caseId, () -&gt; caseGraphService.getCaseGraph(caseId)); } public void removeMetaNodeByKey(long caseId){ metaNodeCache.invalidate(caseId); } public void removeMetaNodeAll(){ metaNodeCache.invalidateAll(); }} 我们在另一个类中注入CacheUtil并调用它的getMetaNode方法，类似于这样： 123456789@Componentpublic class TestComponent { @Autowired CacheUtil cacheUtil; public CaseGraphDTO getMetaData(long caseId){ return cacheUtil.getMetaNode(caseId); }} 当我们第一次调用getMetaNode时，cache使用caseGraphService获取元数据，而后将这一元数据存放的cache中，我们在CacheUtil的getMetaNode方法中添加两行代码来测试一下： 123456789public CaseGraphDTO getMetaNode(long caseId) throws ExecutionException { //metaNodeCache的get方法，如果缓存中已有数据，直接返回数据，如果没有则通过Callable方法获取存入缓存再返回数据 CaseGraphDTO caseGraphDTO = metaNodeCache.get(caseId, () -&gt; caseGraphService.getCaseGraph (caseId)); //getIfPresent方法，如果存在数据即返回 CaseGraphDTO caseGraphDTO1 = metaNodeCache.getIfPresent(caseId); System.out.println(caseGraphDTO1); return caseGraphDTO; } 启动application， 打开断点调试： 可以看到caseGraphDTO1为null，而不是我们预想的元数据；metaNodeCache中localCache的大小为0，也就是根本没有缓存任何数据。 而后我采用另一种方式@PostConstruct的方式来初始化metaNodeCache, 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738@Componentpublic class CacheUtil { @Autowired CaseGraphService caseGraphService; @Value(\"${cache.expire.duration}\") long expireDuration; private Cache&lt;Long, CaseGraphDTO&gt; metaNodeCache; @PostConstruct private void init(){ metaNodeCache = CacheBuilder .newBuilder() .maximumSize(1000) .expireAfterAccess(expireDuration, TimeUnit.MINUTES) .build(); } public CaseGraphDTO getMetaNode(long caseId) throws ExecutionException { //metaNodeCache的get方法，如果缓存中已有数据，直接返回数据，如果没有则通过Callable方法获取存入缓存再返回数据 CaseGraphDTO caseGraphDTO = metaNodeCache.get(caseId, () -&gt; caseGraphService.getCaseGraph (caseId)); //getIfPresent方法，如果存在数据即返回 CaseGraphDTO caseGraphDTO1 = metaNodeCache.getIfPresent(caseId); System.out.println(caseGraphDTO1); return caseGraphDTO; } public void removeMetaNodeByKey(long caseId){ metaNodeCache.invalidate(caseId); } public void removeMetaNodeAll(){ metaNodeCache.invalidateAll(); }} 再进行调试： 这时我们看到caseGraphDTO1中有了数据，并且metaNodeCache中localCache的大小也变成了1. 那么直接初始化成员变量和使用postConstruct来初始化二者的区别是什么呢？ 要将对象p注入到对象a，那么首先就必须得生成对象p与对象a，才能执行注入。所以，如果一个类A中有个成员变量p被@Autowired注解，那么@Autowired注入是发生在A的构造方法执行完之后的。如果想在生成对象时候完成某些初始化操作，而偏偏这些初始化操作又依赖于依赖注入，那么就无法在构造函数中实现。为此，可以使用@PostConstruct注解一个方法来完成初始化，@PostConstruct注解的方法将会在依赖注入完成后被自动调用。 事情似乎明朗起来了，虽然我们在初始化metaNodeCache时没有使用autowired进来的caseGraphService，但我们使用了@Value来注入缓存过期时间（配置中这个值为60）。让我们再来断点调试一下，当使用直接初始化成员变量的方式时，@Value(“${cache.expire.duration}”)注入的过期时间是多少，如下： 我们看到expireDuration的值为0，而不是配置中的60，对于cache的含义即为缓存中的数据被读取后0分钟使其失效，等同于立即失效。所以导致上文我们的缓存中始终没有数据。 让我们再来看看使用PostConstruct初始化时，这个过期时间的值： 此时看到expireDuration的值为60. 因此我们需要记住，构造函数，Autowired(Value)，PostConstruct的执行顺序为： Constructor &gt;&gt; Autowired &gt;&gt; PostConstruct如果初始化成员变量需要使用注入进来的对象或者值，那么应该放在被PostConstruct注解的方法中去做","link":"/2019/09/05/Autowired-Value-注入与-PostConstruct调用顺序/"},{"title":"@Bean","text":"spring @Bean注解的使用@Bean 的用法@Bean是一个方法级别上的注解，主要用在@Configuration注解的类里，也可以用在@Component注解的类里。添加的bean的id为方法名 1234567@Configurationpublic class AppConfig { @Bean public TransferService transferService() { return new TransferServiceImpl(); } 这个配置就等同于之前在xml里的配置 1234&lt;beans&gt; &lt;bean id=\"transferService\" class=\"com.acme.TransferServiceImpl\"/&gt; &lt;/beans&gt; bean的依赖@bean 也可以依赖其他任意数量的bean，如果TransferService 依赖 AccountRepository，我们可以通过方法参数实现这个依赖 123456789@Configurationpublic class AppConfig { @Bean public TransferService transferService(AccountRepository accountRepository) { return new TransferServiceImpl(accountRepository); } } 接受生命周期的回调 任何使用@Bean定义的bean，也可以执行生命周期的回调函数，类似@PostConstruct and @PreDestroy的方法。用法如下 1234567891011121314151617181920212223242526public class Foo { public void init() { // initialization logic }} public class Bar { public void cleanup() { // destruction logic }} @Configurationpublic class AppConfig { @Bean(initMethod = \"init\") public Foo foo() { return new Foo(); } @Bean(destroyMethod = \"cleanup\") public Bar bar() { return new Bar(); } } 默认使用javaConfig配置的bean，如果存在close或者shutdown方法，则在bean销毁时会自动执行该方法，如果你不想执行该方法，则添加@Bean(destroyMethod=””)来防止出发销毁方法 指定bean的scope使用@Scope注解你能够使用@Scope注解来指定使用@Bean定义的bean 12345678@Configurationpublic class MyConfiguration { @Bean @Scope(\"prototype\") public Encryptor encryptor() { // ... } @Scope and scoped-proxyspring提供了scope的代理，可以设置@Scope的属性proxyMode来指定，默认是ScopedProxyMode.NO， 你可以指定为默认是ScopedProxyMode.INTERFACES或者默认是ScopedProxyMode.TARGET_CLASS。以下是一个demo，好像用到了（没看懂这块） 12345678910111213// an HTTP Session-scoped bean exposed as a proxy@Bean@SessionScopepublic UserPreferences userPreferences() { return new UserPreferences();} @Beanpublic Service userService() { UserService service = new SimpleUserService(); // a reference to the proxied userPreferences bean service.setUserPreferences(userPreferences()); return service; 自定义bean的命名默认情况下bean的名称和方法名称相同，你也可以使用name属性来指定 12345678@Configurationpublic class AppConfig { @Bean(name = \"myFoo\") public Foo foo() { return new Foo(); } bean的别名bean的命名支持别名，使用方法如下 1234567@Configurationpublic class AppConfig { @Bean(name = { \"dataSource\", \"subsystemA-dataSource\", \"subsystemB-dataSource\" }) public DataSource dataSource() { // instantiate, configure and return DataSource bean... } bean的描述有时候提供bean的详细信息也是很有用的，bean的描述可以使用 @Description来提供 12345678@Configurationpublic class AppConfig { @Bean @Description(\"Provides a basic example of a bean\") public Foo foo() { return new Foo(); }","link":"/2019/08/29/Bean/"},{"title":"@Builder","text":"lombok的@Builder实际是建造者模式的一个变种，所以在创建复杂对象时常使用 12345678910import lombok.Builder;import lombok.Data;@Data@Builderpublic class People { private String name; private String sex; private int age;} 使用了@Bulider和@Data注解后，就可以使用链式风格优雅地创建对象 1234567891011121314151617181920public class TestLombok { @Test public void testBuilderAnnotation(){ People luoTianyan = People.builder() .sex(\"female\") .age(23) .name(\"LuoTianyan\") .build(); System.out.println(luoTianyan.toString()); //People(name=LuoTianyan, sex=female, age=23) People people = new People(\"LuoTianyan\",\"female\",23); System.out.println(luoTianyan.equals(people)); //true }} class People加上了@Builder和@Data注解后，多了一个静态内部类PeopleBuilder，People调用静态方法builder生成PeopleBuilder对象，PeopleBuilder对象可以使用”.属性名(属性值)”的方式进行属性设置，再调用build()方法就生成了People对象，并且如果两个People对象的属性如果相同，就会认为这两个对象相等，即重写了hashCode和equls方法。 可以利用Javap、cfr进行反编译该字节码； 这里就直接在Intellij IDEA下，查看反编译的文件People.class； 可以看到，生成的有： Getter和Setter方法；访问类型是private无参构造方法，访问类型为default的全部参数的构造方法；重写hashCode、equals、toString方法，则People可以做为Map的key；访问类型为public的静态方法builder，返回的是People.PeopleBuilder对象，非单例；访问类型为public的静态内部类PeopleBuilder，该类主要有build方法，返回类型是People；最后还有个canEqual方法，判断是否与People同类型。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142//// Source code recreated from a .class file by IntelliJ IDEA// (powered by Fernflower decompiler)//public class People { private String name; private String sex; private int age; People(String name, String sex, int age) { this.name = name; this.sex = sex; this.age = age; } public static People.PeopleBuilder builder() { return new People.PeopleBuilder(); } private People() { } public String getName() { return this.name; } public String getSex() { return this.sex; } public int getAge() { return this.age; } public void setName(String name) { this.name = name; } public void setSex(String sex) { this.sex = sex; } public void setAge(int age) { this.age = age; } public boolean equals(Object o) { if (o == this) { return true; } else if (!(o instanceof People)) { return false; } else { People other = (People)o; if (!other.canEqual(this)) { return false; } else { label39: { Object this$name = this.getName(); Object other$name = other.getName(); if (this$name == null) { if (other$name == null) { break label39; } } else if (this$name.equals(other$name)) { break label39; } return false; } Object this$sex = this.getSex(); Object other$sex = other.getSex(); if (this$sex == null) { if (other$sex != null) { return false; } } else if (!this$sex.equals(other$sex)) { return false; } if (this.getAge() != other.getAge()) { return false; } else { return true; } } } } protected boolean canEqual(Object other) { return other instanceof People; } public int hashCode() { int PRIME = true; int result = 1; Object $name = this.getName(); int result = result * 59 + ($name == null ? 43 : $name.hashCode()); Object $sex = this.getSex(); result = result * 59 + ($sex == null ? 43 : $sex.hashCode()); result = result * 59 + this.getAge(); return result; } public String toString() { return \"People(name=\" + this.getName() + \", sex=\" + this.getSex() + \", age=\" + this.getAge() + \")\"; } public static class PeopleBuilder { private String name; private String sex; private int age; PeopleBuilder() { } public People.PeopleBuilder name(String name) { this.name = name; return this; } public People.PeopleBuilder sex(String sex) { this.sex = sex; return this; } public People.PeopleBuilder age(int age) { this.age = age; return this; } public People build() { return new People(this.name, this.sex, this.age); } public String toString() { return \"People.PeopleBuilder(name=\" + this.name + \", sex=\" + this.sex + \", age=\" + this.age + \")\"; } }} 自从Java 6起，Javac就支持“JSR 269 Pluggable Annotation Processing API”规范，只要程序实现了该API，就能在javac运行的时候得到调用。 Lombok就是一个实现了”JSR 269 API”的程序。在使用javac的过程中，它产生作用的具体流程如下： Javac对源代码进行分析，生成一棵抽象语法树(AST) Javac编译过程中调用实现了JSR 269的Lombok程序 此时Lombok就对第一步骤得到的AST进行处理，找到Lombok注解所在类对应的语法树(AST)，然后修改该语法树(AST)，增加Lombok注解定义的相应树节点 Javac使用修改后的抽象语法树(AST)生成字节码文件","link":"/2019/10/11/Builder/"},{"title":"CI/CD","text":"在软件开发中经常会提到持续集成Continuous Integration（CI）和持续交付Continuous Delivery（CD）这几个术语。但它们真正的意思是什么呢？在谈论软件开发时，经常会提到持续集成Continuous Integration（CI）和持续交付Continuous Delivery（CD）这几个术语。但它们真正的意思是什么呢？在本文中，我将解释这些和相关术语背后的含义和意义，例如持续测试Continuous Testing和持续部署Continuous Deployment概览工厂里的装配线以快速、自动化、可重复的方式从原材料生产出消费品。同样，软件交付管道以快速、自动化和可重复的方式从源代码生成发布版本。如何完成这项工作的总体设计称为“持续交付”（CD）。启动装配线的过程称为“持续集成”（CI）。确保质量的过程称为“持续测试”，将最终产品提供给用户的过程称为“持续部署”。一些专家让这一切简单、顺畅、高效地运行，这些人被称为运维开发DevOps践行者。“持续”是什么意思？“持续”用于描述遵循我在此提到的许多不同流程实践。这并不意味着“一直在运行”，而是“随时可运行”。在软件开发领域，它还包括几个核心概念/最佳实践。这些是：频繁发布：持续实践背后的目标是能够频繁地交付高质量的软件。此处的交付频率是可变的，可由开发团队或公司定义。对于某些产品，一季度、一个月、一周或一天交付一次可能已经足够频繁了。对于另一些来说，一天可能需要多次交付也是可行的。所谓持续也有“偶尔、按需”的方面。最终目标是相同的：在可重复、可靠的过程中为最终用户提供高质量的软件更新。通常，这可以通过很少甚至无需用户的交互或掌握的知识来完成（想想设备更新）。自动化流程：实现此频率的关键是用自动化流程来处理软件生产中的方方面面。这包括构建、测试、分析、版本控制，以及在某些情况下的部署。可重复：如果我们使用的自动化流程在给定相同输入的情况下始终具有相同的行为，则这个过程应该是可重复的。也就是说，如果我们把某个历史版本的代码作为输入，我们应该得到对应相同的可交付产出。这也假设我们有相同版本的外部依赖项（即我们不创建该版本代码使用的其它交付物）。理想情况下，这也意味着可以对管道中的流程进行版本控制和重建（请参阅稍后的 DevOps 讨论）。快速迭代：“快速”在这里是个相对术语，但无论软件更新/发布的频率如何，预期的持续过程都会以高效的方式将源代码转换为交付物。自动化负责大部分工作，但自动化处理的过程可能仍然很慢。例如，对于每天需要多次发布候选版更新的产品来说，一轮集成测试integrated testing下来耗时就要大半天可能就太慢了。什么是“持续交付管道”？将源代码转换为可发布产品的多个不同的任务task和作业job通常串联成一个软件“管道”，一个自动流程成功完成后会启动管道中的下一个流程。这些管道有许多不同的叫法，例如持续交付管道、部署管道和软件开发管道。大体上讲，程序管理者在管道执行时管理管道各部分的定义、运行、监控和报告。持续交付管道是如何工作的？软件交付管道的实际实现可以有很大不同。有许多程序可用在管道中，用于源代码跟踪、构建、测试、指标采集，版本管理等各个方面。但整体工作流程通常是相同的。单个业务流程/工作流应用程序管理整个管道，每个流程作为独立的作业运行或由该应用程序进行阶段管理。通常，在业务流程中，这些独立作业是以应用程序可理解并可作为工作流程管理的语法和结构定义的。这些作业被用于一个或多个功能（构建、测试、部署等）。每个作业可能使用不同的技术或多种技术。关键是作业是自动化的、高效的，并且可重复的。如果作业成功，则工作流管理器将触发管道中的下一个作业。如果作业失败，工作流管理器会向开发人员、测试人员和其他人发出警报，以便他们尽快纠正问题。这个过程是自动化的，所以比手动运行一组过程可更快地找到错误。这种快速排错称为快速失败fail fast，并且在抵达管道端点方面同样有价值。“快速失败”是什么意思？管道的工作之一就是快速处理变更。另一个是监视创建发布的不同任务/作业。由于编译失败或测试未通过的代码可以阻止管道继续运行，因此快速通知用户此类情况非常重要。快速失败指的是在管道流程中尽快发现问题并快速通知用户的方式，这样可以及时修正问题并重新提交代码以便使管道再次运行。通常在管道流程中可通过查看历史记录来确定是谁做了那次修改并通知此人及其团队。所有持续交付管道都应该被自动化吗？管道的几乎所有部分都是应该自动化的。对于某些部分，有一些人为干预/互动的地方可能是有意义的。一个例子可能是用户验收测试user-acceptance testing（让最终用户试用软件并确保它能达到他们想要/期望的水平）。另一种情况可能是部署到生产环境时用户希望拥有更多的人为控制。当然，如果代码不正确或不能运行，则需要人工干预。有了对“持续”含义理解的背景，让我们看看不同类型的持续流程以及它们在软件管道上下文中的含义。什么是“持续集成”？持续集成（CI）是在源代码变更后自动检测、拉取、构建和（在大多数情况下）进行单元测试的过程。持续集成是启动管道的环节（尽管某些预验证 —— 通常称为上线前检查pre-flight checks—— 有时会被归在持续集成之前）。持续集成的目标是快速确保开发人员新提交的变更是好的，并且适合在代码库中进一步使用。持续集成是如何工作的？持续集成的基本思想是让一个自动化过程监测一个或多个源代码仓库是否有变更。当变更被推送到仓库时，它会监测到更改、下载副本、构建并运行任何相关的单元测试。持续集成如何监测变更？目前，监测程序通常是像 Jenkins 这样的应用程序，它还协调管道中运行的所有（或大多数）进程，监视变更是其功能之一。监测程序可以以几种不同方式监测变更。这些包括：轮询：监测程序反复询问代码管理系统，“代码仓库里有什么我感兴趣的新东西吗？”当代码管理系统有新的变更时，监测程序会“唤醒”并完成其工作以获取新代码并构建/测试它。定期：监测程序配置为定期启动构建，无论源码是否有变更。理想情况下，如果没有变更，则不会构建任何新内容，因此这不会增加额外的成本。推送：这与用于代码管理系统检查的监测程序相反。在这种情况下，代码管理系统被配置为提交变更到仓库时将“推送”一个通知到监测程序。最常见的是，这可以以 webhook 的形式完成 —— 在新代码被推送时一个挂勾hook的程序通过互联网向监测程序发送通知。为此，监测程序必须具有可以通过网络接收 webhook 信息的开放端口。什么是“预检查”（又称“上线前检查”）？在将代码引入仓库并触发持续集成之前，可以进行其它验证。这遵循了最佳实践，例如测试构建test build和代码审查code review。它们通常在代码引入管道之前构建到开发过程中。但是一些管道也可能将它们作为其监控流程或工作流的一部分。例如，一个名为 Gerrit 的工具允许在开发人员推送代码之后但在允许进入（Git 远程）仓库之前进行正式的代码审查、验证和测试构建。Gerrit 位于开发人员的工作区和 Git 远程仓库之间。它会“接收”来自开发人员的推送，并且可以执行通过/失败验证以确保它们在被允许进入仓库之前的检查是通过的。这可以包括检测新变更并启动构建测试（CI 的一种形式）。它还允许开发者在那时进行正式的代码审查。这种方式有一种额外的可信度评估机制，即当变更的代码被合并到代码库中时不会破坏任何内容。什么是“单元测试” ？单元测试（也称为“提交测试”），是由开发人员编写的小型的专项测试，以确保新代码独立工作。“独立”这里意味着不依赖或调用其它不可直接访问的代码，也不依赖外部数据源或其它模块。如果运行代码需要这样的依赖关系，那么这些资源可以用模拟mock来表示。模拟是指使用看起来像资源的代码存根code stub，可以返回值，但不实现任何功能。在大多数组织中，开发人员负责创建单元测试以证明其代码正确。事实上，一种称为测试驱动开发test-driven develop（TDD）的模型要求将首先设计单元测试作为清楚地验证代码功能的基础。因为这样的代码可以更改速度快且改动量大，所以它们也必须执行很快。由于这与持续集成工作流有关，因此开发人员在本地工作环境中编写或更新代码，并通单元测试来确保新开发的功能或方法正确。通常，这些测试采用断言形式，即函数或方法的给定输入集产生给定的输出集。它们通常进行测试以确保正确标记和处理出错条件。有很多单元测试框架都很有用，例如用于 Java 开发的 JUnit。什么是“持续测试”？持续测试是指在代码通过持续交付管道时运行扩展范围的自动化测试的实践。单元测试通常与构建过程集成，作为持续集成阶段的一部分，并专注于和其它与之交互的代码隔离的测试。除此之外，可以有或者应该有各种形式的测试。这些可包括：集成测试 验证组件和服务组合在一起是否正常。功能测试 验证产品中执行功能的结果是否符合预期。验收测试 根据可接受的标准验证产品的某些特征。如性能、可伸缩性、抗压能力和容量。所有这些可能不存在于自动化的管道中，并且一些不同类型的测试分类界限也不是很清晰。但是，在交付管道中持续测试的目标始终是相同的：通过持续的测试级别证明代码的质量可以在正在进行的发布中使用。在持续集成快速的原则基础上，第二个目标是快速发现问题并提醒开发团队。这通常被称为快速失败。除了测试之外，还可以对管道中的代码进行哪些其它类型的验证？除了测试是否通过之外，还有一些应用程序可以告诉我们测试用例执行（覆盖）的源代码行数。这是一个可以衡量代码量指标的例子。这个指标称为代码覆盖率code-coverage，可以通过工具（例如用于 Java 的 JaCoCo）进行统计。还有很多其它类型的指标统计，例如代码行数、复杂度以及代码结构对比分析等。诸如 SonarQube 之类的工具可以检查源代码并计算这些指标。此外，用户还可以为他们可接受的“合格”范围的指标设置阈值。然后可以在管道中针对这些阈值设置一个检查，如果结果不在可接受范围内，则流程终端上。SonarQube 等应用程序具有很高的可配置性，可以设置仅检查团队感兴趣的内容。什么是“持续交付”？持续交付（CD）通常是指整个流程链（管道），它自动监测源代码变更并通过构建、测试、打包和相关操作运行它们以生成可部署的版本，基本上没有任何人为干预。持续交付在软件开发过程中的目标是自动化、效率、可靠性、可重复性和质量保障（通过持续测试）。持续交付包含持续集成（自动检测源代码变更、执行构建过程、运行单元测试以验证变更），持续测试（对代码运行各种测试以保障代码质量），和（可选）持续部署（通过管道发布版本自动提供给用户）。如何在管道中识别/跟踪多个版本？版本控制是持续交付和管道的关键概念。持续意味着能够经常集成新代码并提供更新版本。但这并不意味着每个人都想要“最新、最好的”。对于想要开发或测试已知的稳定版本的内部团队来说尤其如此。因此，管道创建并轻松存储和访问的这些版本化对象非常重要。在管道中从源代码创建的对象通常可以称为工件artifact。工件在构建时应该有应用于它们的版本。将版本号分配给工件的推荐策略称为语义化版本控制semantic versioning。（这也适用于从外部源引入的依赖工件的版本。）语义版本号有三个部分：主要版本major、次要版本minor和 补丁版本patch。（例如，1.4.3 反映了主要版本 1，次要版本 4 和补丁版本 3。）这个想法是，其中一个部分的更改表示工件中的更新级别。主要版本仅针对不兼容的 API 更改而递增。当以向后兼容backward-compatible的方式添加功能时，次要版本会增加。当进行向后兼容的版本 bug 修复时，补丁版本会增加。这些是建议的指导方针，但只要团队在整个组织内以一致且易于理解的方式这样做，团队就可以自由地改变这种方法。例如，每次为发布完成构建时增加的数字可以放在补丁字段中。如何“分销”工件？团队可以为工件分配分销promotion级别以指示适用于测试、生产等环境或用途。有很多方法。可以用 Jenkins 或 Artifactory 等应用程序进行分销。或者一个简单的方案可以在版本号字符串的末尾添加标签。例如，-snapshot 可以指示用于构建工件的代码的最新版本（快照）。可以使用各种分销策略或工具将工件“提升”到其它级别，例如 -milestone 或 -production，作为工件稳定性和完备性版本的标记。如何存储和访问多个工件版本？从源代码构建的版本化工件可以通过管理工件仓库artifact repository的应用程序进行存储。工件仓库就像构建工件的版本控制工具一样。像 Artifactory 或 Nexus 这类应用可以接受版本化工件，存储和跟踪它们，并提供检索的方法。管道用户可以指定他们想要使用的版本，并在这些版本中使用管道。什么是“持续部署”？持续部署（CD）是指能够自动提供持续交付管道中发布版本给最终用户使用的想法。根据用户的安装方式，可能是在云环境中自动部署、app 升级（如手机上的应用程序）、更新网站或只更新可用版本列表。这里的一个重点是，仅仅因为可以进行持续部署并不意味着始终部署来自管道的每组可交付成果。它实际上指，通过管道每套可交付成果都被证明是“可部署的”。这在很大程度上是由持续测试的连续级别完成的（参见本文中的持续测试部分）。管道构建的发布成果是否被部署可以通过人工决策，或利用在完全部署之前“试用”发布的各种方法来进行控制。在完全部署到所有用户之前，有哪些方法可以测试部署？由于必须回滚/撤消对所有用户的部署可能是一种代价高昂的情况（无论是技术上还是用户的感知），已经有许多技术允许“尝试”部署新功能并在发现问题时轻松“撤消”它们。这些包括：蓝/绿测试/部署在这种部署软件的方法中，维护了两个相同的主机环境 —— 一个“蓝色” 和一个“绿色”。（颜色并不重要，仅作为标识。）对应来说，其中一个是“生产环境”，另一个是“预发布环境”。在这些实例的前面是调度系统，它们充当产品或应用程序的客户“网关”。通过将调度系统指向蓝色或绿色实例，可以将客户流量引流到期望的部署环境。通过这种方式，切换指向哪个部署实例（蓝色或绿色）对用户来说是快速，简单和透明的。当新版本准备好进行测试时，可以将其部署到非生产环境中。在经过测试和批准后，可以更改调度系统设置以将传入的线上流量指向它（因此它将成为新的生产站点）。现在，曾作为生产环境实例可供下一次候选发布使用。同理，如果在最新部署中发现问题并且之前的生产实例仍然可用，则简单的更改可以将客户流量引流回到之前的生产实例 —— 有效地将问题实例“下线”并且回滚到以前的版本。然后有问题的新实例可以在其它区域中修复。金丝雀测试/部署在某些情况下，通过蓝/绿发布切换整个部署可能不可行或不是期望的那样。另一种方法是为金丝雀canary测试/部署。在这种模型中，一部分客户流量被重新引流到新的版本部署中。例如，新版本的搜索服务可以与当前服务的生产版本一起部署。然后，可以将 10％ 的搜索查询引流到新版本，以在生产环境中对其进行测试。如果服务那些流量的新版本没问题，那么可能会有更多的流量会被逐渐引流过去。如果仍然没有问题出现，那么随着时间的推移，可以对新版本增量部署，直到 100％ 的流量都调度到新版本。这有效地“更替”了以前版本的服务，并让新版本对所有客户生效。功能开关对于可能需要轻松关掉的新功能（如果发现问题），开发人员可以添加功能开关feature toggles。这是代码中的 if-then 软件功能开关，仅在设置数据值时才激活新代码。此数据值可以是全局可访问的位置，部署的应用程序将检查该位置是否应执行新代码。如果设置了数据值，则执行代码；如果没有，则不执行。这为开发人员提供了一个远程“终止开关”，以便在部署到生产环境后发现问题时关闭新功能。暗箱发布在暗箱发布dark launch中，代码被逐步测试/部署到生产环境中，但是用户不会看到更改（因此名称中有暗箱dark一词）。例如，在生产版本中，网页查询的某些部分可能会重定向到查询新数据源的服务。开发人员可收集此信息进行分析，而不会将有关接口，事务或结果的任何信息暴露给用户。这个想法是想获取候选版本在生产环境负载下如何执行的真实信息，而不会影响用户或改变他们的经验。随着时间的推移，可以调度更多负载，直到遇到问题或认为新功能已准备好供所有人使用。实际上功能开关标志可用于这种暗箱发布机制。什么是“运维开发”？运维开发DevOps是关于如何使开发和运维团队更容易合作开发和发布软件的一系列想法和推荐的实践。从历史上看，开发团队研发了产品，但没有像客户那样以常规、可重复的方式安装/部署它们。在整个周期中，这组安装/部署任务（以及其它支持任务）留给运维团队负责。这经常导致很多混乱和问题，因为运维团队在后期才开始介入，并且必须在短时间内完成他们的工作。同样，开发团队经常处于不利地位 —— 因为他们没有充分测试产品的安装/部署功能，他们可能会对该过程中出现的问题感到惊讶。这往往导致开发和运维团队之间严重脱节和缺乏合作。DevOps 理念主张是贯穿整个开发周期的开发和运维综合协作的工作方式，就像持续交付那样。持续交付如何与运维开发相交？持续交付管道是几个 DevOps 理念的实现。产品开发的后期阶段（如打包和部署）始终可以在管道的每次运行中完成，而不是等待产品开发周期中的特定时间。同样，从开发到部署过程中，开发和运维都可以清楚地看到事情何时起作用，何时不起作用。要使持续交付管道循环成功，不仅要通过与开发相关的流程，还要通过与运维相关的流程。说得更远一些，DevOps 建议实现管道的基础架构也会被视为代码。也就是说，它应该自动配置、可跟踪、易于修改，并在管道发生变化时触发新一轮运行。这可以通过将管道实现为代码来完成。什么是“管道即代码”？管道即代码pipeline-as-code是通过编写代码创建管道作业/任务的通用术语，就像开发人员编写代码一样。它的目标是将管道实现表示为代码，以便它可以与代码一起存储、评审、跟踪，如果出现问题并且必须终止管道，则可以轻松地重建。有几个工具允许这样做，如 Jenkins 2。DevOps 如何影响生产软件的基础设施？传统意义上，管道中使用的各个硬件系统都有配套的软件（操作系统、应用程序、开发工具等）。在极端情况下，每个系统都是手工设置来定制的。这意味着当系统出现问题或需要更新时，这通常也是一项自定义任务。这种方法违背了持续交付的基本理念，即具有易于重现和可跟踪的环境。多年来，很多应用被开发用于标准化交付（安装和配置）系统。同样，虚拟机virtual machine被开发为模拟在其它计算机之上运行的计算机程序。这些 VM 要有管理程序才能在底层主机系统上运行，并且它们需要自己的操作系统副本才能运行。后来有了容器container。容器虽然在概念上与 VM 类似，但工作方式不同。它们只需使用一些现有的操作系统结构来划分隔离空间，而不需要运行单独的程序和操作系统的副本。因此，它们的行为类似于 VM 以提供隔离但不需要过多的开销。VM 和容器是根据配置定义创建的，因此可以轻易地销毁和重建，而不会影响运行它们的主机系统。这允许运行管道的系统也可重建。此外，对于容器，我们可以跟踪其构建定义文件的更改 —— 就像对源代码一样。因此，如果遇到 VM 或容器中的问题，我们可以更容易、更快速地销毁和重建它们，而不是在当前环境尝试调试和修复。这也意味着对管道代码的任何更改都可以触发管道新一轮运行（通过 CI），就像对代码的更改一样。这是 DevOps 关于基础架构的核心理念之一。","link":"/2019/07/19/CI-CD/"},{"title":"","text":"【EnUSP】windows/mac客户端的针对B/S和C/S业务配置采用相同的配置入口[TOC] 需求描述可以在应用发布中，添加Web应用和网络层应用。 其中网络层应用即对应C/S应用，并通过隧道方式可以访问。 在网络层应用发布中，添加内网策略配置，定义内网访问的感兴趣流量。 同时可以定义访问域名黑白名单，设定访问的权限。 需求分析在应用类型中添加CS应用，对应白名单， 在安全策略访问策略中添加 CS策略对应黑名单， 用户登录时一并下发给网关 实现方案一、 添加cs应用: 应用类型中添加cs应用选项 CS应用对应的配置有 1.配置项（ip和域名）， 2.ip IP ip为特定的某一个ip而不是IP段 协议 （TCP、UDP、ANY）(ANY表示TCP,UDP都可) 端口 3.域名 域名 内网IP 协议 （TCP、UDP、ANY）(ANY表示TCP,UDP都可) 端口 合并存到tb_apps_service_info 里的server字段中 例: TCP://enlink.cn-192.168.10.10:443 UDP://192.168.10.10:1010 ANY://192.168.10.10:9000 添加cs应用同时绑定角色和应用分组和网关以及信用等级等 添加应用接口 ip:port/api/service/add method: P body 12345678910111213{ connectState: \"0\" description: \"\" group: \"\" groupSearch: \"\" icon: \"/files/icon/app.png\" id: \"\" name: \"cs\" path: \"\" server: \"tcp://192.168.10.10:9555\" terminalProtocol: \"\" type: \"cs\"} 二、应用分组中添加cs组 1INSERT INTO `tb_apps_service_group`(`id`, `name`, `parent_id`, `path`, `description`, `sort_number`, `state`, `creator`, `create_time`, `updator`, `update_time`) VALUES ('3', 'cs', '0', '/cs', '', 'A', '0', '00000000-2019-0918-0000-admin0000000', '2020-03-09 13:45:14', '00000000-2019-0918-0000-admin0000000', '2020-03-26 19:45:24'); 三、在策略表tb_apps_strategy_conftype中加上一种类型53用作CS策略,同时在/user/strategy/access/add接口中加上cs策略type=53的判断 四、用户登录通知网关 延用web应用下发逻辑","link":"/2020/05/30/CS应用配置/"},{"title":"BUILD","text":"bazel笔记：bazel的编译是基于工作区，也就是项目的根目录 workspace文件：制定当前文件夹就是一个bazel工作区。 一个或多个build文件，如果工作区中的一个目录包含build文件，那么他就是一个package.因此，要制定一个目录为bazel的工作区，就只要在该目录下创建一个空的workspace即可。//: 该符号标识根目录下的文件 target：cc_binary (name)cc_libarycc_test bazel有两个关键元素，一个是package，另一个是targetpackage是可独立编译的project包，由workspace文件（可为空）标识；target是BUILD文件中的关键元素，也就是编译的目标，目标可以是二进制文件（cc_binary), 可以是libary(cc_libary) 12visiblity: [\"//visibility:public\"],['//visibility:private'](私有)，['//some/package:__pkg__'](注意冒号) ex: package(default_visibility = [\"//visibility:public\"]) Dependencies： 在构建或者执行时，目标A依赖于目标B，这种依赖关系产生一个有向无环图，我们将这种有向无环图称为依赖图， 一个目标的直接依赖关系指的是在依赖图中可以以1步到达依赖对象的关系， 二传递依赖关系指的是需要经过数步才能到达依赖对象的关系。 事实上在构建的上下文中，有两种依赖关系，实际依赖关系和声明依赖关系。两者差别极小。实际依赖关系指的是构建或者执行的时候A需要B，而声名依赖关系仅仅是在包A中有从A到B的一条依赖线。 为了确保构建的正确性，实际依赖关系A必须是声名依赖关系的子图，也就是说，每一个在A中的直接依赖关系对x—&gt;y，必须也是D中的直接依赖关系对。 在a、b、c三个个字包含了BUILD文件的三个包中，a依赖b，b依赖c，比如下图： 其中的声明依赖关系为：deps：a–&gt;b;b–&gt;c; 实际依赖关系为：各自srcs的目标文件中的import 及相应的函数的表征：a–&gt;b；b–&gt;c; 三种常见的构建规则的依赖关系类型： 1、srcs依赖：也就是BUILD文件中规则的属性Srcs的值，他表示构建该规则时使用的程序代码； 2、deps依赖：也就是规则中的deps属性的值，表示构建规则时所需要的头文件等； 3、data依赖：它不是代码，只是构建规则时需要的数据； 构建系统在一个独立的目录中进行测试，该目录中仅仅只有命名为data的文件可视，因此，需要测试时，需要声明数据为data文件； 注：您不应将目录指定为构建系统的输入，而应明确或使用该glob()函数枚举其中包含的文件集 用于强制递归。 java_binaryjava_binary（name，deps，srcs，data，resources，args，classpath_resources，compatible_with，create_executable，deploy_env，deploy_manifest_lines，deprecation，distribs，exec_compatible_with，features，javacopts，jvm_flags，launcher，licenses，main_class，output_licenses，plugins，resource_jars，resource_strip_prefix，restricted_to，runtime_deps，stamp，tags，testonly，toolchains，use_testrunner，visibility） 构建Java归档文件（“jar文件”），以及与规则同名的包装器shell脚本。包装器shell脚本使用一个类路径，其中包括二进制所依赖的每个库的jar文件。 包装器脚本接受几个唯一标志。请参阅 //src/main/java/com/google/devtools/build/lib/bazel/rules/java/java_stub_template.txt包装器接受的可配置标志和环境变量的列表。 建个target可视化项目的依赖项将项目拆分为多个目标和包控制包之间的目标可见性通过标签引用目标部署目标 设置工作区理解BUILD文件构建项目查看依赖关系图完善bazel 构建指定多个生成目标使用多个包使用标签引用目标打包部署的Java目标进一步读 构建一个项目前需要先建个工作区工作区是保存项目源文件和bazel的目录。工作区文件，将目录及其内容标识为bazel工程位于项目目录结构的根目录构建文件包含几种不同类型的bazel指令最重要的类型是构建规则告诉Bazel如何构建所需的输出例如可执行的二进制文件或库生成文件中生成规则的每个实例都称为目标指向一组特定的源文件和依赖项目标也可以指向其他目标projectrunner目标实例化bazel的内置Java_二进制规则目标中的属性显式地声明其依赖项和选项规则告诉bazel构建一个.jar文件和一个包装shell脚本虽然name属性是必需的，许多是可选的例如，在ProjectRunner规则目标中，name是目标的名称srcs指定bazel用于构建目标的源文件main_类指定包含main方法的类注意目标标签//部分是构建文件rela的位置ProjectRunner是我们在构建文件中命名的目标Bazel的产出与以下类似 12345INFO: Found 1 target...Target //:ProjectRunner up-to-date: bazel-bin/ProjectRunner.jar bazel-bin/ProjectRunnerINFO: Elapsed time: 1.021s, Critical Path: 0.83s bazel将构建输出放在工作目录下的bazel bin目录中浏览其内容以了解Bazel的输出结构Bazel要求在生成文件中显式声明生成依赖项Bazel使用这些语句创建项目的依赖关系图从而实现精确的增量构建让我们可视化示例项目的依赖关系。首先，生成依赖关系图的文本表示形式 在工作区根目录下运行命令bazel query --nohost_deps --noimplicit_deps &quot;deps(//:ProjectRunner)&quot; --output graph上面的命令告诉bazel查找目标的所有依赖项//:ProjectRunner然后，将文本粘贴到graphviz中如您所见，该项目有一个构建两个源文件的目标现在您已经设置了工作区，构建了项目，并检查了它。完善构建 当一个目标就足以满足小型项目的需求 ，您可能希望将较大的项目拆分为多个目标和包允许快速增量生成只重建已更改的内容 通过同时构建项目的多个部分来加速构建 指定多个生成目标让我们将示例项目构建分为两个目标。替换内容 1234567891011java_binary( name = \"ProjectRunner\", srcs = [\"src/main/java/com/example/ProjectRunner.java\"], main_class = \"com.example.ProjectRunner\", deps = [\":greeter\"],)java_library( name = \"greeter\", srcs = [\"src/main/java/com/example/Greeting.java\"],) 通过这种配置，Bazel首先构建了Greeter库。然后是projectrunner二进制文件java_二进制文件中的deps属性告诉bazel，需要Greeter库来构建ProjectRunner二进制文件让我们构建这个新版本的项目。运行以下命令bazel build //:ProjectRunner Bazel的输出与以下类似 12345INFO: Found 1 target...Target //:ProjectRunner up-to-date: bazel-bin/ProjectRunner.jar bazel-bin/ProjectRunnerINFO: Elapsed time: 2.454s, Critical Path: 1.58s 现在测试新构建的二进制文件 bazel-bin/ProjectRunner如果现在修改projectrunner.java并重新生成项目，则bazel仅重新编译该文件查看依赖关系图，您可以看到ProjectRunner 依赖于与以前相同的输入，但构建的结构不同现在您已经用两个目标构建了项目。ProjectRunner目标生成两个源文件并依赖于另一个目标:greeter它会生成一个附加的源文件 现在让我们把这个项目分成多个包你现在看下src/main/java/com/example/cmdline这个文件夹您可以看到它还包含一个生成文件 BUILD加上一些源文件 因此，对于Bazel，工作区现在包含两个包 //src/main/java/com/example/cmdline 和 // 因为在工作区的根目录下有一个生成文件看一下 src/main/java/com/example/cmdline/BUILD 文件 123456java_binary( name = \"runner\", srcs = [\"Runner.java\"], main_class = \"com.example.cmdline.Runner\", deps = [\"//:greeter\"]) 运行目标依赖greeter目标 在// 包下因此，目标标签 //:greeterbazel 通过deps属性知道这一点看看依赖关系图但是，要使构建成功，必须在//src/main/java/com/example/cmdline/BUILD 文件中明确地给出运行者目标目标可见性在//BUILD中使用visibility属性这是因为默认情况下，目标仅对同一个build 文件中的其他目标可见Bazel使用目标可见性来防止诸如包含泄漏到公共API中的实现细节的库为此，将可见性属性添加到java-tutorial/BUILD中的greeter对象中。 bazel build //src/main/java/com/example/cmdline:runner 12345java_library( name = \"greeter\", srcs = [\"src/main/java/com/example/Greeting.java\"], visibility = [\"//src/main/java/com/example/cmdline:__pkg__\"],) 现在我们来构建新的包。在t的根目录下运行以下命令bazel的输出大概如下 12345INFO: Found 1 target...Target //src/main/java/com/example/cmdline:runner up-to-date: bazel-bin/src/main/java/com/example/cmdline/runner.jar bazel-bin/src/main/java/com/example/cmdline/runner INFO: Elapsed time: 1.576s, Critical Path: 0.81s 现在测试新构建的二进制文件 ./bazel-bin/src/main/java/com/example/cmdline/runner 现在，您已经将项目修改为两个包，每个包含一个目标，并了解它们之间的依赖关系使用标签引用目标在构建文件和命令行中，bazel使用目标标签进行引用//:ProjectRunner或//src/main/java/com/example/cmdline:runner 他们的语法如下： //path/to/package:target-name 如果目标是规则目标 则path/to/package是包含BUILD生成文件的目录的路径目标名称是您在构建文件中为目标命名的名称，如果目标是个文件目标则path/to/package 是指向包根目录的路径目标名称是目标文件的名称，包括其完整路径。 引用同一包中的目标时 您可以跳过包路径 就用 //:target-name引用同一生成文件中的目标时甚至可以跳过工作区根标识符 就用:target-name例如在java-tutorial/BUILD目标文件您不必指定包路径因为工作区根本身就是一个包 (//)你的两个目标标签很简单//:ProjectRunner 和 //:greeter.然而目标在 //src/main/java/com/example/cmdline/BUILD 文件您必须指定全包路径你的目标标签是//src/main/java/com/example/cmdline:runner打包部署一个Java目标现在我们通过构建二进制文件来打包部署Java目标。正如您所记得的，java_二进制构建规则生成一个.jar包和包装shell脚本看下runner.jar的内容使用如下命令 jar tf bazel-bin/src/main/java/com/example/cmdline/runner.jar 内容如下： 123456META-INF/META-INF/MANIFEST.MFcom/com/example/com/example/cmdline/com/example/cmdline/Runner.class 如你所见runner.jar包含Runner.class但不是它的依赖性 Greeting.classrunner脚本 bazel生成的runner脚本将greeter.jar添加到类路径中 ，所以如果你像这样放着，他将会在本地运行，但它不会在另一台机器上独立运行 幸运的是，java_二进制规则允许您构建一个自包含的depl要构建它，请在构建runner时向文件名添加_deploy.jar后缀 bazel build //src/main/java/com/example/cmdline:runner_deploy.jar Bazel输出与以下类似 1234 INFO: Found 1 target...Target //src/main/java/com/example/cmdline:runner_deploy.jar up-to-date: bazel-bin/src/main/java/com/example/cmdline/runner_deploy.jarINFO: Elapsed time: 1.700s, Critical Path: 0.23s 你刚刚建立了一个runner_deploy.jar它可以独立运行在开发环境之外因为它包含所需的运行时依赖项。 隐含的输出目标name.jar：Java归档文件，包含与二进制文件直接依赖项对应的类文件和其他资源。name-src.jar：包含源（“源jar”）的存档。name_deploy.jar：适合部署的Java归档文件（仅在明确请求时构建）。构建规则的目标会创建一个自包含的jar文件，其中包含一个清单，允许使用命令或包装器脚本的 选项运行它 。使用包装器脚本是首选，因为它还传递JVM标志和加载本机库的选项。 _deploy.jarjava -jar–singlejarjava -jar 部署jar包含类加载器可以找到的所有类，这些类从头到尾从二进制文件的包装器脚本中搜索类路径。它还包含依赖项所需的本机库。它们在运行时自动加载到JVM中。 如果您的目标指定了一个启动器 属性，那么_deploy.jar将是一个原生二进制文件，而不是一个普通的JAR文件。这将包含启动程序以及规则的任何本机（C ++）依赖项，所有这些都链接到静态二进制文件。实际的jar文件的字节将附加到该本机二进制文件，创建一个包含可执行文件和Java代码的二进制blob。您可以直接执行生成的jar文件，就像执行任何本机二进制文件一样。 name_deploy-src.jar：包含从目标的传递闭包收集的源的存档。这些将匹配 deploy.jar除了jar没有匹配的源jar的类。deps没有java_binary规则的规则中 不允许 使用属性srcs; 这样的规则要求 main_class提供者 runtime_deps。 以下代码段说明了一个常见错误： 12345678java_binary（ name =“DontDoThis”， srcs = [ ... ,, \"GeneratedJavaFile.java\"＃生成的.java文件 ] deps = [ \":generating_rule\",]，＃生成该文件的规则） 改为： 1234567java_binary（ name =“DoThisInstead”， srcs = [ ......， “：generating_rule” ]） 参数属性nameName; required 此目标的唯一名称。 最好使用作为应用程序主入口点的源文件的名称（减去扩展名）。例如，如果您的入口点被调用 Main.java，那么您的名字可能就是Main。depsList of labels; optional 要链接到目标的其他库的列表。看到一般的评论deps在 共同所有的生成规则属性 。srcsList of labels; optional 处理以创建目标的源文件列表。几乎总是需要此属性; 看下面的例外。.java编译 类型的源文件。在生成.java文件的情况下， 通常建议在此处放置生成规则的名称，而不是文件本身的名称。这不仅提高了可读性，而且使规则对未来的更改更具弹性：如果生成规则将来生成不同的文件，您只需要修复一个地方：outs生成规则。您不应该列出生成规则，deps 因为它是无操作。 类型的源文件.srcjar被解压缩和编译。（如果您需要生成一组.java带有genrule 的文件，这非常有用。） 规则：如果规则（通常genrule或filegroup）生成上面列出的任何文件，它们将以与源文件所述相同的方式使用。 除非main_class属性指定运行时类路径上的类或指定runtime_deps参数，否则几乎总是需要 此参数。 resourcesList of labels; optional 要包含在Java jar中的数据文件列表。如果指定了资源，它们将与.class编译生成的常用文件一起捆绑在jar中 。jar文件中资源的位置由项目结构决定。Bazel首先查找Maven的 标准目录布局，（“src”目录后跟“资源”目录孙子）。如果找不到，那么Bazel会查找名为“java”或“javatests”的最顶层目录（例如，如果资源处于/x/java/y/java/z，则资源的路径将是y/java/z。此启发式扫描不能被覆盖。 资源可以是源文件或生成的文件。 classpath_resourcesList of labels; optional 除非没有其他方式，否则不要使用此选项）必须位于Java树根目录的资源列表。此属性的唯一目的是支持第三方库，这些库要求在类路径上找到它们的资源”myconfig.xml”。由于名称空间冲突的危险，它只允许在二进制文件而不是库中。 create_executableBoolean; optional; nonconfigurable; default is True 二进制文件是否可执行。不可执行的二进制文件将传递的运行时Java依赖项收集到部署jar中，但不能直接执行。如果设置了此属性，则不会创建包装器脚本。如果设置了launcher或main_class属性，则将此值设置为0是错误的。deploy_envList of labels; optional java_binary表示此二进制文件的部署环境 的其他目标的列表。在构建将由另一个加载的插件时设置此属性 java_binary。设置此属性将排除此二进制文件的运行时类路径（和部署jar）中的所有依赖项，这些依赖项在此二进制文件和指定的目标之间共享deploy_env。deploy_manifest_linesList of strings; optional 要添加到META-INF/manifest.mf为*_deploy.jar目标生成的文件 的行列表。此属性的内容不符合“使变量”替换。javacoptsList of strings; optional 此库的额外编译器选项。受“Make变量”替换和 Bourne shell标记化的约束。这些编译器选项在全局编译器选项之后传递给javac。 jvm_flagsList of strings; optional 要在运行此二进制文件时生成的包装器脚本中嵌入的标志列表。受$（位置）和 “Make变量”替换，以及 Bourne shell标记化。Java二进制文件的包装器脚本包括一个CLASSPATH定义（用于查找所有相关的jar）并调用正确的Java解释器。包装器脚本生成的命令行包含主类的名称，后跟a，”$@”因此您可以在类名后传递其他参数。但是，必须在命令行上的类名之前指定用于由JVM进行解析的参数。在jvm_flags列出类名之前，将内容添加到包装脚本中。 请注意，此属性对 输出没有影响*_deploy.jar。 launcherLabel; optional 指定将用于运行Java程序的二进制文件，而不是bin/javaJDK附带的普通程序。目标必须是a cc_binary。任何cc_binary实现 Java Invocation API的都可以指定为此属性的值。默认情况下，Bazel将使用普通的JDK启动程序（bin / java或java.exe）。 相关的 –java_launcherBazel标志仅影响未指定属性的那些 java_binary和java_test目标 。launcher 请注意，根据您使用的是JDK启动程序还是其他启动程序，您的本机（C ++，SWIG，JNI）依赖项的构建方式会有所不同： 如果您使用的是普通的JDK启动程序（默认），则将本机依赖项构建为名为的共享库{name}_nativedeps.so，其中 {name}是name此java_binary规则的属性。在此配置中，链接器不会删除未使用的代码。如果您正在使用任何其他启动程序，则本机（C ++）依赖项将静态链接到名为的二进制文件{name}_nativedeps，其中此java_binary规则{name} 的name属性。在这种情况下，链接器将从生成的二进制文件中删除它认为未使用的任何代码，这意味着除非该cc_library目标指定，否则任何仅通过JNI访问的C ++代码都可能无法链接alwayslink = 1。使用除默认JDK启动程序以外的任何启动程序时，*_deploy.jar输出的格式会更改。有关详细信息，请参阅主 java_binary文档。 main_classString; optional 具有main()用作入口点的方法的类的名称。如果规则使用此选项，则不需要srcs=[…]列表。因此，使用此属性，可以从已包含一个或多个main()方法的Java库生成可执行文件。此属性的值是类名，而不是源文件。该类必须在运行时可用：它可以由此规则（从srcs）编译，或由直接或传递依赖（通过runtime_deps或 deps）提供。如果该类不可用，则二进制文件将在运行时失败; 没有构建时间检查。 pluginsList of labels; optional Java编译器插件在编译时运行。java_plugin无论何时构建此规则，都将运行此属性中指定的每个属性。库也可以从使用的依赖项继承插件 exported_plugins。插件生成的资源将包含在此规则的结果jar中。resource_jarsList of labels; optional 包含Java资源的档案集。如果指定，这些jar的内容将合并到输出jar中。 resource_strip_prefixString; optional 从Java资源中剥离的路径前缀。如果指定，则从resources 属性中的每个文件中剥离此路径前缀。资源文件不在此目录下是错误的。如果未指定（默认值），则根据与源文件的Java包相同的逻辑确定资源文件的路径。例如，源文件 stuff/java/foo/bar/a.txt位于foo/bar/a.txt。 runtime_depsList of labels; optional 库可用于最终二进制文件或仅在运行时测试。与普通类似deps，这些将出现在运行时类路径中，但与它们不同，不在编译时类路径上。应在此处列出仅在运行时所需的依赖关系。依赖性分析工具应该忽略出现在两个目标 runtime_deps和deps。stampInteger; optional; default is -1 启用链接标记。是否将构建信息编码为二进制文件。可能的值：stamp = 1：将构建信息标记到二进制文件中。只有在依赖项发生变化时才会重建标记的二进制文件。如果存在依赖于构建信息的测试，请使用此选项。stamp = 0：始终使用常量值替换构建信息。这提供了良好的构建结果缓存。stamp = -1：嵌入构建信息由 - [no]戳标志控制。toolchainsList of labels; optional 提供“Make variables”的工具链集 ，该目标可以在其某些属性中使用。一些规则具有工具链，其默认情况下可以使用Make变量。use_testrunnerBoolean; optional; default is False 使用测试运行器（默认情况下 com.google.testing.junit.runner.BazelTestRunner）类作为Java程序的主入口点，并将测试类作为bazel.test_suite 系统属性的值提供给测试运行器。您可以使用它来覆盖默认行为，即使用测试运行器获取 java_test规则，而不是将其用于java_binary规则。你不太可能想要这样做。一种用途是用于AllTest 由另一个规则调用的规则（例如，在运行测试之前设置数据库）。该AllTest 规则必须被声明为java_binary，但仍应使用测试运行器作为其主要入口点。可以使用main_classattribute 覆盖测试运行器类的名称。java_importjava_import（name，deps，data，compatible_with，constraints，deprecation，distribs，exec_compatible_with，exports，features，jars，licenses，neverlink，proguard_specs，restricted_to，runtime_deps，srcjar，tags，testonly，visibility）此规则允许将预编译.jar文件用作库java_library和 java_binary规则。 例子 12345678java_import（ name =“maven_model”， jars = [ “maven_model / Maven的以太网提供商-3.2.3.jar”， “maven_model / Maven的模型3.2.3.jar”， “maven_model / Maven的模型建设者-3.2.3.jar”， ]） 参数 属性nameName; required 此目标的唯一名称。 depsList of labels; optional 要链接到目标的其他库的列表。请参阅java_library.deps。constraintsList of strings; optional; nonconfigurable 作为Java库对此规则施加额外约束。exportsList of labels; optional 目标可供此规则的用户使用。请参阅java_library.exports。jarsList of labels; required 提供给依赖于此目标的Java目标的JAR文件列表。neverlinkBoolean; optional; default is False 仅使用此库进行编译，而不是在运行时使用。如果库将在执行期间由运行时环境提供，则很有用。像这样的库的示例是用于IDE插件的IDE API或tools.jar用于在标准JDK上运行的任何内容。proguard_specsList of labels; optional 用作Proguard规范的文件。这些将描述Proguard使用的规范集。如果指定，它们将android_binary根据此库添加到任何目标。此处包含的文件必须只有幂等规则，即-dontnote，-dontwarn，assumenosideeffects和以-keep开头的规则。其他选项只能出现在 android_binaryproguard_specs中，以确保非重言式合并。runtime_depsList of labels; optional 库可用于最终二进制文件或仅在运行时测试。请参阅java_library.runtime_deps。srcjarLabel; optional 包含已编译JAR文件的源代码的JAR文件。 12345java_library java_library（name，deps，srcs，data，resources，compatible_with，deprecation，distribs，exec_compatible_with，exported_plugins，exports，features，javacopts，licenses，neverlink，plugins，proguard_specs，resource_jars，resource_strip_prefix，restricted_to，runtime_deps，tags，testonly，能见度） 此规则编译源并将源链接到.jar文件中。 隐含的输出目标libname.jar：包含类文件的Java归档文件。libname-src.jar：包含源（“源jar”）的存档。参数属性nameName; required 此目标的唯一名称。 depsList of labels; optional 要链接到此库的库列表。看到一般的评论deps在 共同所有的生成规则属性 。由java_library列出的规则构建的jar deps将位于此规则的编译时类路径中。此外，它们的传递闭包 deps，runtime_deps并将exports在运行时类路径上。 相比之下，data属性中的目标包含在运行文件中，但既不包含在编译时也不包含在运行时类路径中。 srcsList of labels; optional 处理以创建目标的源文件列表。几乎总是需要此属性; 看下面的例外。.java编译 类型的源文件。在生成.java文件的情况下， 通常建议在此处放置生成规则的名称，而不是文件本身的名称。这不仅提高了可读性，而且使规则对未来的更改更具弹性：如果生成规则将来生成不同的文件，您只需要修复一个地方：outs生成规则。您不应该列出生成规则，deps 因为它是无操作。 类型的源文件.srcjar被解压缩和编译。（如果您需要生成一组.java带有genrule 的文件，这非常有用。） 规则：如果规则（通常genrule或filegroup）生成上面列出的任何文件，它们将以与源文件所述相同的方式使用。 除非main_class属性指定运行时类路径上的类或指定runtime_deps参数，否则几乎总是需要 此参数。 dataList of labels; optional 此库在运行时所需的文件列表。看到一般的评论data在 共同所有的生成规则属性 。在构建时java_library，Bazel不会将这些文件放在任何地方; 如果 data文件是生成文件，那么Bazel会生成它们。构建依赖于此java_libraryBazel 的测试时，data会将文件复制或链接 到runfiles区域。 resourcesList of labels; optional 要包含在Java jar中的数据文件列表。如果指定了资源，它们将与.class编译生成的常用文件一起捆绑在jar中 。jar文件中资源的位置由项目结构决定。Bazel首先查找Maven的 标准目录布局，（“src”目录后跟“资源”目录孙子）。如果找不到，那么Bazel会查找名为“java”或“javatests”的最顶层目录（例如，如果资源处于/x/java/y/java/z，则资源的路径将是y/java/z。此启发式扫描不能被覆盖。 资源可以是源文件或生成的文件。 exported_pluginsList of labels; optional java_plugin要导出到直接依赖于此库的库 的s（例如注释处理器）列表。指定的java_plugins 列表将应用于任何直接依赖于此库的库，就像该库已明确声明这些标签一样plugins。 exportsList of labels; optional 导出的图书馆。此处的列表规则将使它们可用于父规则，就像父项明确依赖于这些规则一样。对于常规（非导出），情况并非如此deps。 简介：规则X可以访问Y中的代码，如果它们之间存在依赖路径，该路径以deps边缘开始，后跟零个或多个 exports边缘。让我们看一些例子来说明这一点。 假设甲取决于乙和乙取决于Ç。在这种情况下，C是A 的传递依赖，因此更改C的源并重建A将正确地重建所有内容。然而，A将无法使用C中的类。为了允许这种情况，要么A必须在其中声明C deps，要么B可以通过在其（B）中声明C来使A（以及可能依赖于A的任何东西）变得更容易。exports 属性。 所有直接父规则都可以使用导出库的闭包。举一个稍微不同的例子：A依赖于B，B依赖于C和D，并且还导出C而不是D.现在A可以访问C但不能访问D.现在，如果C和D导出了一些库，C’和D’，A只能访问C’但不能访问D’。 重要提示：导出的规则不是常规依赖项。坚持前面的例子，如果B导出C并且想要也使用C，它也必须自己列出它 deps。 javacoptsList of strings; optional 此库的额外编译器选项。受“Make变量”替换和 Bourne shell标记化的约束。这些编译器选项在全局编译器选项之后传递给javac。 neverlinkBoolean; optional; default is False 此库是仅应用于编译而不是用于运行时。如果库将在执行期间由运行时环境提供，则很有用。此类库的示例是IDE插件的IDE API或tools.jar标准JDK上运行的任何内容。请注意，neverlink = 1这不会阻止编译器将此库中的材料内联到依赖于它的编译目标中，如Java语言规范所允许的那样（例如， 原始类型的static final常量String或常量类型的常量）。因此，当运行时库与编译库相同时，首选用例是。 如果运行时库与编译库不同，则必须确保它仅在JLS禁止编译器内联的位置（并且必须适用于JLS的所有未来版本）中有所不同。 pluginsList of labels; optional Java编译器插件在编译时运行。java_plugin无论何时构建此规则，都将运行此属性中指定的每个属性。库也可以从使用的依赖项继承插件 exported_plugins。插件生成的资源将包含在此规则的结果jar中。proguard_specsList of labels; optional 用作Proguard规范的文件。这些将描述Proguard使用的规范集。如果指定，它们将android_binary根据此库添加到任何目标。此处包含的文件必须只有幂等规则，即-dontnote，-dontwarn，assumenosideeffects和以-keep开头的规则。其他选项只能出现在 android_binaryproguard_specs中，以确保非重言式合并。resource_jarsList of labels; optional 包含Java资源的档案集。如果指定，这些jar的内容将合并到输出jar中。 resource_strip_prefixString; optional 从Java资源中剥离的路径前缀。如果指定，则从resources 属性中的每个文件中剥离此路径前缀。资源文件不在此目录下是错误的。如果未指定（默认值），则根据与源文件的Java包相同的逻辑确定资源文件的路径。例如，源文件 stuff/java/foo/bar/a.txt位于foo/bar/a.txt。 runtime_depsList of labels; optional 库可用于最终二进制文件或仅在运行时测试。与普通类似deps，这些将出现在运行时类路径中，但与它们不同，不在编译时类路径上。应在此处列出仅在运行时所需的依赖关系。依赖性分析工具应该忽略出现在两个目标 runtime_deps和deps。 123java_lite_proto_library java_lite_proto_library（name，deps，data，compatible_with，deprecation，distribs，exec_compatible_with，features，licenses，restricted_to，tags，testonly，visibility）java_lite_proto_library从.proto文件生成Java代码。 deps必须指出proto_library 规则。 例： 12345678910111213java_library（ name =“lib”， deps = [“：foo”]，）java_lite_proto_library（ name =“foo”， deps = [“：bar”]，）proto_library（ name =“bar”，） 参数属性nameName; required 此目标的唯一名称。 depsList of labels; optional proto_library 生成Java代码 的规则列表。 123java_proto_library java_proto_library（name，deps，data，compatible_with，deprecation，distribs，exec_compatible_with，features，licenses，restricted_to，tags，testonly，visibility）java_proto_library从.proto文件生成Java代码。 deps必须指出proto_library 规则。 例： 12345678910111213java_library（ name =“lib”， deps = [“：foo_java_proto”]，）java_proto_library（ name =“foo_java_proto”， deps = [“：foo_proto”]，）proto_library（ name =“foo_proto”，） 属性nameName; required 此目标的唯一名称。 depsList of labels; optional proto_library 生成Java代码 的规则列表。 java_testjava_test（名称，DEPS，索马里红新月会，数据，资源，ARGS，classpath_resources，compatible_with，create_executable，deploy_manifest_lines，折旧，发行商，exec_compatible_with，功能，片状，javacopts，jvm_flags，发射器，牌照，地方，MAIN_CLASS，插件，resource_jars，resource_strip_prefix，restricted_to，runtime_deps，shard_count，size，stamp，tags，test_class，testonly，timeout，toolchains，use_testrunner，visibility）一个java_test()规则编译一个Java测试。测试是围绕测试代码的二进制包装器。调用测试运行器的main方法而不是正在编译的主类。 隐含的输出目标name.jar：Java存档。name_deploy.jar：适合部署的Java归档文件。（仅在显式请求时才构建。）有关详细信息，请参阅 java_binaryname_deploy.jar输出的 说明。请参阅有关java_binary（）参数的部分。此规则还支持所有测试规则（* _test）共有的所有属性。 例子 1234567891011121314151617java_library（ name =“tests”， srcs = glob（[“*。java”]）， deps = [ “// java的/ COM /富/碱：testResources”， “//的Java / COM /富/测试/ UTIL”， ]）java_test（ name =“AllTests”， size =“small”， runtime_deps = [ “：测试”， “// UTIL / MySQL的”， ]） 参数属性nameName; required 此目标的唯一名称。 depsList of labels; optional 要链接到目标的其他库的列表。看到一般的评论deps在 共同所有的生成规则属性 。srcsList of labels; optional 处理以创建目标的源文件列表。几乎总是需要此属性; 看下面的例外。.java编译 类型的源文件。在生成.java文件的情况下， 通常建议在此处放置生成规则的名称，而不是文件本身的名称。这不仅提高了可读性，而且使规则对未来的更改更具弹性：如果生成规则将来生成不同的文件，您只需要修复一个地方：outs生成规则。您不应该列出生成规则，deps 因为它是无操作。 类型的源文件.srcjar被解压缩和编译。（如果您需要生成一组.java带有genrule 的文件，这非常有用。） 规则：如果规则（通常genrule或filegroup）生成上面列出的任何文件，它们将以与源文件所述相同的方式使用。 除非main_class属性指定运行时类路径上的类或指定runtime_deps参数，否则几乎总是需要 此参数。 resourcesList of labels; optional 要包含在Java jar中的数据文件列表。如果指定了资源，它们将与.class编译生成的常用文件一起捆绑在jar中 。jar文件中资源的位置由项目结构决定。Bazel首先查找Maven的 标准目录布局，（“src”目录后跟“资源”目录孙子）。如果找不到，那么Bazel会查找名为“java”或“javatests”的最顶层目录（例如，如果资源处于/x/java/y/java/z，则资源的路径将是y/java/z。此启发式扫描不能被覆盖。 资源可以是源文件或生成的文件。 classpath_resourcesList of labels; optional 除非没有其他方式，否则不要使用此选项）必须位于Java树根目录的资源列表。此属性的唯一目的是支持第三方库，这些库要求在类路径上找到它们的资源”myconfig.xml”。由于名称空间冲突的危险，它只允许在二进制文件而不是库中。 create_executableBoolean; optional; nonconfigurable; default is True 二进制文件是否可执行。不可执行的二进制文件将传递的运行时Java依赖项收集到部署jar中，但不能直接执行。如果设置了此属性，则不会创建包装器脚本。如果设置了launcher或main_class属性，则将此值设置为0是错误的。deploy_manifest_linesList of strings; optional 要添加到META-INF/manifest.mf为*_deploy.jar目标生成的文件 的行列表。此属性的内容不符合“使变量”替换。javacoptsList of strings; optional 此库的额外编译器选项。受“Make变量”替换和 Bourne shell标记化的约束。这些编译器选项在全局编译器选项之后传递给javac。 jvm_flagsList of strings; optional 要在运行此二进制文件时生成的包装器脚本中嵌入的标志列表。受$（位置）和 “Make变量”替换，以及 Bourne shell标记化。Java二进制文件的包装器脚本包括一个CLASSPATH定义（用于查找所有相关的jar）并调用正确的Java解释器。包装器脚本生成的命令行包含主类的名称，后跟a，”$@”因此您可以在类名后传递其他参数。但是，必须在命令行上的类名之前指定用于由JVM进行解析的参数。在jvm_flags列出类名之前，将内容添加到包装脚本中。 请注意，此属性对 输出没有影响*_deploy.jar。 launcherLabel; optional 指定将用于运行Java程序的二进制文件，而不是bin/javaJDK附带的普通程序。目标必须是a cc_binary。任何cc_binary实现 Java Invocation API的都可以指定为此属性的值。默认情况下，Bazel将使用普通的JDK启动程序（bin / java或java.exe）。 相关的 –java_launcherBazel标志仅影响未指定属性的那些 java_binary和java_test目标 。launcher 请注意，根据您使用的是JDK启动程序还是其他启动程序，您的本机（C ++，SWIG，JNI）依赖项的构建方式会有所不同： 如果您使用的是普通的JDK启动程序（默认），则将本机依赖项构建为名为的共享库{name}_nativedeps.so，其中 {name}是name此java_binary规则的属性。在此配置中，链接器不会删除未使用的代码。如果您正在使用任何其他启动程序，则本机（C ++）依赖项将静态链接到名为的二进制文件{name}_nativedeps，其中此java_binary规则{name} 的name属性。在这种情况下，链接器将从生成的二进制文件中删除它认为未使用的任何代码，这意味着除非该cc_library目标指定，否则任何仅通过JNI访问的C ++代码都可能无法链接alwayslink = 1。使用除默认JDK启动程序以外的任何启动程序时，*_deploy.jar输出的格式会更改。有关详细信息，请参阅主 java_binary文档。 main_classString; optional 具有main()用作入口点的方法的类的名称。如果规则使用此选项，则不需要srcs=[…]列表。因此，使用此属性，可以从已包含一个或多个main()方法的Java库生成可执行文件。此属性的值是类名，而不是源文件。该类必须在运行时可用：它可以由此规则（从srcs）编译，或由直接或传递依赖（通过runtime_deps或 deps）提供。如果该类不可用，则二进制文件将在运行时失败; 没有构建时间检查。 pluginsList of labels; optional Java编译器插件在编译时运行。java_plugin无论何时构建此规则，都将运行此属性中指定的每个属性。库也可以从使用的依赖项继承插件 exported_plugins。插件生成的资源将包含在此规则的结果jar中。resource_jarsList of labels; optional 包含Java资源的档案集。如果指定，这些jar的内容将合并到输出jar中。 resource_strip_prefixString; optional 从Java资源中剥离的路径前缀。如果指定，则从resources 属性中的每个文件中剥离此路径前缀。资源文件不在此目录下是错误的。如果未指定（默认值），则根据与源文件的Java包相同的逻辑确定资源文件的路径。例如，源文件 stuff/java/foo/bar/a.txt位于foo/bar/a.txt。 runtime_depsList of labels; optional 库可用于最终二进制文件或仅在运行时测试。与普通类似deps，这些将出现在运行时类路径中，但与它们不同，不在编译时类路径上。应在此处列出仅在运行时所需的依赖关系。依赖性分析工具应该忽略出现在两个目标 runtime_deps和deps。stampInteger; optional; default is 0 启用链接标记。是否将构建信息编码为二进制文件。可能的值：stamp = 1：将构建信息标记到二进制文件中。只有在依赖项发生变化时才会重建标记的二进制文件。如果存在依赖于构建信息的测试，请使用此选项。stamp = 0：始终使用常量值替换构建信息。这提供了良好的构建结果缓存。stamp = -1：嵌入构建信息由 - [no]戳标志控制。test_classString; optional 由测试运行器加载的Java类。默认情况下，如果未定义此参数，则使用传统模式并使用测试参数。将–nolegacy_bazel_java_test标志设置为不在第一个参数上回退。 此属性指定此测试要运行的Java类的名称。很少需要设置它。如果省略此参数，则将使用目标name及其source-root-relative路径推断出该参数。如果测试位于已知源根目录之外，则Bazel将报告错误（如果test_class 未设置）。 对于JUnit3，测试类需要是子类， junit.framework.TestCase或者需要有一个suite()返回junit.framework.Test（或子类Test）的公共静态方法 。对于JUnit4，该类需要注释 org.junit.runner.RunWith。 此属性允许多个java_test规则共享相同的Test （TestCase，TestSuite，…）。通常将附加信息传递给它（例如通过jvm_flags=[‘-Dkey=value’]），以便其行为在每种情况下都不同，例如运行不同的测试子集。此属性还允许在javatests树外使用Java测试。 toolchainsList of labels; optional 提供“Make variables”的工具链集 ，该目标可以在其某些属性中使用。一些规则具有工具链，其默认情况下可以使用Make变量。use_testrunnerBoolean; optional; default is True 使用测试运行器（默认情况下 com.google.testing.junit.runner.BazelTestRunner）类作为Java程序的主入口点，并将测试类作为bazel.test_suite 系统属性的值提供给测试运行器。您可以使用它来覆盖默认行为，即使用测试运行器获取 java_test规则，而不是将其用于java_binary规则。你不太可能想要这样做。一种用途是用于AllTest 由另一个规则调用的规则（例如，在运行测试之前设置数据库）。该AllTest 规则必须被声明为java_binary，但仍应使用测试运行器作为其主要入口点。可以使用main_classattribute 覆盖测试运行器类的名称。 java_package_configurationjava_package_configuration（name，data，compatible_with，deprecation，distribs，features，javacopts，licenses，packages，restricted_to，tags，testonly，visibility）要应用于一组包的配置。配置可以添加到 java_toolchain.javacoptss。 例： 1234567891011121314151617181920java_package_configuration（ name =“my_configuration”， packages = [“：my_packages”]， javacopts = [“ - Werror”]，）package_group（ name =“my_packages”， 包裹= [ “// COM /我/项目/ ...” “ - // COM /我/项目/测试/ ...” ]）java_toolchain（ ... package_configuration = [ “：my_configuration” ]） 参数属性nameName; required 此目标的唯一名称。 dataList of labels; optional 此配置在运行时所需的文件列表。javacoptsList of strings; optional Java编译器标志。packagesList of labels; optional package_group应该应用配置 集。 java_pluginjava_plugin（name，deps，srcs，data，resources，compatible_with，deprecation，distribs，exec_compatible_with，features，generate_api，javacopts，licenses，neverlink，output_licenses，plugins，processor_class，proguard_specs，resource_jars，resource_strip_prefix，restricted_to，tags，testonly，能见度）java_plugin为Bazel运行的Java编译器定义插件。目前，唯一受支持的插件类型是注释处理器。A java_library或 java_binary规则可以通过plugins 属性依赖于它们来运行插件。A java_library还可以自动将插件导出到直接依赖它的库 exported_plugins。 隐含的输出目标libname.jar：Java存档。java_library除了添加processor_class参数外， 参数与之相同。 参数属性nameName; required 此目标的唯一名称。 depsList of labels; optional 要链接到此库的库列表。看到一般的评论deps在 共同所有的生成规则属性 。由java_library列出的规则构建的jar deps将位于此规则的编译时类路径中。此外，它们的传递闭包 deps，runtime_deps并将exports在运行时类路径上。 相比之下，data属性中的目标包含在运行文件中，但既不包含在编译时也不包含在运行时类路径中。 srcsList of labels; optional 处理以创建目标的源文件列表。几乎总是需要此属性; 看下面的例外。.java编译 类型的源文件。在生成.java文件的情况下， 通常建议在此处放置生成规则的名称，而不是文件本身的名称。这不仅提高了可读性，而且使规则对未来的更改更具弹性：如果生成规则将来生成不同的文件，您只需要修复一个地方：outs生成规则。您不应该列出生成规则，deps 因为它是无操作。 类型的源文件.srcjar被解压缩和编译。（如果您需要生成一组.java带有genrule 的文件，这非常有用。） 规则：如果规则（通常genrule或filegroup）生成上面列出的任何文件，它们将以与源文件所述相同的方式使用。 除非main_class属性指定运行时类路径上的类或指定runtime_deps参数，否则几乎总是需要 此参数。 dataList of labels; optional 此库在运行时所需的文件列表。看到一般的评论data在 共同所有的生成规则属性 。在构建时java_library，Bazel不会将这些文件放在任何地方; 如果 data文件是生成文件，那么Bazel会生成它们。构建依赖于此java_libraryBazel 的测试时，data会将文件复制或链接 到runfiles区域。 resourcesList of labels; optional 要包含在Java jar中的数据文件列表。如果指定了资源，它们将与.class编译生成的常用文件一起捆绑在jar中 。jar文件中资源的位置由项目结构决定。Bazel首先查找Maven的 标准目录布局，（“src”目录后跟“资源”目录孙子）。如果找不到，那么Bazel会查找名为“java”或“javatests”的最顶层目录（例如，如果资源处于/x/java/y/java/z，则资源的路径将是y/java/z。此启发式扫描不能被覆盖。 资源可以是源文件或生成的文件。 generates_apiBoolean; optional; default is False 此属性标记生成API代码的注释处理器。如果规则使用API 生成注释处理器，则只有在生成规则之后调度其编译操作时，依赖于它的其他规则才能引用生成的代码。当启用了–java_header_compilation时，此属性指示Bazel引入调度约束。 警告：此属性会影响构建性能，仅在必要时使用它。 javacoptsList of strings; optional 此库的额外编译器选项。受“Make变量”替换和 Bourne shell标记化的约束。这些编译器选项在全局编译器选项之后传递给javac。 neverlinkBoolean; optional; default is False 此库是仅应用于编译而不是用于运行时。如果库将在执行期间由运行时环境提供，则很有用。此类库的示例是IDE插件的IDE API或tools.jar标准JDK上运行的任何内容。请注意，neverlink = 1这不会阻止编译器将此库中的材料内联到依赖于它的编译目标中，如Java语言规范所允许的那样（例如， 原始类型的static final常量String或常量类型的常量）。因此，当运行时库与编译库相同时，首选用例是。 如果运行时库与编译库不同，则必须确保它仅在JLS禁止编译器内联的位置（并且必须适用于JLS的所有未来版本）中有所不同。 output_licensesLicence type; optional 看到 common attributespluginsList of labels; optional Java编译器插件在编译时运行。java_plugin无论何时构建此规则，都将运行此属性中指定的每个属性。库也可以从使用的依赖项继承插件 exported_plugins。插件生成的资源将包含在此规则的结果jar中。processor_classString; optional 处理器类是Java编译器应该用作注释处理器入口点的类的完全限定类型。如果未指定，此规则将不会为Java编译器的注释处理提供注释处理器，但其运行时类路径仍将包含在编译器的注释处理器路径中。（这主要供 Error Prone插件使用，它使用 java.util.ServiceLoader从注释处理器路径加载 。）proguard_specsList of labels; optional 用作Proguard规范的文件。这些将描述Proguard使用的规范集。如果指定，它们将android_binary根据此库添加到任何目标。此处包含的文件必须只有幂等规则，即-dontnote，-dontwarn，assumenosideeffects和以-keep开头的规则。其他选项只能出现在 android_binaryproguard_specs中，以确保非重言式合并。resource_jarsList of labels; optional 包含Java资源的档案集。如果指定，这些jar的内容将合并到输出jar中。 resource_strip_prefixString; optional 从Java资源中剥离的路径前缀。如果指定，则从resources 属性中的每个文件中剥离此路径前缀。资源文件不在此目录下是错误的。如果未指定（默认值），则根据与源文件的Java包相同的逻辑确定资源文件的路径。例如，源文件 stuff/java/foo/bar/a.txt位于foo/bar/a.txt。java_runtimejava_runtime（name，srcs，compatible_with，deprecation，distribs，features，java，java_home，licenses，restricted_to，tags，testonly，visibility）指定Java运行时的配置。 例： 12345java_runtime（ name =“jdk-9-ea + 153”， srcs = glob（[“jdk9-ea + 153 / **”]）， java_home =“jdk9-ea + 153”，） 参数属性nameName; required 此目标的唯一名称。 srcsList of labels; optional 运行时中的所有文件。javaLabel; optional java可执行文件的路径。java_homeString; optional 运行时根目录的路径。受“制造”变量替代。如果此路径是绝对路径，则该规则表示具有众所周知路径的非密集Java运行时。在那种情况下，srcs和 属性必须为空。java_toolchainjava_toolchain（名称，启动类路径，compatible_with，折旧，发行商，extclasspath，功能，forcibly_disable_header_compilation，genclass，header_compiler，header_compiler_direct，ijar，jacocorunner，javabuilder，javabuilder_jvm_opts，javac的，javac_supports_workers，javacopts，jvm_opts，许可证，oneversion，oneversion_whitelist，package_configuration，resourcejar，restricted_to，singlejar，source_version，tags，target_version，testonly，timezone_data，tools，visibility，xlint）指定Java编译器的配置。可以通过–java_toolchain参数更改要使用的工具链。通常，除非要调整Java编译器，否则不应编写这些规则。 例子一个简单的例子是： 123456789java_toolchain（ name =“toolchain”， source_version =“7”， target_version =“7”， bootclasspath = [“// tools / jdk：bootclasspath”]， xlint = [“classfile”，“divzero”，“empty”，“options”，“path”]， javacopts = [“ - g”]， javabuilder =“：JavaBuilder_deploy.jar”，） 参数属性nameName; required 此目标的唯一名称。 bootclasspathList of labels; optional Java目标bootclasspath条目。对应于javac的-bootclasspath标志。extclasspathList of labels; optional Java目标extdir条目。对应于javac的-extdir标志。forcibly_disable_header_compilationBoolean; optional; default is False 覆盖–java_header_compilation以在不支持它的平台上禁用标头编译，例如JDK 7 Bazel。genclassList of labels; required GenClass部署jar的标签。header_compilerList of labels; optional 标头编译器的标签。如果启用了–java_header_compilation，则为必需。header_compiler_directList of labels; optional 标头编译器的可选标签，用于不包含任何API生成注释处理器的直接类路径操作。此工具不支持注释处理。 ijarList of labels; required ijar可执行文件的标签。jacocorunnerLabel; optional JacocoCoverageRunner部署jar的标签。javabuilderList of labels; required JavaBuilder部署jar的标签。javabuilder_jvm_optsList of strings; optional 调用JavaBuilder时JVM的参数列表。javacList of labels; optional javac jar的标签。javac_supports_workersBoolean; optional; default is True 如果JavaBuilder支持作为持久性工作程序运行，则为True，否则为false。javacoptsList of strings; optional Java编译器的额外参数列表。有关可能的Java编译器标志的详尽列表，请参阅Java编译器文档。jvm_optsList of strings; optional 调用Java编译器时JVM的参数列表。有关此选项的可能标志的详细列表，请参阅Java虚拟机文档。oneversionLabel; optional 单版本强制二进制文件的标签。oneversion_whitelistLabel; optional 单版本白名单的标签。package_configurationList of labels; optional 应该应用于指定包组的配置。resourcejarList of labels; optional 资源jar构建器可执行文件的标签。singlejarList of labels; required SingleJar部署jar的标签。source_versionString; optional Java源代码版本（例如，’6’或’7’）。它指定Java源代码中允许哪组代码结构。target_versionString; optional Java目标版本（例如，’6’或’7’）。它指定应该为哪个Java运行时构建类。timezone_dataLabel; optional 包含时区数据的资源jar的标签。如果设置，则时区数据将作为所有java_binary规则的隐式运行时依赖项添加。toolsList of labels; optional 可用于jvm_opts中标签扩展的工具标签。xlintList of strings; optional 要添加或从默认列表中删除的警告列表。用破折号在它之前删除它。有关更多信息，请参阅-Xlint选项上的Javac文档。","link":"/2019/07/30/BUILD/"},{"title":"@ConditionalOnClass","text":"Spring的新引入的注解@ConditionalOnClass是Springboot实现自动配置的重要支撑之一。其用途是判断当前classpath下是否存在指定类，若是则将当前的配置装载入spring容器。举例来说，如果在maven中引入了velocity，那么视图就使用velocity，若引入的是freemarker，则使用freemarker. 但是眼见为虚，手敲为实，所以自己决定来验证下其使用。 场景设在新日暮里，主角是van，是一个喜爱摔跤的格斗爱好者，他将向新日暮里的其他选手发起友谊比赛。这里，有两位新日暮里的强者，一位是新日暮里王，billy；另一位来自自由的大美利坚，香蕉君banana。van急于找人摔跤，因此只要有人在，他就发起挑战。 入口类： 123456789101112131415161718192021222324package com.ff.fun;import com.ff.fun.player.Van;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.CommandLineRunner; import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplicationpublic class SbfunApplication implements CommandLineRunner{ @Autowired private Van van; public static void main(String[] args) { SpringApplication.run(SbfunApplication.class, args); } @Override public void run(String... args) throws Exception { //do something van.fight(); }} 没什么好多说的，我们的主角Van开始了格斗。 主角Van： 1234567891011121314151617package com.ff.fun.player; import com.ff.fun.fighter.Fighter; public class Van { private Fighter fighter; public Van(Fighter fighter) { this.fighter = fighter; } public void fight(){ System.out.println(\"van：boy next door,do you like 玩游戏\"); fighter.fight(); }} 新日暮里格斗家们： 123456package com.ff.fun.fighter; public interface Fighter { void fight();} 12345678package com.ff.fun.fighter; public class Banana implements Fighter { @Override public void fight() { System.out.println(\"banana:自由的气息，蕉迟但到\"); }} 123456789package com.ff.fun.fighter; public class Billy implements Fighter{ @Override public void fight(){ System.out.println(\"billy：吾乃新日暮里的王，三界哲学的主宰。\"); }} Config： 1234567891011121314151617181920212223package com.ff.fun.config; import com.ff.fun.player.Van;import com.ff.fun.fighter.Billy;import com.ff.fun.fighter.Fighter;import org.springframework.boot.autoconfigure.condition.ConditionalOnClass;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration; @Configuration@ConditionalOnClass({Billy.class})public class VanConfig { @Bean public Fighter billy(){ return new Billy(); } @Bean public Van van(){ return new Van(billy()); }} 1234567891011121314151617181920212223package com.ff.fun.config; import com.ff.fun.player.Van;import com.ff.fun.fighter.Banana;import com.ff.fun.fighter.Fighter;import org.springframework.boot.autoconfigure.condition.ConditionalOnClass;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration; @Configuration@ConditionalOnClass({Banana.class})public class Van2Config { @Bean public Fighter banana(){ return new Banana(); } @Bean public Van van(){ return new Van(banana()); }} 这就是重点了，这两个带条件的配置类，一个是当Billy在的时候启用，一个是在当Banana在的时候启用试验环节：编译后，在target中，可见 这两个类，这时候如果直接执行程序，会输出： 12van：boy next door,do you like 玩游戏billy：吾乃新日暮里的王，三界哲学的主宰。 说明，当候选类都在的情况下，spring会挑其中之一（至于如何选择的得另行研究）； 这时候，删掉Billy.class,让吾王下线，新日暮里就只剩下Banana一个哲学战士了，此时，再次运行，输出为： 12van：boy next door,do you like 玩游戏banana:自由的气息，蕉迟但到 可以看到，billy的配置没有加载，van的对手是banana，@ConditionalOnClass这个注解起到了选择Config的作用。 此时，如果将香蕉君也删掉，新日暮里空无一人(⊙︿⊙)，此时运行结果为： 1234567891011121314***************************APPLICATION FAILED TO START*************************** Description: Field van in com.ff.fun.SbfunApplication required a bean of type 'com.ff.fun.player.Van' that could not be found. - Bean method 'van' not loaded because @ConditionalOnClass did not find required class 'com.ff.fun.fighter.Banana' - Bean method 'van' not loaded because @ConditionalOnClass did not find required class 'com.ff.fun.fighter.Billy' Action: Consider revisiting the conditions above or defining a bean of type 'com.ff.fun.player.Van' in your configuration.","link":"/2019/09/04/ConditionalOnClass/"},{"title":"@ConditionalOnProperty","text":"@ConditionalOnProperty 问题在最近的项目中遇到一个实际问题，该项目要与老项目整合，但是该项目与老项目用的数据库不是同一个，因此要做数据库同步。由于数据库同步与正常业务解耦，仅仅依赖该项目处理后的数据，再加上数据库同步用的Oracle，因此打算在dev版本上面不加入数据库同步，在test与prod版本上加入数据库同步。这样就要求在dev版本下，对第二个数据源的配置不生效；而test与prod版本下，第二个数据源生效。 解决方案经过一番寻觅，发现了Spring boot中有个注解@ConditionalOnProperty，这个注解能够控制某个configuration是否生效。具体操作是通过其两个属性name以及havingValue来实现的，其中name用来从application.properties中读取某个属性值，如果该值为空，则返回false;如果值不为空，则将该值与havingValue指定的值进行比较，如果一样则返回true;否则返回false。如果返回值为false，则该configuration不生效；为true则生效。 代码: 123456789101112@Configuration//如果synchronize在配置文件中并且值为true@ConditionalOnProperty(name = \"synchronize\", havingValue = \"true\")public class SecondDatasourceConfig { @Bean(name = \"SecondDataSource\") @Qualifier(\"SecondDataSource\") @ConfigurationProperties(prefix = \"spring.second.datasource\") public DataSource jwcDataSource() { return DataSourceBuilder.create().build(); }}","link":"/2019/09/04/ConditionalOnProperty/"},{"title":"DAO和DTO","text":"DTO(data transfer object):数据传输对象，以前被称为值对象(VO,value object)，作用仅在于在应用程序的各个子系统间传输数据，在表现层展示。与POJO对应一个数据库实体不同，DTO并不对应一个实体，可能仅存储实体的部分属性或加入符合传输需求的其他的属性。 DAO(data access object):数据访问对象。提供访问数据库的抽象接口，或者持久化机制，而不暴露数据库的内部详细信息。DAO提供从程序调用到持久层的匹配。 DTO即数据传输对象。之前不明白有些框架中为什么要专门定义DTO来绑定表现层中的数据，为什么不能直接用实体模型呢，有了DTO同时还要维护DTO与Model之间的映射关系，多麻烦。 摘两个比较有意义的段落。 表现层与应用层之间是通过数据传输对象（DTO）进行交互的，数据传输对象是没有行为的POCO对象，它 的目的只是为了对领域对象进行数据封装，实现层与层之间的数据传递。为何不能直接将领域对象用于 数据传递？因为领域对象更注重领域，而DTO更注重数据。不仅如此，由于“富领域模型”的特点，这样 做会直接将领域对象的行为暴露给表现层。 需要了解的是，数据传输对象DTO本身并不是业务对象。数据传输对象是根据UI的需求进行设计的，而不 是根据领域对象进行设计的。比如，Customer领域对象可能会包含一些诸如FirstName, LastName, Email, Address等信息。但如果UI上不打算显示Address的信息，那么CustomerDTO中也无需包含这个 Address的数据 简单来说Model面向业务，我们是通过业务来定义Model的。而DTO是面向界面UI，是通过UI的需求来定义的。通过DTO我们实现了表现层与Model之间的解耦，表现层不引用Model，如果开发过程中我们的模型改变了，而界面没变，我们就只需要改Model而不需要去改表现层中的东西。","link":"/2019/06/27/DAO和DTO/"},{"title":"DatabaseProxySearch","text":"mybatis学习心得1.What do I have?我有什么 2.SqlMapCofing.xml(1)从配置文件中的property中我们能知道数据库连接信息，有了这些信息就可以创建Connection对象(2)mappers有了mapper就有了映射配置信息,就能读到下面IUserDao.xml文件 3.IUserDao.xml有了namespace的全限定类名和 有了id的方法名能唯一确定执行哪条sql有了sql语句就可以获取PreparedStatement有了resultType就可以知道结果封装到哪里 读取配置文件：用到的技术就是解析XML的技术我们此处用到的技术是dom4j解析xml技术 接下来1.根据配置文件的信息创建Connection对象注册驱动，获取链接2.获取预处理对象PreparedStatement此时需要sql语句conn.preparedStatement(sql);得到PreparedStatementsql就从配置文件中来3.执行查询preparedStatement.execute();得到ResultSet4.遍历结果集用于封装List list = new ArrayList();while（resultSet.next()）{ E element = xxx; xxx怎么得到？这里用到的技术是反射 是根据全限定类名resultType通过反射得到的 实例化一个对象出来 即: E element = （E）Class.forName(配置的全限定类名).newInstance(); 这里需要强转一下，因为得到的是一个obj对象 进行封装，把每个xx的内容都添加到element中 我们的实体类属性和表中的列名是一致的， 于是我们就可以把表的列名看成是实体类的属性名称 就可以使用反射的方式根据名称获取每个属性，并把值赋进去。 把element加到list中 list.add(element);}; 5.返回list return list; 注意配置文件位置使用绝对路径或相对路径都不好文件位置只能使用两种方式第一个：使用类加载器，它只能读取类路径的配置文件Resources.getResourceAsStream(配置文件)得到输入流InputStream第二个：使用servletContext对象的getRealPath()获取部署在环境里的真实路径创建工厂mybatis使用了构建者模式SqlSessionFactoryBuilder把输入流给构建者builder会给我们建好工厂sqlsessionfactory工厂给我们产生SqlSession这里使用的是工厂模式降低类之间的依赖关系即解藕通过sqlsession代理模式getMapper创建Dao接口实现类getMapper 都做了什么？类加载器它使用的和被代理的对象是相同的类加载器代理对象要实现的接口：和被代理对象实现相同的接口如何代理：它就是增强的方法，我们需要自己来提供此处是一个InvocationHandler的接口，我们需要些一个该接口的实现类在实现类中调用selectList方法Proxy.newProxyInstance(类加载器，代理对象要实现的接口字节码数组，如何代理)","link":"/2019/07/11/DatabaseProxySearch/"},{"title":"@Configuration","text":"从Spring3.0，@Configuration用于定义配置类，可替换xml配置文件，被注解的类内部包含有一个或多个被@Bean注解的方法，这些方法将会被AnnotationConfigApplicationContext或AnnotationConfigWebApplicationContext类进行扫描，并用于构建bean定义，初始化Spring容器。 ** 注意：@Configuration注解的配置类有如下要求：** @Configuration不可以是final类型；@Configuration不可以是匿名类；嵌套的configuration必须是静态类。 一、用@Configuration加载spring1.1、@Configuration配置spring并启动spring容器1.2、@Configuration启动容器+@Bean注册Bean1.3、@Configuration启动容器+@Component注册Bean1.4、使用 AnnotationConfigApplicationContext 注册 AppContext 类的两种方法1.5、配置Web应用程序(web.xml中配置AnnotationConfigApplicationContext) 二、组合多个配置类2.1、在@configuration中引入spring的xml配置文件2.2、在@configuration中引入其它注解配置2.3、@configuration嵌套（嵌套的Configuration必须是静态类） 三、@EnableXXX注解四、@Profile逻辑组配置五、使用外部变量 一、@Configuation加载Spring方法1.1、@Configuration配置spring并启动spring容器@Configuration标注在类上，相当于把该类作为spring的xml配置文件中的，作用为：配置spring容器(应用上下文) 12345678910package com.dxz.demo.configuration;import org.springframework.context.annotation.Configuration;@Configurationpublic class TestConfiguration { public TestConfiguration() { System.out.println(\"TestConfiguration容器启动初始化。。。\"); }} 相当于： 1234567891011121314&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:jdbc=\"http://www.springframework.org/schema/jdbc\" xmlns:jee=\"http://www.springframework.org/schema/jee\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:util=\"http://www.springframework.org/schema/util\" xmlns:task=\"http://www.springframework.org/schema/task\" xsi:schemaLocation=\" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc-4.0.xsd http://www.springframework.org/schema/jee http://www.springframework.org/schema/jee/spring-jee-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-4.0.xsd\" default-lazy-init=\"false\"&gt;&lt;/beans&gt; 主方法中测试 12345678910111213141516package com.dxz.demo.configuration;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;public class TestMain { public static void main(String[] args) { // @Configuration注解的spring容器加载方式，用AnnotationConfigApplicationContext替换ClassPathXmlApplicationContext ApplicationContext context = new AnnotationConfigApplicationContext(TestConfiguration.class); // 如果加载spring-context.xml文件： // ApplicationContext context = new // ClassPathXmlApplicationContext(\"spring-context.xml\"); }} 从运行主方法结果可以看出，spring容器已经启动了： 1.2、@Configuration启动容器+@Bean注册Bean，@Bean下管理bean的生命周期@Bean标注在方法上(返回某个实例的方法)，等价于spring的xml配置文件中的，作用为：注册bean对象 bean类： 123456789101112131415161718192021222324package com.dxz.demo.configuration;public class TestBean { private String username; private String url; private String password; public void sayHello() { System.out.println(\"TestBean sayHello...\"); } public String toString() { return \"username:\" + this.username + \",url:\" + this.url + \",password:\" + this.password; } public void start() { System.out.println(\"TestBean 初始化。。。\"); } public void cleanUp() { System.out.println(\"TestBean 销毁。。。\"); }} 配置类： 1234567891011121314151617181920package com.dxz.demo.configuration;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Scope;@Configurationpublic class TestConfiguration { public TestConfiguration() { System.out.println(\"TestConfiguration容器启动初始化。。。\"); } // @Bean注解注册bean,同时可以指定初始化和销毁方法 // @Bean(name=\"testBean\",initMethod=\"start\",destroyMethod=\"cleanUp\") @Bean @Scope(\"prototype\") public TestBean testBean() { return new TestBean(); }} 主方法测试类： 1234567891011121314151617181920package com.dxz.demo.configuration;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;public class TestMain { public static void main(String[] args) { // @Configuration注解的spring容器加载方式，用AnnotationConfigApplicationContext替换ClassPathXmlApplicationContext ApplicationContext context = new AnnotationConfigApplicationContext(TestConfiguration.class); // 如果加载spring-context.xml文件： // ApplicationContext context = new // ClassPathXmlApplicationContext(\"spring-context.xml\"); //获取bean TestBean tb = (TestBean) context.getBean(\"testBean\"); tb.sayHello(); }} 结果： 注：(1)、@Bean注解在返回实例的方法上，如果未通过@Bean指定bean的名称，则默认与标注的方法名相同；(2)、@Bean注解默认作用域为单例singleton作用域，可通过@Scope(“prototype”)设置为原型作用域；(3)、既然@Bean的作用是注册bean对象，那么完全可以使用@Component、@Controller、@Service、@Ripository等注解注册bean，当然需要配置@ComponentScan注解进行自动扫描。 @Bean下管理bean的生命周期可以使用基于 Java 的配置来管理 bean 的生命周期。@Bean 支持两种属性，即 initMethod 和destroyMethod，这些属性可用于定义生命周期方法。在实例化 bean 或即将销毁它时，容器便可调用生命周期方法。生命周期方法也称为回调方法，因为它将由容器调用。使用 @Bean 注释注册的 bean 也支持 JSR-250 规定的标准 @PostConstruct 和 @PreDestroy 注释。如果您正在使用 XML 方法来定义 bean，那么就应该使用 bean 元素来定义生命周期回调方法。以下代码显示了在 XML 配置中通常使用 bean 元素定义回调的方法。 1234567891011121314@Configuration@ComponentScan(basePackages = \"com.dxz.demo.configuration\")public class TestConfiguration { public TestConfiguration() { System.out.println(\"TestConfiguration容器启动初始化。。。\"); } //@Bean注解注册bean,同时可以指定初始化和销毁方法 @Bean(name=\"testBean\",initMethod=\"start\",destroyMethod=\"cleanUp\") @Scope(\"prototype\") public TestBean testBean() { return new TestBean(); }} 启动类： 1234567891011121314public class TestMain { public static void main(String[] args) { ApplicationContext context = new AnnotationConfigApplicationContext(TestConfiguration.class); TestBean tb = (TestBean) context.getBean(\"testBean\"); tb.sayHello(); System.out.println(tb); TestBean tb2 = (TestBean) context.getBean(\"testBean\"); tb2.sayHello(); System.out.println(tb2); }} 结果： 分析： 结果中的1：表明initMethod生效 结果中的2：表明@Scope(“prototype”)生效 1.3、@Configuration启动容器+@Component注册Beanbean类： 12345678910111213141516171819202122232425262728package com.dxz.demo.configuration;import org.springframework.stereotype.Component;//添加注册bean的注解@Componentpublic class TestBean { private String username; private String url; private String password; public void sayHello() { System.out.println(\"TestBean sayHello...\"); } public String toString() { return \"username:\" + this.username + \",url:\" + this.url + \",password:\" + this.password; } public void start() { System.out.println(\"TestBean 初始化。。。\"); } public void cleanUp() { System.out.println(\"TestBean 销毁。。。\"); }} 配置类： 1234567891011121314151617181920212223package com.dxz.demo.configuration;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Scope;@Configuration//添加自动扫描注解，basePackages为TestBean包路径@ComponentScan(basePackages = \"com.dxz.demo.configuration\")public class TestConfiguration { public TestConfiguration() { System.out.println(\"TestConfiguration容器启动初始化。。。\"); } /*// @Bean注解注册bean,同时可以指定初始化和销毁方法 // @Bean(name=\"testNean\",initMethod=\"start\",destroyMethod=\"cleanUp\") @Bean @Scope(\"prototype\") public TestBean testBean() { return new TestBean(); }*/} 主方法测试获取bean对象： 1234567891011121314151617181920package com.dxz.demo.configuration;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;public class TestMain { public static void main(String[] args) { // @Configuration注解的spring容器加载方式，用AnnotationConfigApplicationContext替换ClassPathXmlApplicationContext ApplicationContext context = new AnnotationConfigApplicationContext(TestConfiguration.class); // 如果加载spring-context.xml文件： // ApplicationContext context = new // ClassPathXmlApplicationContext(\"spring-context.xml\"); //获取bean TestBean tb = (TestBean) context.getBean(\"testBean\"); tb.sayHello(); }} sayHello()方法都被正常调用。 1.4、使用 AnnotationConfigApplicationContext 注册 AppContext 类的两种方法1.4.1、 配置类的注册方式是将其传递给 AnnotationConfigApplicationContext 构造函数 123456789public static void main(String[] args) { // @Configuration注解的spring容器加载方式，用AnnotationConfigApplicationContext替换ClassPathXmlApplicationContext ApplicationContext context = new AnnotationConfigApplicationContext(TestConfiguration.class); //获取bean TestBean tb = (TestBean) context.getBean(\"testBean\"); tb.sayHello(); } 1.4.2、 AnnotationConfigApplicationContext 的register 方法传入配置类来注册配置类 1234public static void main(String[] args) { ApplicationContext ctx = new AnnotationConfigApplicationContext(); ctx.register(AppContext.class)} 1.5、配置Web应用程序(web.xml中配置AnnotationConfigApplicationContext)过去，您通常要利用 XmlWebApplicationContext 上下文来配置 Spring Web 应用程序，即在 Web 部署描述符文件 web.xml 中指定外部 XML 上下文文件的路径。XMLWebApplicationContext 是 Web 应用程序使用的默认上下文类。以下代码描述了 web.xml 中指向将由 ContextLoaderListener 监听器类载入的外部 XML 上下文文件的元素。 12345678910111213141516171819&lt;web-app&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt; org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;sampleServlet&lt;/servlet-name&gt; &lt;servlet-class&gt; org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;/servlet&gt;...&lt;/web-app&gt; 现在，您要将 web.xml 中的上述代码更改为使用 AnnotationConfigApplicationContext 类。切记，XmlWebApplicationContext 是 Spring 为 Web 应用程序使用的默认上下文实现，因此您永远不必在您的web.xml 文件中显式指定这个上下文类。现在，您将使用基于 Java 的配置，因此在配置 Web 应用程序时，需要在web.xml 文件中指定 AnnotationConfigApplicationContext 类。上述代码将修改如下： 1234567891011121314151617181920212223242526272829303132333435&lt;web-app&gt; &lt;context-param&gt; &lt;param-name&gt;contextClass&lt;/param-name&gt; &lt;param-value&gt; org.springframework.web.context. support.AnnotationConfigWebApplicationContext &lt;/param-value&gt; &lt;/context-param&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt; demo.AppContext &lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt; org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;sampleServlet&lt;/servlet-name&gt; &lt;servlet-class&gt; org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextClass&lt;/param-name&gt; &lt;param-value&gt; org.springframework.web.context. support.AnnotationConfigWebApplicationContext &lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt;...&lt;/web-app&gt; 以上修改后的 web.xml 现在定义了 AnnotationConfigWebApplicationContext 上下文类，并将其作为上下文参数和 servlet 元素的一部分。上下文配置位置现在指向 AppContext 配置类。这非常简单。下一节将演示 bean 的生命周期回调和范围的实现。 1.6、@Configuation总结 @Configuation等价于 @Bean等价于 @ComponentScan等价于&lt;context:component-scan base-package=”com.dxz.demo”/&gt; 二、组合多个配置类2.1、在@configuration中引入spring的xml配置文件 123456789package com.dxz.demo.configuration2;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.ImportResource;@Configuration@ImportResource(\"classpath:applicationContext-configuration.xml\")public class WebConfig {} bean类： 1234567891011121314151617181920212223package com.dxz.demo.configuration2;public class TestBean2 { private String username; private String url; private String password; public void sayHello() { System.out.println(\"TestBean2 sayHello...\"); } public String toString() { return \"TestBean2 username:\" + this.username + \",url:\" + this.url + \",password:\" + this.password; } public void start() { System.out.println(\"TestBean2 初始化。。。\"); } public void cleanUp() { System.out.println(\"TestBean2 销毁。。。\"); }} 测试类： 1234567891011121314151617181920package com.dxz.demo.configuration2;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;public class TestMain2 { public static void main(String[] args) { // @Configuration注解的spring容器加载方式，用AnnotationConfigApplicationContext替换ClassPathXmlApplicationContext ApplicationContext context = new AnnotationConfigApplicationContext(WebConfig.class); // 如果加载spring-context.xml文件： // ApplicationContext context = new // ClassPathXmlApplicationContext(\"spring-context.xml\"); // 获取bean TestBean2 tb = (TestBean2) context.getBean(\"testBean2\"); tb.sayHello(); }} 结果： 2.2、在@configuration中引入其它注解配置 12345678910111213package com.dxz.demo.configuration2;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Import;import org.springframework.context.annotation.ImportResource;import com.dxz.demo.configuration.TestConfiguration;@Configuration@ImportResource(\"classpath:applicationContext-configuration.xml\")@Import(TestConfiguration.class)public class WebConfig {} 测试类： 12345678910111213141516171819202122232425package com.dxz.demo.configuration2;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;import com.dxz.demo.configuration.TestBean;public class TestMain2 { public static void main(String[] args) { // @Configuration注解的spring容器加载方式，用AnnotationConfigApplicationContext替换ClassPathXmlApplicationContext ApplicationContext context = new AnnotationConfigApplicationContext(WebConfig.class); // 如果加载spring-context.xml文件： // ApplicationContext context = new // ClassPathXmlApplicationContext(\"spring-context.xml\"); // 获取bean TestBean2 tb2 = (TestBean2) context.getBean(\"testBean2\"); tb2.sayHello(); TestBean tb = (TestBean) context.getBean(\"testBean\"); tb.sayHello(); }} 结果： 2.3、@configuration嵌套（嵌套的Configuration必须是静态类）通过配置类嵌套的配置类，达到组合多个配置类的目的。但注意内部类必须是静态类。 上代码： 123456789101112131415161718192021222324252627package com.dxz.demo.configuration3;import org.springframework.stereotype.Component;@Componentpublic class TestBean { private String username; private String url; private String password; public void sayHello() { System.out.println(\"TestBean sayHello...\"); } public String toString() { return \"username:\" + this.username + \",url:\" + this.url + \",password:\" + this.password; } public void start() { System.out.println(\"TestBean start\"); } public void cleanUp() { System.out.println(\"TestBean destory\"); }} 123456789101112131415161718192021222324package com.dxz.demo.configuration3;public class DataSource { private String dbUser; private String dbPass; public String getDbUser() { return dbUser; } public void setDbUser(String dbUser) { this.dbUser = dbUser; } public String getDbPass() { return dbPass; } public void setDbPass(String dbPass) { this.dbPass = dbPass; } @Override public String toString() { return \"DataSource [dbUser=\" + dbUser + \", dbPass=\" + dbPass + \"]\"; }} 配置类： 123456789101112131415161718192021package com.dxz.demo.configuration3;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;@Configuration@ComponentScan(basePackages = \"com.dxz.demo.configuration3\")public class TestConfiguration { public TestConfiguration() { System.out.println(\"TestConfiguration容器启动初始化。。。\"); } @Configuration static class DatabaseConfig { @Bean DataSource dataSource() { return new DataSource(); } }} 启动类： 12345678910111213141516171819package com.dxz.demo.configuration3;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;public class TestMain { public static void main(String[] args) { // @Configuration注解的spring容器加载方式，用AnnotationConfigApplicationContext替换ClassPathXmlApplicationContexts ApplicationContext context = new AnnotationConfigApplicationContext(TestConfiguration.class); //bean TestBean tb = (TestBean) context.getBean(\"testBean\"); tb.sayHello(); DataSource ds = (DataSource) context.getBean(\"dataSource\"); System.out.println(ds); }} 结果： 123TestConfiguration容器启动初始化。。。TestBean sayHello...DataSource [dbUser=null, dbPass=null] 3、@EnableXXX注解配合@Configuration使用，包括 @EnableAsync, @EnableScheduling, @EnableTransactionManagement, @EnableAspectJAutoProxy, @EnableWebMvc。 @EnableAspectJAutoProxy—《spring AOP 之：@Aspect注解》 @EnableScheduling–《Spring 3.1新特性之二：@Enable*注解的源码,spring源码分析之定时任务Scheduled注解》 4、@Profile逻辑组配置见《Spring的@PropertySource + Environment，@PropertySource（PropertySourcesPlaceholderConfigurer）+@Value配合使用》 5、使用外部变量1、@PropertySource + Environment，通过@PropertySource注解将properties配置文件中的值存储到Spring的 Environment中，Environment接口提供方法去读取配置文件中的值，参数是properties文件中定义的key值。2、@PropertySource（PropertySourcesPlaceholderConfigurer）+@Value 见《Spring的@PropertySource + Environment，@PropertySource（PropertySourcesPlaceholderConfigurer）+@Value配合使用》","link":"/2019/08/30/Configuration/"},{"title":"ES分页查询最大数","text":"elasticsearch 分页查询最大存储数 12345678curl -X PUT http://localhost:9200/_cluster/settings -H 'Content-Type: application/json' -d'{ \"persistent\" : { \"search.max_open_scroll_context\": 5000 }, \"transient\": { \"search.max_open_scroll_context\": 5000 }}","link":"/2020/05/09/ES分页查询最大数/"},{"title":"ES返回值数量","text":"ES返回值数量超过10000条解决方式 ES默认返回数据量为10000条， 当分页的from超过10000条的时候，es就会如下报错： 1Result window is too large, from + size must be less than or equal to:[10000] but was [10500]. See the scroll api for a more efficient way to requestlarge data sets. This limit can be set by changing the[index.max_result_window] index level parameter 解决方案: 通过设置index 的设置参数max_result_window的值eg： 1curl -XPUT http://es-ip:9200/_settings -d '{ \"index\" : { \"max_result_window\" : 100000000}} 2.修改集群配置config/elasticsearch.yml 文件增加如下配置 max_result_window: 100000","link":"/2020/01/17/ES返回值数量/"},{"title":"IUserDao.xml(mapper)","text":"12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.enlink.dao.IUserDao\"&gt; &lt;!--配置查询所有--&gt; &lt;!--返回类型封装到哪里--&gt; &lt;select id=\"findAll\" resultType=\"com.enlink.domain.User\"&gt; select * from user &lt;/select&gt;&lt;/mapper&gt;","link":"/2019/07/10/IUserDao-xml-mapper/"},{"title":"IP段计算","text":"Java根据网段计算子网掩码，起始IP，结束IP 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148/** * IP的相关计算 * @author zengchaowang * @version 1.0 * */public class IpCaculate { /** * 根据掩码位数计算掩码 * @param maskIndex 掩码位 * @return 子网掩码 */ public static String getNetMask(String maskIndex) { StringBuilder mask = new StringBuilder(); Integer inetMask = 0; try { inetMask = Integer.parseInt(maskIndex); } catch (NumberFormatException e) { System.out.println(e.getMessage()); return null; } if (inetMask &gt; 32) { return null; } // 子网掩码为1占了几个字节 int num1 = inetMask / 8; // 子网掩码的补位位数 int num2 = inetMask % 8; int array[] = new int[4]; for (int i = 0; i &lt; num1; i++) { array[i] = 255; } for (int i = num1; i &lt; 4; i++) { array[i] = 0; } for (int i = 0; i &lt; num2; num2--) { array[num1] += 1 &lt;&lt; 8 - num2; } for (int i = 0; i &lt; 4; i++) { if (i == 3) { mask.append(array[i]); } else { mask.append(array[i] + \".\"); } } return mask.toString(); } /** * 根据网段计算起始IP 网段格式:x.x.x.x/x * 一个网段0一般为网络地址,255一般为广播地址. * 起始IP计算:网段与掩码相与之后加一的IP地址 * @param segment 网段 * @return 起始IP */ public static String getStartIp(String segment) { StringBuffer startIp = new StringBuffer(); if (segment == null) { return null; } String arr[] = segment.split(\"/\"); String ip = arr[0]; String maskIndex = arr[1]; String mask = IpCaculate.getNetMask(maskIndex); if (4 != ip.split(\"\\\\.\").length || mask == null) { return null; } int ipArray[] = new int[4]; int netMaskArray[] = new int[4]; for (int i = 0; i &lt; 4; i++) { try { ipArray[i] = Integer.parseInt(ip.split(\"\\\\.\")[i]); netMaskArray[i] = Integer.parseInt(mask.split(\"\\\\.\")[i]); if (ipArray[i] &gt; 255 || ipArray[i] &lt; 0 || netMaskArray[i] &gt; 255 || netMaskArray[i] &lt; 0) { return null; } ipArray[i] = ipArray[i] &amp; netMaskArray[i]; if(i==3){ startIp.append(ipArray[i]+1); }else{ startIp.append(ipArray[i]+\".\"); } } catch (NumberFormatException e) { System.out.println(e.getMessage()); } } return startIp.toString(); } /** * 根据网段计算结束IP * @param segment * @return 结束IP */ public static String getEndIp(String segment) { StringBuffer endIp=new StringBuffer(); String startIp = getStartIp(segment); if (segment == null) { return null; } String arr[] = segment.split(\"/\"); String maskIndex = arr[1]; //实际需要的IP个数 int hostNumber = 0; int startIpArray[] = new int[4]; try { hostNumber=1&lt;&lt;32-(Integer.parseInt(maskIndex)); for (int i = 0; i &lt;4; i++) { startIpArray[i] = Integer.parseInt(startIp.split(\"\\\\.\")[i]); if(i == 3){ startIpArray[i] = startIpArray[i] - 1; break; } } startIpArray[3] = startIpArray[3] + (hostNumber - 1); } catch (NumberFormatException e) { System.out.println(e.getMessage()); } if(startIpArray[3] &gt;255){ int k = startIpArray[3] / 256; startIpArray[3] = startIpArray[3] % 256; startIpArray[2] = startIpArray[2] + k; } if(startIpArray[2] &gt; 255){ int j = startIpArray[2] / 256; startIpArray[2] = startIpArray[2] % 256; startIpArray[1] = startIpArray[1] + j; if(startIpArray[1] &gt; 255){ int k = startIpArray[1] / 256; startIpArray[1] = startIpArray[1] % 256; startIpArray[0] = startIpArray[0] + k; } } for(int i = 0; i &lt; 4; i++){ if(i == 3){ startIpArray[i] = startIpArray[i] - 1; } if(\"\" == endIp.toString()||endIp.length()==0){ endIp.append(startIpArray[i]); }else{ endIp.append(\".\" + startIpArray[i]); } } return endIp.toString(); } } 根据网段计算起始IP 网段格式:x.x.x.x/x 网段内需要的IP数量为：2的（32-掩码位）次方个。 因为一个网段0一般为网络地址,255一般为广播地址， 所以第一个可用IP起始IP计算:网段与掩码相与之后加一的IP地址，结束IP为通过计算需要的所有IP数然后做累加运算，超过256则进位进行运算。","link":"/2019/12/13/IP段计算/"},{"title":"Gson","text":"Gson处理json数据，转换javaBean的时候，替换输出字段名，解析日期的坑 有的时候，我们输出的json数据可能跟原始javabean不一样。为了说明这个问题，举例如下： 123456789101112131415161718192021222324252627282930313233package com.zhdw.mgrclient.test;import java.util.Date;public class Person { private String name; private int age; private Date birthday; public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public Date getBirthday() { return birthday; } public void setBirthday(Date birthday) { this.birthday = birthday; }} javabean对象，比较简单，以下是测试类： 123456789101112131415161718package com.zhdw.mgrclient.test;import java.util.Date;import com.google.gson.Gson;public class JSONTest { public static void main(String[] args) { Person hbk = new Person(); hbk.setAge(30); hbk.setName(\"黄宝康\"); hbk.setBirthday(new Date()); Gson gson = new Gson(); String result = gson.toJson(hbk); System.out.println(result); }} 运行输出 {“name”:”黄宝康”,”age”:30,”birthday”:”May 21, 2018 11:50:23 AM”} 需求，我需要输出NAME，而不是小写的name，第二，日期格式不是我想要的。 Gson针对这两个问题，提供了相关注解，只需要在Person的name字段加入相关注解即可。 12@SerializedName(\"NAME\") private String name; 再次运行输出如下：可以看出name已经输出成NAME了，第一个问题解决。 {“NAME”:”黄宝康”,”age”:30,”birthday”:”May 21, 2018 11:53:25 AM”} 针对第二个问题，查看了相关博客，在不同环境中由于返回的Local字符串不同，在不同环境日期格式输出会不一样。 Gson默认处理Date对象的序列化/反序列化是通过一个SimpleDateFormat对象来实现的，通过下面的代码去获取实例： DateFormat.getDateTimeInstance() 在不同的locale环境中，这样获取到的SimpleDateFormat的模式字符串会不一样。 Gson gson = new GsonBuilder().setDateFormat(“yyyy-MM-dd HH:mm:ss”).create(); 使用GsonBuilder()构建Gson对象，而不是之前的new Gson();这样运行输出了我们想要的格式。 {“NAME”:”黄宝康”,”age”:30,”birthday”:”2018-05-21 11:57:16”} 其他设置： 如打印美化的json格式数据等 123GsonBuilder builder = new GsonBuilder();builder.setPrettyPrinting();Gson gson = builder.create(); 通过上面的设置，控制台输出的json格式是带有缩进的。 上面为了解决大小写需求是通过注解的方式解决的，其实Gson还可以通过如下方式解决。（先去掉注解@SerializedName(“NAME”)） 1234567891011121314151617181920212223public static void main(String[] args) { Person hbk = new Person(); hbk.setAge(30); hbk.setName(\"黄宝康\"); hbk.setBirthday(new Date()); GsonBuilder builder = new GsonBuilder(); builder.setPrettyPrinting(); // 设置字段名称策略 builder.setFieldNamingStrategy(new FieldNamingStrategy() { @Override public String translateName(Field field) { // 业务规则可以自定义，实际项目可能会比较复杂 if(field.getName().equals(\"name\")){ return \"NAME\"; } return field.getName(); } }); Gson gson = builder.setDateFormat(\"yyyy-MM-dd HH:mm:ss\").create(); String result = gson.toJson(hbk); System.out.println(result); } 默认Gson输出json，会把javabean的所有字段都输出，有的时候，业务上需要隐藏敏感字段。适应，给Person增加ignore属性，同时生成getter和setter方法。 12345678private String ignore;public String getIgnore() { return ignore;}public void setIgnore(String ignore) { this.ignore = ignore;} 测试类： 123456789public static void main(String[] args) { Person hbk = new Person(); hbk.setAge(30); hbk.setName(\"黄宝康\"); hbk.setBirthday(new Date()); hbk.setIgnore(\"不要看见我\"); Gson gson = new Gson(); System.out.println(gson.toJson(hbk)); } 控制台会输出ignore字段 {“name”:”黄宝康”,”age”:30,”birthday”:”May 23, 2018 4:08:04 PM”,”ignore”:”不要看见我”} 为了不看见ignore字段，我们只需要在ignore属性前加上transient修饰符即可。 private transient String ignore; 再次运行测试类，输出 {“name”:”黄宝康”,”age”:30,”birthday”:”May 23, 2018 4:12:41 PM”}","link":"/2020/01/10/Gson/"},{"title":"Java获取IP地址","text":"Java 通过Request请求获取IP地址 12345678910111213141516171819202122232425262728293031323334353637383940public class IpAddressUtil { /** * 获取Ip地址 * @param request * @return */ private static String getIpAddress(HttpServletRequest request) { String Xip = request.getHeader(\"X-Real-IP\"); String XFor = request.getHeader(\"X-Forwarded-For\"); if(StringUtils.isNotEmpty(XFor) &amp;&amp; !\"unKnown\".equalsIgnoreCase(XFor)){ //多次反向代理后会有多个ip值，第一个ip才是真实ip int index = XFor.indexOf(\",\"); if(index != -1){ return XFor.substring(0,index); }else{ return XFor; } } XFor = Xip; if(StringUtils.isNotEmpty(XFor) &amp;&amp; !\"unKnown\".equalsIgnoreCase(XFor)){ return XFor; } if (StringUtils.isBlank(XFor) || \"unknown\".equalsIgnoreCase(XFor)) { XFor = request.getHeader(\"Proxy-Client-IP\"); } if (StringUtils.isBlank(XFor) || \"unknown\".equalsIgnoreCase(XFor)) { XFor = request.getHeader(\"WL-Proxy-Client-IP\"); } if (StringUtils.isBlank(XFor) || \"unknown\".equalsIgnoreCase(XFor)) { XFor = request.getHeader(\"HTTP_CLIENT_IP\"); } if (StringUtils.isBlank(XFor) || \"unknown\".equalsIgnoreCase(XFor)) { XFor = request.getHeader(\"HTTP_X_FORWARDED_FOR\"); } if (StringUtils.isBlank(XFor) || \"unknown\".equalsIgnoreCase(XFor)) { XFor = request.getRemoteAddr(); } return XFor; }} 详解首先，我们获取 X-Forwarded-For 中第0位的IP地址，它就是在HTTP扩展协议中能表示真实的客户端IP。具体就像这样： X-Forwarded-For: client, proxy1, proxy2，proxy… 所以你应该知道为什么要取第0位了吧！如果 X-Forwarded-For 获取不到，就去获取X-Real-IP ，X-Real-IP 获取不到，就依次获取Proxy-Client-IP 、WL-Proxy-Client-IP 、HTTP_CLIENT_IP 、 HTTP_X_FORWARDED_FOR 。最后获取不到才通过request.getRemoteAddr()获取IP， X-Real-IP 就是记录请求的客户端真实IP。跟X-Forwarded-For 类似。 Proxy-Client-IP 顾名思义就是代理客户端的IP，如果客户端真实IP获取不到的时候，就只能获取代理客户端的IP了。 WL-Proxy-Client-IP 是在Weblogic下获取真实IP所用的的参数。 HTTP_CLIENT_IP 与 HTTP_X_FORWARDED_FOR 可以理解为X-Forwarded-For ， 因为它们是PHP中的用法。 备注：StringUtils是Apache的工具类, pom.xml加依赖即可；也可以去Apache下载commons-lang3的最新包。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.3.2&lt;/version&gt;&lt;/dependency&gt;","link":"/2019/12/04/Java获取IP地址/"},{"title":"JAVA基础","text":"java 基础 https://blog.csdn.net/qq_44543508/article/details/102651841","link":"/2020/03/19/JAVA基础/"},{"title":"HandlerInterceptor的preHandle、postHandle、afterCompletion方法的作用","text":"处理器拦截器（HandlerInterceptor）详解(1)preHandle方法是进行处理器拦截用的，顾名思义，该方法将在Controller处理之前进行调用。SpringMVC中的Interceptor拦截器是链式的，可以同时存在多个Interceptor，然后SpringMVC会根据声明的前后顺序一个接一个的执行，而且所有的Interceptor中的preHandle方法都会在Controller方法调用之前调用。（SpringMVC的这种Interceptor链式结构也是可以进行中断的，这种中断方式是令preHandle的返回值为false，当preHandle的返回值为false的时候整个请求就结束了。）重写preHandle方法，在请求发生前执行。(2)postHandle是进行处理器拦截用的，这个方法只会在当前这个Interceptor的preHandle方法返回值为true的时候才会执行。它的执行时间是在处理器进行处理之后，也就是在Controller的方法调用之后执行，但是它会在DispatcherServlet进行视图的渲染之前执行，也就是说在这个方法中你可以对ModelAndView进行操作。这个方法的链式结构跟正常访问的方向是相反的，也就是说先声明的Interceptor拦截器该方法反而会后调用。重写postHandle方法，在请求完成后执行。(3)afterCompletion这个方法只会在当前这个Interceptor的preHandle方法返回值为true的时候才会执行。该方法将在整个请求完成之后，也就是DispatcherServlet渲染了视图执行。（如性能监控中我们可以在此记录结束时间并输出消耗时间，还可以进行一些资源清理）","link":"/2019/09/24/HandlerInterceptor的preHandle、postHandle、afterCompletion方法的作用/"},{"title":"@EqualsAndHashCode","text":"原文中提到的大致有以下几点： 此注解会生成equals(Object other) 和 hashCode()方法。 它默认使用非静态，非瞬态的属性 可通过参数exclude排除一些属性 可通过参数of指定仅使用哪些属性 它默认仅使用该类中定义的属性且不调用父类的方法 可通过callSuper=true解决上一点问题。让其生成的方法中调用父类的方法。 另：@Data相当于@Getter @Setter @RequiredArgsConstructor @ToString @EqualsAndHashCode这5个注解的合集。 通过官方文档，可以得知，当使用@Data注解时，则有了@EqualsAndHashCode注解，那么就会在此类中存在equals(Object other) 和 hashCode()方法，且不会使用父类的属性，这就导致了可能的问题。 比如，有多个类有相同的部分属性，把它们定义到父类中，恰好id（数据库主键）也在父类中，那么就会存在部分对象在比较时，它们并不相等，却因为lombok自动生成的equals(Object other) 和 hashCode()方法判定为相等，从而导致出错。 修复此问题的方法很简单： 使用@Getter @Setter @ToString代替@Data并且自定义equals(Object other) 和 hashCode()方法，比如有些类只需要判断主键id是否相等即足矣。 或者使用在使用@Data时同时加上@EqualsAndHashCode(callSuper=true)注解。 任意类的定义都可以添加@EqualsAndHashCode注解，让lombok帮你生成equals(Object other)和hashCode()方法的实现。默认情况下会使用非静态和非transient型字段来生成，但是你也通过在字段上添加@EqualsAndHashCode.Include或者@EqualsAndHashCode.Exclude修改你使用的字段（甚至指定各种方法的输出）。或者你也可以通过在类上使用@EqualsAndHashCode(onlyExplicitlyIncluded = true)，且在特定字段或特定方法上添加@EqualsAndHashCode.Include来指定他们。 如果将@EqualsAndHashCode添加到继承至另一个类的类上，这个功能会有点棘手。一般情况下，为这样的类自动生成equals和hashCode方法是一个坏思路，因为超类也有定义了一些字段，他们也需要equals/hashCode方法但是不会自动生成。通过设置callSuper=true，可以在生成的equals和hashCode方法里包含超类的方法。对于hashCode，·super.hashCode()·会被包含在hash算法内，而对于equals，如果超类实现认为它与传入的对象不一致则会返回false。注意：并非所有的equals都能正确的处理这样的情况。然而刚好lombok可以，若超类也使用lombok来生成equals方法，那么你可以安全的使用它的equals方法。如果你有一个明确的超类, 你得在callSuper上提供一些值来表示你已经斟酌过，要不然的话就会产生一条警告信息。 当你的类没有继承至任何类（非java.lang.Object, 当然任何类都是继承于Object类的），而你却将callSuer置为true, 这会产生编译错误（译者注： java: Generating equals/hashCode with a supercall to java.lang.Object is pointless. ）。因为这会使得生成的equals和hashCode方法实现只是简单的继承至Object类的方法，只有相同的对象并且相同的hashCode才会判定他们相等。若你的类继承至另一个类又没有设置callSuper, 则会产品一个告警，因为除非超类没有（或者没有跟相等相关的）字段，否则lombok无法为你生成考虑超类声明字段的实现。你需要编写自己的实现，或者依赖callSuper的链式功能。你也可以使用配置关键字lombok.equalsAndHashCode.callSuper.（译者注：配置关键字这里还没搞明白！！！） NEW in Lombok 0.10: 除非你的类是final类型并且只是继承至java.lang.Object, 否则lombok会生成一个canEqual方法，这以为着JPA代理仍然可以等于他们的基类，但是添加新状态的子类不会破坏equals契约。下文解释了为什么需要这种方法的复杂原因：How to Write an Equality Method in Java。如果层次结构中的所有类都是scala case类和带有lombok生成的equals方法的类的混合，则所有的相等都能正常工作。如果你需要编写自己的equals方法，那么如果更改equals和hashCode方法，都应该始终覆盖canEqual. NEW in Lombok 1.14.0: 要将注释放在equals的另一个参数（以及相关的canEqual）方法上，可以使用onParam = @ __（{@ AnnotationsHere}）。 但要小心！ 这是一个实验性功能。 有关更多详细信息，请参阅有关onX功能的文档。 看代码示例，最好自己写一下，然后查看编译后的class文件。 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.amos.lombok;import lombok.EqualsAndHashCode;/** * @author amos */@EqualsAndHashCodepublic class EqualsAndHashCodeExample { private transient int transientVar = 10; private String name; private double score; /** * 不包含该字段 */ @EqualsAndHashCode.Exclude private Shape shape = new Square(5, 10); private String[] tags; /** * 不包含该字段 */ @EqualsAndHashCode.Exclude private int id; public String getName() { return this.name; } @EqualsAndHashCode(callSuper = true) public static class Square extends Shape { private final int width, height; public Square(int width, int height) { this.width = width; this.height = height; } } public static class Shape { }} 编译后的class文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118//// Source code recreated from a .class file by IntelliJ IDEA// (powered by Fernflower decompiler)//package com.amos.lombok;import java.util.Arrays;public class EqualsAndHashCodeExample { private transient int transientVar = 10; private String name; private double score; private EqualsAndHashCodeExample.Shape shape = new EqualsAndHashCodeExample.Square(5, 10); private String[] tags; private int id; public EqualsAndHashCodeExample() { } public String getName() { return this.name; } public boolean equals(final Object o) { if (o == this) { return true; } else if (!(o instanceof EqualsAndHashCodeExample)) { return false; } else { EqualsAndHashCodeExample other = (EqualsAndHashCodeExample)o; if (!other.canEqual(this)) { return false; } else { label31: { Object this$name = this.getName(); Object other$name = other.getName(); if (this$name == null) { if (other$name == null) { break label31; } } else if (this$name.equals(other$name)) { break label31; } return false; } if (Double.compare(this.score, other.score) != 0) { return false; } else { return Arrays.deepEquals(this.tags, other.tags); } } } } protected boolean canEqual(final Object other) { return other instanceof EqualsAndHashCodeExample; } public int hashCode() { int PRIME = true; int result = 1; Object $name = this.getName(); int result = result * 59 + ($name == null ? 43 : $name.hashCode()); long $score = Double.doubleToLongBits(this.score); result = result * 59 + (int)($score &gt;&gt;&gt; 32 ^ $score); result = result * 59 + Arrays.deepHashCode(this.tags); return result; } public static class Shape { public Shape() { } } public static class Square extends EqualsAndHashCodeExample.Shape { private final int width; private final int height; public Square(int width, int height) { this.width = width; this.height = height; } public boolean equals(final Object o) { if (o == this) { return true; } else if (!(o instanceof EqualsAndHashCodeExample.Square)) { return false; } else { EqualsAndHashCodeExample.Square other = (EqualsAndHashCodeExample.Square)o; if (!other.canEqual(this)) { return false; } else if (!super.equals(o)) { return false; } else if (this.width != other.width) { return false; } else { return this.height == other.height; } } } protected boolean canEqual(final Object other) { return other instanceof EqualsAndHashCodeExample.Square; } public int hashCode() { int PRIME = true; int result = super.hashCode(); result = result * 59 + this.width; result = result * 59 + this.height; return result; } }} 注意超类不加callSuper且不手动覆写 上述的代码，通过下面的代码测试 1234567public static void main(String[] args) { Square square1 = new Square(1, 2); Square square2 = new Square(1, 2); // false System.out.println(square1.equals(square2)); } 可以看出，明明对象应该是相等的，但是就是不等！！因为下面的代码导致，可以看出使用的是Object的equals方法，该方法一定要对象完全一样且hashCode一样才判定相等。 123else if (!super.equals(o)) { return false;} 所以，为了避免这个问题，你要么手动覆写超类的equals，要么在超类上加callSuper注解。","link":"/2019/10/12/EqualsAndHashCode-注解详解/"},{"title":"搭建DNS服务器","text":"Linux下DNS服务器常规操作Red Hat Linux的各个版本已经包含DNS服务器的软件–Bind，一般不需要用户另行安装，如果用户需要安装最新版本，可以到Bind官网http://www.bind.com/浏览最新消息。也可以到其它网站下载。 源码软件包：https://www.isc.org/downloads/ 例如，在其它网站中下载源码包软件包bind-9.10.4-P1.tar.gz 以下是安装过程中的一些指令： 12345[root@localhost root]# tar xzvf bind-9.10.4-P1.tar.gz[root@localhost root]# cd bind-9.10.4-P1[root@localhost bind-9.10.4-P1]# ./configure[root@localhost bind-9.10.4-P1]# make[root@localhost bind-9.10.4-P1]# make install 其中各参数含义如下： 1234tar xzvf bind-9.10.4-P1.tar.gz //解压缩软件包./configure //针对机器做安装的检查和设置，大部分工作由机器自动完成make //编译make install //安装 软件包的功能 1234567Bind：提供了域名服务的主要程序以及相关文件。Bind-utils:提供了对DNS服务器的测试工具程序（nslookupdup、dig等）Bind-chroot:为Bind提供了一个伪装的根目录以增强安全性Caching-namserver:为配置Bind作为缓存域名服务器提供必要的默认配置文件，用于参考 DNS常规操作1.启动DNS服务器：/etc/init.d/named start2.停止DNS服务器：/etc/init.d/named stop3.重新启动DNS服务器:/etc/init.d/named restart DNS配置文件 与DNS相关的两个特殊文件1./etc/resolv.conf 该文件用来指定系统中DNS服务器的IP地址和一些相关信息，格式如下： 123search abc.com.cnnameserver 10.1.6.250nameserver 192.168.1.254 2./etc/host.conf 该文件决定进行域名解析时查找host文件和DNS服务器的顺序，其格式如下： order hosts,bind Bind的配置文件 Bind的主配置文件是etc/name.conf,该文件是文本文件，一般需手动生成。除了主配置文件外，/var/named目录下的所有文件都是DNS服务器的相关配置文件，下面详细讲述这些文件的配置。 1.name.conf文件详解 1234567891011121314151617181920212223options {listen-on port 53 { 127.0.0.1; }; //设置named服务器监听端口及IP地址listen-on-v6 port 53 { ::1; };directory \"/var/named\"; //设置区域数据库文件的默认存放地址dump-file \"/var/named/data/cache_dump.db\";statistics-file \"/var/named/data/named_stats.txt\";memstatistics-file \"/var/named/data/named_mem_stats.txt\";allow-query { localhost; }; //允许DNS查询客户端allow-query-cache { any; };};logging {channel default_debug {file \"data/named.run\";severity dynamic;};};view localhost_resolver {match-clients { any; };match-destinations { any; };recursion yes; //设置允许递归查询include \"/etc/named.rfc1912.zones\";}; 2.区域配置文件/etc/named.rfc1912.zones 12345678910111213141516171819202122zone \".\" IN { //定义了根域type hint; //定义服务器类型为hintfile \"named.ca\"; //定义根域的配置文件名};zone \"localdomain\" IN { //定义正向DNS区域type master; //定义区域类型file \"localdomain.zone\"; //设置对应的正向区域地址数据库文件allow-update { none; }; //设置允许动态更新的客户端地址（none为禁止）};zone \"localhost\" IN {type master;file \"localhost.zone\";allow-update { none; };};zone \"0.0.127.in-addr.arpa\" IN { //设置反向DNS区域type master;file \"named.local\";allow-update { none; };}; 3.根域配置文件named.ca 根域配置文件设定根域的域名数据库，包括根域中13台DNS服务器的信息。几乎所有系统的这个文件都是一样的，用户不需要进行修改。 4.正向域名解析数据库文件 1234567891011121314151617$TTL 600@ IN SOA dns.cwlinux.com dnsadmin.cwlinux.com. (//SOA字段 2015031288 //版本号 同步一次 +1 1H //更新时间 2M // 更新失败，重试更新时间 2D // 更新失败多长时间后此DNS失效时间 1D //解析不到请求不予回复时间) IN NS dns //有两域名服务器 IN NS ns2 IN MX 10 mial // 定义邮件服务器，10指优先级 0-99 数字越小优先级越高ns2 IN A 192.168.1.113 //ns2域名服务器的ip地址dns IN A 192.168.1.10 //dns域名服务器的ip地址mail IN A 192.168.1.111 //邮件服务器的ip地址www IN A 192.168.1.112 //www.cwlinux.com的ip地址pop IN CNAME mail //pop的正式名字是mailftp IN CNAME www //ftp的正式名字是www 5.反向域名解析数据库文件 12345678910111213$TTL 600@ IN SOA dns.cwlinux.com. dnsadmin.cwlinux.com. ( 2014031224 1H 2M 2D 1D) IN NS dns.cwlinux.com.10 IN PTR dns.cwlinux.com. //反向解析PTR格式111 IN PTR mail.cwlinux.com.112 IN PTR www.cwlinux.com.// 声明域的时候已经有了，192.168.1 所以我们只需要输入10既代表192.168.1.10jc DNS客户端的配置文件 Linux系统中，DNS客户端的配置文件是/etc/resolv.conf,该文件记录了DNS服务器的地址和域名。 一般格式如下： 123#more /etc/resolv.confnameserver 10.1.6.250 domainname abc.com.cn 其中，关键字nameserver记录该域中DNS服务器的IP地址，domainname记录所在域的名称。","link":"/2019/11/25/Linux搭建DNS服务器/"},{"title":"MAC设置ip","text":"BasicIPv6ValidationError解决办法 打开终端按如下命令操作 1.列出你的网卡 networksetup -listallnetworkservices 2.关闭ipv6 networksetup -setv6off “你网卡名字” 3.设置ip地址 networksetup -setmanual “网卡名字” 192.168.31.2 255.255.255.0 192.168.1.1","link":"/2020/03/19/MAC设置ip/"},{"title":"MIME TYPE","text":"网页下载文件时，设置content-type Ext MIME Type .caj application/caj .pdf application/pdf .doc application/msword .dot application/msword .docx application/vnd.openxmlformats-officedocument.wordprocessingml.document .dotx application/vnd.openxmlformats-officedocument.wordprocessingml.template .docm application/vnd.ms-word.document.macroEnabled.12 .dotm application/vnd.ms-word.template.macroEnabled.12 .xls application/vnd.ms-excel .xlt application/vnd.ms-excel .xla application/vnd.ms-excel .xlsx application/vnd.openxmlformats-officedocument.spreadsheetml.sheet .xltx application/vnd.openxmlformats-officedocument.spreadsheetml.template .xlsm application/vnd.ms-excel.sheet.macroEnabled.12 .xltm application/vnd.ms-excel.template.macroEnabled.12 .xlam application/vnd.ms-excel.addin.macroEnabled.12 .xlsb application/vnd.ms-excel.sheet.binary.macroEnabled.12 .ppt application/vnd.ms-powerpoint .pot application/vnd.ms-powerpoint .pps application/vnd.ms-powerpoint .ppa application/vnd.ms-powerpoint .pptx application/vnd.openxmlformats-officedocument.presentationml.presentation .potx application/vnd.openxmlformats-officedocument.presentationml.template .ppsx application/vnd.openxmlformats-officedocument.presentationml.slideshow .ppam application/vnd.ms-powerpoint.addin.macroEnabled.12 .pptm application/vnd.ms-powerpoint.presentation.macroEnabled.12 .potm application/vnd.ms-powerpoint.presentation.macroEnabled.12 .ppsm application/vnd.ms-powerpoint.slideshow.macroEnabled.12","link":"/2020/02/23/MIMETYPE/"},{"title":"Mac 安装任何来源","text":"Mac安装来源：若没有“任何来源”这个选项打开终端（Terminal.app），拷贝粘贴sudo spctl --master-disable，按回车键","link":"/2019/12/02/Mac安装任何来源/"},{"title":"Mac格式化u盘和dmg","text":"2.格式化U盘 这个U盘最好选择一个USB3.0版本、8G或16G内存的。 3.将ISO文件转换为dmg格式 执行命令 cd /users/用户名/下载文件夹hdiutil convert AAAA.iso -format UDRW -o BBBBB.dmg //AAAA为你下载ISO镜像的名字；BBBBB你随意取就好4.制作启动U盘 diskutil list diskutil unmountDisk /dev/diskN //N为上面命令中显示出的你U盘的序号，一般是2。 sudo dd if =BBBBB.dmg of=/dev/rdiskN bs=2m //N同样为你U盘的序号","link":"/2019/10/29/Mac格式化u盘和dmg/"},{"title":"MybatisTest","text":"1234567891011121314151617181920212223242526272829303132333435363738394041package com.enlink.test;import com.enlink.dao.IUserDao;import com.enlink.domain.User;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import java.io.InputStream;import java.util.List;/** * @author Tong * mybatistest */public class MybatisTest { /** * @param args */ public static void main(String[] args)throws Exception { //1.读取配置文件 InputStream in = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); //2.创建SqlSessionFactory工厂 SqlSessionFactoryBuilder builder = new SqlSessionFactoryBuilder(); SqlSessionFactory factory = builder.build(in); //3.使用工厂生产SqlSession对象 SqlSession session = factory.openSession(); //4.使用SqlSession创建Dao接口的代理对象 IUserDao userDao = session.getMapper(IUserDao.class); //5.使用代理对象执行方法 List&lt;User&gt; users = userDao.findAll(); for(User user : users){ System.out.println(user); } //6.释放资源 session.close(); in.close(); }}","link":"/2019/07/10/MybatisTest/"},{"title":"KARAF","text":"Karaf：简述对Karaf的认识 Karaf是基于OSGI之上建立的应用容器，能方便部署各种选定的组件，简化打包和安装应用的操作难度。Open DayLight项目发布之初，后台框架仅采用OSG技术，但自从第三版氦He版本至今， Open Daylight项目就采用了Kaaf作为后台的框架，明显提升了项目的可用性和灵活性。Karaf是一个 Apache软件基金会项目，具有 Apache v2许可证。是一个基于实时运行的轻量级的基于OSGI的容器，各种组件和应用都能部署到这个容器中。一、Karaf 的架构图二、Karaf 的安装目录结构 /bin: 启动脚本/etc: 初始化文件/data: 工作目录/cache: OSGi框架包缓存/generated-bundles: 部署使用的临时文件夹/log: 日志文件/deploy: 热部署目录/instances: 含有子实例的目录/lib: 包含引导库/lib/ext:JRE扩展目录/lib/endorsed: 赞同库目录/system: OSGi包库，作为一个Maven2存储库 三、Karaf的基本特性Karaf典型的特性有:热部署： Karaf支持 OSGI bundles的热部署。实现这个支持的关键在于 Karaf持续监测[home]/deply目录内的jar文件。每次当一个jar文件被复制到这个文件夹内，它将在运行时被安装。可以更新或删除它，这个改动将被自动处理。此外， Karaf也支持 exploded bundles和自定义的部署(默认包含 blueprint和 spring)动态配置：服务通常通过OsGi服务的配置管理来进行配置。这样的配置可在 Karaf中的[home]/ec目录使用合适的文件进行定义。这样配置被监控，并且属性的改变将传播给服务。日志系统：使用Log4J支持的集中日志后端，Kaaf能支持许多不同的APls(JDK14、JCL、SLF4J、 Avalon、 Tomcat、OSGi)。供应：库或应用的供应( Provisioning)能通过不同的方式完成。供应在本地下载、安装、启动。原生OS整合：Karaf能以服务的方式整合到一个操作系统OS中，这样生命周期就和这个OS相绑定。扩展的内核控制台：Karaf的内核控制台可管理服务，安装新应用或库，并且管理它们的状态。这个内核能通过部署新命令以安装新功能或应用实现动态扩展。远程访问：使用任何的SSH客户端以连接Karaf并且在控制台发出命令。基于JAAS的安全框架。管理实例：Karaf提供简单的命令以管理多实例。通过控制台能简单地创建、删除、启动、终止 Karaf的实例。四、karaf在 Open Daylight使用中的一些常用命令[cmd]-help：获取一个指定命令的相关帮助。若需要了解 bundle: list命令，则输入:bundle: list --help; Karat将返回有关这个命令的帮助。features: list：查看已安装的 features。feature: install my_feature：安装本地 feature的命令。如 feature: install odl-reston。feature:uninstall my_feature:卸载已经安装的fm的命令。如 feature uninstall odl-restconf feature: repo-add my repo：将库安装至本地。如 feature: repo-add camel2.152 log: display：显示系统日志，可配合grep来筛选需要查看的内容。system: shutdown：关闭系统，退出 Karaf。同样也可运行halt命令退出 Karaf。注意:进入 Karaf的控制台后，按&lt; Tab &gt;键即可显示目前所有可用的命令。控制台支持&lt; Tab &gt;键的输入辅助完成功能，当输入一个命令的前面一些字符时，按&lt; Tab &gt;键后，控制台将自动列出所有可能的命令，并且在只有一个提示命令时自动完成该命令的输入。 Karaf教程之安装和应用开发 概览 安装和启动 一些便利的命令 Tasklist - 一个小的OSGI应用 父pom和通用工程的设置 Tasklist-model Tasklist-persistence Tasklist-ui Tasklist-features 在Karaf中安装应用 总结 概览 以这篇文章为起点，我将开始写一系列关于Apache Karaf的文章，Apache Karaf是一个基于Equinox或者Felix框架开发的一个OSGI容器。与这些框架最大的不同在于它带来了优秀的管理功能。 Karaf突出的特性如下: 可扩展控制台和Bash相似的命令补齐功能 ssh控制台 从maven仓库中获取bundles和features的部署文件 可以在命令行中轻松的创建一个新的实例 所有这些特性使开发服务端OSGi应用和开发一般的java应用几乎一样的简单。从某个层面上，部署和管理比目前我知道所有应用服务器都好。所有这些karaf特性的结合带来了最终的程序。在我看来，Karaf允许轻量级的开发风格就像J2EE结合spring应用程序的灵活性一样。 安装和启动 从karaf官网下载Karaf4.0.7。解压并运行bin/karaf。一些便利的命令 命令 描述 la 显示所有安装的bundle list 显示用户的bundle service:list 显示active的OSGI服务。这个列表十分长。这里有一个方便的方法，就是你可以使用unix的管道，如 “ls exports 显示导出的packages和bundles，这个命令可以帮助你查找出package是来自哪里 feature:list 显示所有feature包括已安装和未安装的 feature:install webconsole 访问http://localhost:8181/system/console，用karaf/karaf登录并花些时间看下它所提供的信息 diag 显示那些无法启动的bundle的诊断信息 log:tail 显示日志。可以按ctrl-c返回控制台 Ctrl-d 退出控制台。如果是主控制台，则karaf被关闭 重启后OSGI容器会保存状态 注意Karaf和其他所有的OSGI容器一样会维护已安装和已启动的bundles的最后状态。所以如果有些组件再也不能工作，重启也不一定有所帮助。为了启动一个全新的Karaf，可以停止karaf然后删除data目录或者启动的时候加个clean参数bin/karaf clean。 检查日志 Karaf运行的时候非常安静。为了不错过错误消息，可以运行tail -f data/karaf.log让日志文件处于打开状态。 Tasklist - 一个小的OSGI应用 没有任何有用的应用，Karaf是一个很棒但是没用的容器。所以让我们来创建第一个应用吧。好消息是创建一个OSGI应用非常的简单，maven会处理很多事情。和一个正常的maven工程的差异是非常少的。我推荐使用Eclipse 4写应用程序，Eclipse 4默认已经安装m2eclipse插件。 从github的Karaf-Tutorial仓库中获取源码。 git clone https://github.com/cschneider/Karaf-Tutorial.git 或者从 https://github.com/cschneider/Karaf-Tutorial/zipball/master 下载示例工程，然后解压到某个目录下。 导入到Eclipse 启动Eclipse Neon或者更新版本。 在Eclipse中选择File -&gt; Import -&gt; Existing maven project -&gt; Browse到刚才解压的目录选择tasklist 子文件夹。 Eclipse将显示所有它找到的maven工程。 单击默认导入所有工程。 Eclipse将导入所有的工程并使用m2eclipse插件管理它们之间所有的依赖。 tasklist有以下这些工程组成。 模块 描述 tasklist-model 服务接口和Task类 tasklist-persistence 提供了一个TaskService接口的简单持久化实现 tasklist-ui 使用TaskService显示任务列表的servlet tasklist-features 为应用提供features的描述信息，使应用能够很容易的安装到Karaf容器中 父pom和通用工程的设置 pom.xml打包bundle，而maven-bundle-plugin创建具有OSGi Manifest信息的jar包。默认情况，插件导入java文件import的或者在bluneprint上下文引用的所有的包。同时导出不包含内部实现的所有包。在我们的例子中我们想要model包被导入，但是persistence的实现包不导入。按照命名约定，我们不需要额外的配置。 Tasklist-model 这个工程包含domain model，这里指Task类和TaskService接口。model同时被persistence工程和用户界面工程使用。任意TaskService服务的使用者只需要依赖model工程。所以不用直接绑定到当前的实现。 Tasklist-persistence 简单持久化实现的TaskServiceImpl类用一个简单的HashMap管理task。类用@Singleton注解标记自己为blueprint bean。注解@OsgiServiceProvider用于标记为自己是一个OSGi服务，而@Properties注解允许添加服务属性。在这个例子中我们设置的属性service.exported.interfaces可以被我们在后续的教程要介绍的CXF-DOSGI使用。目前该教程这个属性可以删除。 123456@OsgiServiceProvider@Properties(@Property(name = \"service.exported.interfaces\", value = \"*\"))@Singletonpublic class TaskServiceImpl implements TaskService { ...} blueprint-maven-plugin插件将处理上面的类，且自动创建合适的blueprint xml文件。这样可以帮我们节省手动写blueprint xml文件的时间。 自动创建的blueprint xml文件位于target/generated-resources文件夹下 1234&lt;blueprint xmlns=\"http://www.osgi.org/xmlns/blueprint/v1.0.0\"&gt; &lt;bean id=\"taskService\" class=\"net.lr.tasklist.persistence.impl.TaskServiceImpl\" /&gt; &lt;service ref=\"taskService\" interface=\"net.lr.tasklist.model.TaskService\" /&gt;&lt;/blueprint&gt; Tasklist-ui UI工程使用一个TaskServlet显示任务列表和单独的任务。为了和tasks任务协作，这个servlet需要TaskService服务。我们通过使用注解@Inject和@OsgiService注入TaskService服务，@Inject注解能够根据类型注入任意的实例。而注解@OsgiService将创建一个指定类型的OSGiSerivce服务的blueprint引用。（译注：这里我是这么理解的，@OsgiService表示从Karaf中获取一个bean，@Inject则负责注入）这个类被暴露为一个OSGi服务,该类是一个有特别属性alias=/tasklist的接口java.http.Servlet实现。这样做触发了pax web的whiteboard扩展器接收这个服务，并使用url /tasklist映射到该servlet。 相关代码小片段 12345678@OsgiServiceProvider(classes = Servlet.class)@Properties(@Property(name = \"alias\", value = \"/tasklist\"))@Singletonpublic class TaskListServlet extends HttpServlet { @Inject @OsgiService TaskService taskService;} 自动创建的blueprint xml文件位于target/generated-resources文件夹下 1234567891011&lt;blueprint xmlns=\"http://www.osgi.org/xmlns/blueprint/v1.0.0\"&gt; &lt;reference id=\"taskService\" availability=\"mandatory\" interface=\"net.lr.tasklist.model.TaskService\" /&gt; &lt;bean id=\"taskServlet\" class=\"net.lr.tasklist.ui.TaskListServlet\"&gt; &lt;property name=\"taskService\" ref=\"taskService\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;service ref=\"taskServlet\" interface=\"javax.servlet.http.HttpServlet\"&gt; &lt;service-properties&gt; &lt;entry key=\"alias\" value=\"/tasklist\" /&gt; &lt;/service-properties&gt; &lt;/service&gt;&lt;/blueprint&gt; Tasklist-features 最后一个工程仅安装一个feature描述信息文件到一个maven仓库，这样我们可以很容易的安装应用到Karaf。描述信息定义了一个叫tasklist的feature和将从maven仓库下载安装的bundles 1234567891011&lt;feature name=\"example-tasklist-persistence\" version=\"${pom.version}\"&gt; &lt;bundle&gt;mvn:net.lr.tasklist/tasklist-model/${pom.version}&lt;/bundle&gt; &lt;bundle&gt;mvn:net.lr.tasklist/tasklist-persistence/${pom.version}&lt;/bundle&gt;&lt;/feature&gt;&lt;feature name=\"example-tasklist-ui\" version=\"${pom.version}\"&gt; &lt;feature&gt;http&lt;/feature&gt; &lt;feature&gt;http-whiteboard&lt;/feature&gt; &lt;bundle&gt;mvn:net.lr.tasklist/tasklist-model/${pom.version}&lt;/bundle&gt; &lt;bundle&gt;mvn:net.lr.tasklist/tasklist-ui/${pom.version}&lt;/bundle&gt;&lt;/feature&gt; 一个feature可以由其他features和bundles组成，这些features和bundles也应该被安装。一般bundles使用mvn urls。这也意味着它们从配置好的仓库或者你的本地maven仓库~/.m2/repository中加载。 在Karaf中安装应用 12feature:repo-add mvn:net.lr.tasklist/tasklist-features/1.0.0-SNAPSHOT/xmlfeature:install example-tasklist-persistence example-tasklist-ui 添加features描述信息文件到Karaf，所以它被添加到可用的features列表中，然后安装和启动tasklist feature。运行命令之后tasklist应用应该运行了。 list 检查tasklist的所有bundles是在active状态。如果没有，尝试启动它们和检查日志。 http:list 123ID | Servlet | Servlet-Name | State | Alias | Url---------------------------------------------------------------------56 | TaskListServlet | ServletModel-2 | Deployed | /tasklist | [/tasklist/*] 应该显示TaskListServlet。默认情况下这个例子应该运行在http://localhost:8181/tasklist。 你可以通过创建一个文本文件”etc/org.ops4j.pax.web.cfg”改变端口。文本内容为”org.osgi.service.http.port=8080”。这个方式将告诉HttpService使用这个端口8080. 现在tasklist应用应该运行在http://localhost:8080/tasklist地址。 总结 在该教程中我们已经安装了Karaf和学习了一些命令。然后我们创建了一个小的OSGi应用，这个应用涉及到servlets，OSGi服务，blueprint和whiteboard模式。下一个教程我们看下在OSGi中Apache Camel和Apache CXF的使用。 Apache Karaf用户指导 一 安装karaf 本章讲述如何在windows和unix平台安装Apache Karaf，这里你将知道预先要安装什么软件，如何下载并且安装Karaf。 预安装需求硬件： 20M磁盘剩余空间。操作系统：Windows：Windows vista，Windows XP sp2，windows2000。- Unix：Ubuntu Linux，Powerdog Linux,MacOS，AIX,HP-UX,Solaris，任何支持java的unix平台。环境：Java SE 1.5x或者更高。- 环境变量JAVAHOME必须设置为java运行时的安装目录，比如C:\\Program Files\\Java\\jdk1.6.026。按住windows和break键切换到高级选项，点击环境变量，把上面的路径加入到变量中。从源码构建如果你想从源码构建KARAF，需求会有点不同： 硬件： 200M磁盘空间便于apache karaf源码展开或者是SVN的验证，以及maven构建和依赖maven组件的下载。环境：JDK 1.5或者是更高。- Apache maven 2.2.1.在windows上构建这过程说明如何下载和安装windows上的源码文件。注：Karaf需要java5编译，构建和运行。 在浏览器输入http://karaf.apache.org/index/community/download.html. 滚动到“Apache Karaf”区域选择需要的链接。源码文件名称类似于：apache-karaf-x.y-src.zip.解压缩zip文件到你选择的目录中，请记住非法java路径的限制，比如！，%等。用maven 2.2.1 或者更高的java5来构建karaf。 构建karaf的方法如下：Cd [karaf安装路径]\\src- Mvn这两个步骤均需要10到15分钟。用zip工具加压文件，windows的路径是 [karaf安装路径]\\assembly\\target\\apache-karaf-x.y.zip转到开始karaf一节在Unix上构建本过程将讲述如何在unix系统上下载和安装源码文件。这里假定unix机器上有浏览器，没有浏览器的请参照前面二进制unix安装区域。注：Karaf需要java5编译，构建和运行。 在浏览器输入http://karaf.apache.org/ download.html.滚动到“Apache Karaf”区域选择需要的链接。源码文件名称类似于：apache-karaf-x.y-src.zip.解压缩zip文件到你选择的目录中，例如：gunzip apache-karaf-x.y-src.tar.gz tar xvf apache-karaf-x.y-src.tar请记住非法java路径的限制，比如！，%等用maven构建karaf建议方法如下: cd [karaf安装路径]\\src cvn 5.解压缩刚刚创建的文件 cd [karaf安装路径]/assembly/target gunzip apache-karaf-x.y.tar.gz tar xvf apache-karaf-x.y.tar转到开始karaf一节Windows安装过程这里说明如何在windows系统上下载和安装二进制文件。 在浏览器输入 http://karaf.apache.org/index/community/download.html. 滚动到“Apache Karaf”区域选择需要的链接。源码文件名称类似于：apache-karaf-x.y-src.zip.解压缩zip文件到你选择的目录中，请记住非法java路径的限制，比如！，%等。转到开始karaf一节可选：在Windows中启用彩色控制台输出 提示：如果你安装karaf到很深的路径或者是非法的java路径，！，%等，你可以创建一个bat文件来执行：subst S: “C:\\your very % problematic path!\\KARAF” 这样karaf的路径是 s: 这样的短类型。Unix安装过程这里属于如何在unix系统上下载和安装二进制文件。 在浏览器输入http://karaf.apache.org/ download.html.滚动到“Apache Karaf”区域选择需要的链接。源码文件名称类似于：apache-karaf-x.y.tar.gz解压缩zip文件到你选择的目录中，例如：gunzip apache-karaf-x.y.tar.gz tar xvf apache-karaf-x.y.tar 请记住非法java路径的限制，比如！，%等。转到开始karaf一节安装后的步骤在开始使用karaf之前强烈建议设置指向JDK的JAVA_HOME用来定位java可执行文件，并且应该配置为指向已安装java se5或者6的根目录。 二 目录结构 Karaf安装目录结构如下： /bin: 启动脚本/etc: 初始化文件/data: 工作目录/cache: OSGi框架包缓存/generated-bundles: 部署使用的临时文件夹/log: 日志文件/deploy: 热部署目录/instances: 含有子实例的目录/lib: 包含引导库/lib/ext:JRE扩展目录/lib/endorsed: 赞同库目录/system: OSGi包库，作为一个Maven2存储库Data文件夹包括karaf所有的工作和临时文件，如果你想从一个初始状态重启，你可以清空这个目录，这和“恢复初始化设置”一样的效果。 三 启动和停止karaf 本章介绍如何启动和停止Apache Karaf和各种可用的选项。 启动karafWindows下：打开一个控制台窗口，更改到安装目录，并运行Karaf。对于二进制文件，运行 Cd [karaf安装目录] 然后输入：bin\\karaf.bat Linux下：打开一个控制台窗口，更改到安装目录，并运行Karaf。运行 Cd [karaf安装目录] 然后输入：bin\\karaf 警告：karaf运行后不要关闭控制台，否则会终止karaf（除非用非控制台启动karaf）。 非控制台启动karaf没有控制台也可以启动karaf，它总可以是用远程SSH访问，是用下面的命令： bin\\karaf.bat server 或者是unix下：bin/karaf server 在后台启动karaf采用以下命令可以轻易地在后台进程启动karaf： Bin\\start.bat 或者在unix下：bin/start 重置模式启动karaf清空[karaf安装目录]\\data文件夹就可以简单的在重置模式启动karaf，为方便起见，在karaf启动脚本使用以下参数也可以实现重置启动： bin/start clean 停止karaf无论是windows还是unix，你都可以在karaf控制台采用以下命令来停止它： osgi:shutdown, 或者简单的是：shutdown。 Shutdown命令会询问你是否真的想要停止，如果你确认停止并且拒绝确认信息，你可以用-f或者-force选项： osgi：shutdown –f， 也可以用时间参数来延迟停止，时间参数有不同的形式。 首先，可以是绝对时间各式hh：mm。第二，也可以是+m，其中m是等待的分。现在就是+0。 下面的命令可以在10:35am关闭karaf: osgi:shutdown 10:35。 下面的命令在10分钟后关闭karaf： osgi:shutdown +10。 如果你在主控制台运行，用注销或者Ctrl+D退出控制台也可以终止karaf实例。 在控制台你可以运行如下命令： bin\\stop.bat 或者在unix下： bin/stop。 四 使用控制台 查看可用的命令按提示键tab可以在控制台看到可用的命令列表： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758root@root&gt; &lt;tab&gt;Display all 182 possibilities? (y or n)*:help addurl admin:change-optsadmin:change-rmi-registry-port admin:change-ssh-port admin:connectadmin:create admin:destroy admin:listadmin:rename admin:start admin:stopbundle-level cancel catchange-opts change-rmi-registry-port change-ssh-portclear commandlist config:cancelconfig:edit config:list config:propappendconfig:propdel config:proplist config:propsetconfig:update connect createcreate-dump destroy dev:create-dumpdev:dynamic-import dev:framework dev:print-stack-tracesdev:restart dev:show-tree dev:watchdisplay display-exception dynamic-importeach echo editexec exports features:addurlfeatures:info features:install features:listfeatures:listrepositories features:listurl features:listversionsfeatures:refreshurl features:removerepository features:removeurlfeatures:uninstall framework getgrep head headershelp history ifimports info installjaas:cancel jaas:commandlist jaas:listjaas:manage jaas:roleadd jaas:roledeljaas:update jaas:useradd jaas:userdeljaas:userlist java listlistrepositories listurl listversionslog:clear log:display log:display-exceptionlog:get log:set log:taillogout ls managemore new osgi:bundle-levelosgi:headers osgi:info osgi:installosgi:list osgi:ls osgi:refreshosgi:resolve osgi:restart osgi:shutdownosgi:start osgi:start-level osgi:stoposgi:uninstall osgi:update packages:exportspackages:imports print-stack-traces printfpropappend propdel proplistpropset refresh refreshurlremoverepository removeurl renameresolve restart roleaddroledel set shell:catshell:clear shell:each shell:echoshell:exec shell:grep shell:headshell:history shell:if shell:infoshell:java shell:logout shell:moreshell:new shell:printf shell:sleepshell:sort shell:tac shell:tailshow-tree shutdown sleepsort ssh ssh:sshssh:sshd sshd startstart-level stop tactail uninstall updateuseradd userdel userlistwatchroot@root&gt; 获得命令的帮助要查看一个特定的命令的帮助，在命令后加–help或使用help命令加上命令的名称： karaf@root&gt; feature:list --help描述 feature:list 列出库中定义的所有功能。语法 `feature:list [options]`选项 --help 显示此帮助信息 -i, --installed 只列出已安装的功能列表更多…所有可用命令列表和它们的用法，也可以在一个专门的章节。 你在开发指南会获得更多的shell语法的深入引导。 如开发指南解释的那样，控制台可以很容易的被新命令扩展。 五 网络控制台 Karaf Web控制台提供了一个运行时的图形概述。 你可以是用它来： 安装和卸载功能启动，停止，安装捆绑创建子实例配置Karaf查看日志信息安装web控制台默认情况下web控制台是不安装的，安装请在karaf提示中运行以下命令： root@karaf&gt; features:install webconsole 访问Web控制台访问本地运行的一个karaf实例，在浏览器输入： http://localhost:8181/system/console 使用karaf用户名和karaf密码来登录系统，如果你修改过用户名或密码，请是用修改后的。 改变web控制台端口号默认情况下，控制台在8181端口运行，你可以通过创建属性文件etc/org.ops4j.pax.web.cfg并在后面添加如下属性设置（改成任意你想要的端口号）来改变端口： org.osgi.service.http.port=8181 六 远程控制台 使用远程实例初始化远程实例用它本地控制台管理karaf实例不总是有意义的，你可以用远程控制台远程管理karaf。 当你启动karaf时，它使任何其他Karaf控制台或纯SSH客户端可以通过SSH访问远程控制台。远程控制台提供本地控制台的所有功能，并通过运行它里面的容器和服务给远程用户完全控制。 SSH主机名和端口号在配置文件[karaf安装目录]/etc/org.apache.karaf.shell.cfg用以下默认值配置： sshPort=8101sshHost=0.0.0.0sshRealm=karafhostKey=${karaf.base}/etc/host.key你可以用初始化命令或者编辑上面的文件更改这个配置，但是需要重启ssh控制台以便它是用新的参数。 123456789# define helper functions bundle-by-sn = { bm = new java.util.HashMap ; each (bundles) { $bm put ($it symbolicName) $it } ; $bm get $1 }bundle-id-by-sn = { b = (bundle-by-sn $1) ; if { $b } { $b bundleId } { -1 } }# edit configconfig:edit org.apache.karaf.shellconfig:propset sshPort 8102config:update # force a restartosgi:restart --force (bundle-id-by-sn org.apache.karaf.shell.ssh) 远程连接和断开使用ssh：ssh命令 你可以使用ssh：ssh命令来连接远程karaf控制台。 karaf@root&gt; ssh:ssh -l karaf -P karaf -p 8101 hostname 注意：默认的密码是karaf，但是我们强烈建议个更改。在安全模块查看更多信息。 为了确定你已经连接到正确的karaf实例，输入ssh：info在karaf提示符。返回当前连接的实例的信息，如下所示。 1234567Karaf Karaf home /local/apache-karaf-2.0.0 Karaf base /local/apache-karaf-2.0.0 OSGi Framework org.eclipse.osgi - 3.5.1.R35x_v20090827JVM Java Virtual MachineJava HotSpot(TM) Server VM version 14.1-b02 ... 使用karaf客户端Karaf允许你安全的连接到远程karaf实例而不必运行本地karaf实例。 例如，在同一台机器上快速连接在server模式下运行的karaf实例，在karaf安装目录运行以下命令：bin/client。 通常情况下，你需要提供主机名，端口，用户名和密码来连接到远程实例。如果你使用的客户端在一个较大的脚本，你可以附加控制台命令如下： bin/client -a 8101 -h hostname -u karaf -p karaf features:install wrapper 显示可用的客户端选项，输入： 1234567891011&gt; bin/client --helpApache Karaf client -a [port] specify the port to connect to -h [host] specify the host to connect to -u [user] specify the user name -p [password] specify the password --helpshows this help message -vraise verbosity -r [attempts] retry connection establishment (up to attempts times) -d [delay]intra-retry delay (defaults to 2 seconds) [commands]commands to run 如果没有指定的命令，客户端将在互动模式。 使用纯SSH客户端你也可以使用你的unix系统中的纯SSH客户端或者windows SSH客户端像putty来连接。 ~$ ssh -p 8101 karaf@localhost karaf@localhost's password: 从远程控制台断开按Ctrl+D，shell：logout或者简单的在karaf提示符输入logout就可以断开远程控制台。 关闭远程实例使用远程控制台如果你已经用ssh:ssh命令或者karaf客户端连接到远程控制台，你可以用使用osgi:shutdown命令来停止远程实例。 注意：在远程控制台按Ctrl + D键，简单地关闭远程连接并返回到本地shell。 使用karaf客户端使用karaf客户端停止远程实例，在karaf安装目录/lib目录运行以下命令： bin/client -u karaf -p karaf -a 8101 hostname osgi:shutdown 七 子实例 管理子实例一个Karaf的子实例是一个副本，你可以分别启动和部署应用程序。实例不包含的完整副本Karaf，但只有一个配置文件和数据文件夹的副本，其中包含了所有运行中的信息，日志文件和临时文件。 使用管理控制台命令管理控制台命令允许您在同一台机器创建和管理Karaf实例。每一个新的运行时是运行时创建的子实例。您可以轻松地使用的名称管理子实例，而不是网络地址。有关管理命令的详细信息，请参阅管理命令。 创建子实例你可以在karaf控制台输入admin:create创建新的运行时实例. 如下例子所示，admin:create将使运行时在活动的运行时{实例/名称}目录创建新的运行时安装。新的实例是一个新的karaf实例并且分配一个SSH端口号基于始于8101的增量和一个RMI注册端口号基于始于1099的增量。 12345678910111213141516171819202122karaf@root&gt;admin:create finnCreating new instance on SSH port 8106 and RMI port 1100 at: /home/fuse/esb4/instances/finnCreating dir: /home/fuse/esb4/instances/finn/binCreating dir: /home/fuse/esb4/instances/finn/etcCreating dir: /home/fuse/esb4/instances/finn/systemCreating dir: /home/fuse/esb4/instances/finn/deployCreating dir: /home/fuse/esb4/instances/finn/dataCreating file: /home/fuse/esb4/instances/finn/etc/config.propertiesCreating file: /home/fuse/esb4/instances/finn/etc/java.util.logging.propertiesCreating file: /home/fuse/esb4/instances/finn/etc/org.apache.felix.fileinstall-deploy.cfgCreating file: /home/fuse/esb4/instances/finn/etc/org.apache.karaf.log.cfgCreating file: /home/fuse/esb4/instances/finn/etc/org.apache.karaf.features.cfgCreating file: /home/fuse/esb4/instances/finn/etc/org.ops4j.pax.logging.cfgCreating file: /home/fuse/esb4/instances/finn/etc/org.ops4j.pax.url.mvn.cfgCreating file: /home/fuse/esb4/instances/finn/etc/startup.propertiesCreating file: /home/fuse/esb4/instances/finn/etc/system.propertiesCreating file: /home/fuse/esb4/instances/finn/etc/org.apache.karaf.shell.cfgCreating file: /home/fuse/esb4/instances/finn/etc/org.apache.karaf.management.cfgCreating file: /home/fuse/esb4/instances/finn/bin/karafCreating file: /home/fuse/esb4/instances/finn/bin/startCreating file: /home/fuse/esb4/instances/finn/bin/stopkaraf@root&gt; 改变子实例端口号你可以使用admin:change-ssh-port命令来改变分配给子实例的SSH端口号。命令语法是： admin:change-ssh-port 实例 端口号，需要注意的必须停止子实例才能运行此命令。 同样，你可以使用admin:change-rmi-registry-port命令改变分配给子实例的RMI注册端口号。命令的语法是： admin:change-rmi-registry-port instance port，需要注意的必须停止子实例才能运行此命令。 启动子实例新的子实例在停止状态下被创建，用admin:start命令来启动子实例并使之准备主机应用。这个命令需要一个标识你想启动的子实例的instance-name参数。 列出所有容器实例要查看一个特定的安装下运行的所有Karaf实例的列表，使用admin:list命令。 1234567karaf@root&gt;admin:list SSH Port RMI Port State Pid Name[8107] [ 1106] [Started ] [10628] harry[8101] [ 1099] [Started ] [20076] root[8106] [ 1105] [Stopped ] [15924] dick[8105] [ 1104] [Started ] [18224] tomkaraf@root&gt; 连接到子实例你可以使用admin:connect命令连接到开始的子实例远程控制台，这需要三个参数： admin:connect [-u username] [-p password] instance, 一旦你连接到子实例，karaf提示符显示现在实例的名字，如下： karaf@harry&gt; 停止一个子实例在实例自己内部停止一个子实例，输入osgi:shutdown或者简单的shutdown。 远程停止子实例，换句话说，从父或者兄弟实例，使用admin:stop： admin:stop instance 注销一个子实例你可以使用admin:destroy命令永久的删除一个停止的子实例： *admin:destroy instance*请注意只有停止的实例可以被注销。 使用管理脚本你也可以管理本地的karaf实例，在karaf安装目录/bin目录下的管理员脚本提供了像管理员控制台相同的命令，除了admin:connect。 12345678910&gt; bin/adminAvailable commands: change-ssh-port - Changes the secure shell port of an existing container instance. change-rmi-registry-port - Changes the RMI registry port (used by management layer) of an existing container instance. create - Creates a new container instance. destroy - Destroys an existing container instance. list - List all existing container instances. start - Starts an existing container instance. stop - Stops an existing container instance.Type 'command --help' for more help on the specified command. 例如，列出所有本机器的karaf实例，输入： bin/admin list八 安全 管理用户名和密码默认安全配置使用一个位于 karaf安装目录/etc/users.properties属性文件存储授权的用户和他们的密码。 默认的用户名是karaf，与之相关联的密码也是karaf。我们强烈建议在将karaf转移到产品前通过编辑上面的文件修改默认密码。 在karaf中用户现在被使用在三个地方： 访问SSH控制台访问JMX管理层访问web控制台这三种方式的全部委托基于安全认证的相同的JAAS。 users.properties文件包含一或者多行，每行都定义了一个用户，密码和相关的角色。 user=password[,role][,role]… 管理角色JAAS角色可以被各种组件使用。三个管理层（SSH,JMX和web控制台）都使用用基于认证系统的全局角色。默认的角色名称在etc/system.properties中使用karaf.admin.role系统属性配置，并且默认值是admin。对管理层进行身份验证的所有用户必须有这个角色的定义。 这个值的语法如下： [classname:]principal 其中classname是主要对象的类名（默认以org.apache.karaf.jaas.modules.RolePrincipal），主要是这一类（默认为admin）的主要的名称。 注意，可以使用以下配置ConfigAdmin对于一个给定的层改变角色： Layer PID ValueSSH org.apache.karaf.shell sshRoleJMX org.apache.karaf.management jmxRoleWeb org.apache.karaf.webconsole role启用密码加密为了避免密码是纯文本，密码可以加密存储在配置文件中。 这可以通过以下命令轻易的实现： 123456# edit configconfig:edit org.apache.karaf.jaasconfig:propset encryption.enabled trueconfig:update # force a restartdev:restart 用户第一次登录，密码将在 etc/users.properties 配置文件中被自动的加密。加密密码在前面加上{CRYPT}，因此很容易识别。 管理领域更多关于更改默认领域或者部署新领域信息将会在开发者指南中被提供。 部署安全供应商有些应用程序需要特定的安全性提供者可用，如BouncyCastle。JVM在这些jar包的使用上施加一些限制：他们必须签署和引导类路径上可用。部署这些供应商的方法之一是把他们放在位于$ JAVAHOME/ JRE/ lib / ext的JRE文件夹中并且修改安全策略配置（$JAVAHOME/jre/lib/security/java.security）以登记等供应商。 虽然这种方法工作的很好，他将会有全局的影响并且需要你配置所有相应的服务器。 Karaf提供了一个简单的方式来配置额外的安全性提供者： 把你的供应商jar放在karaf-install-dir/lib/ext中修改初始化文件 karaf-install-dir/etc/config.properties ，添加如下属性： org.apache.karaf.security.providers = xxx,yyy 这个属性的值是一个逗号分隔的提供商类名的注册名单。 例如：org.apache.karaf.security.providers = org.bouncycastle.jce.provider.BouncyCastleProvider 此外，你可能想向系统中捆绑的供应商的类提供访问，使所有束可以访问那些。它可以通过在相同的配置文件中修改org.osgi.framework.bootdelegation 属性来实现： 1org.osgi.framework.bootdelegation = ...,org.bouncycastle* 九 故障转移部署 Karaf提供故障转移功能，使用一个简单的锁定文件系统或JDBC锁定机制。在这两种情况下，一个容器级锁系统允许绑定预装到副Karaf实例，以提供更快的故障转移性能。 简单文件锁定简单文件锁定机制用于驻留在同一台主机的实例故障转移配置。 要使用这个功能，按照如下形式编辑$KARAF_HOME/etc/system.properties文件中的每个系统上的主/从设置： 1234karaf.lock=truekaraf.lock.class=org.apache.felix.karaf.main.SimpleFileLockkaraf.lock.dir=&lt;PathToLockFileDirectory&gt;karaf.lock.delay=10 说明：确保karaf.lock.dir属性指向相同的主从实例目录，以便当主释放从，从只能获得锁定。 JDBC锁定JDBC锁定机制的目的就是为了存在单独机器上的故障转移配置。在此部署中，主实例拥有一个Karaf锁定数据库上的表上的锁，如果主失去了锁，等待从进程获得锁定表，并全面启动它的容器。 要使用这个功能，按照如下形式设置每个系统上的主/从设置： 更新CLASSPATH包含JDBC驱动程序 更新KARAF_HOME/bin/ karaf脚本有独特的JMX远程端口设置，如果实例驻留在同一主机上 更新KARAF_HOME的/ etc/ system.properties文件如下： 1234567891011karaf.lock=truekaraf.lock.class=org.apache.felix.karaf.main.DefaultJDBCLockkaraf.lock.level=50karaf.lock.delay=10karaf.lock.jdbc.url=jdbc:derby://dbserver:1527/samplekaraf.lock.jdbc.driver=org.apache.derby.jdbc.ClientDriverkaraf.lock.jdbc.user=userkaraf.lock.jdbc.password=passwordkaraf.lock.jdbc.table=KARAF_LOCKkaraf.lock.jdbc.clustername=karafkaraf.lock.jdbc.timeout=30 说明： 如果JDBC驱动程序不在classpath中会失败。将创建数据库名称“sample”，如果它不存在于数据库。Karaf的第一个实例来获得锁定表的是主实例。如果数据库连接丢失，主实例尝试正常关闭，主数据库服务恢复时允许一个从的实例成为主，前主将需要手动重新启动。Oracle的JDBC锁定在JDBC锁定情况下如果你采用oracle作为你的数据库，在$KARAF_HOME/etc/system.properties 文件中的karaf.lock.class 属性必须指向org.apache.felix.karaf.main.OracleJDBCLock。 否则，对于你的设置初始化system.properties文件是正常的，例如： 123456789karaf.lock=truekaraf.lock.class=org.apache.felix.karaf.main.OracleJDBCLockkaraf.lock.jdbc.url=jdbc:oracle:thin:@hostname:1521:XEkaraf.lock.jdbc.driver=oracle.jdbc.OracleDriverkaraf.lock.jdbc.user=userkaraf.lock.jdbc.password=passwordkaraf.lock.jdbc.table=KARAF_LOCKkaraf.lock.jdbc.clustername=karafkaraf.lock.jdbc.timeout=30 正如默认的JDBC锁定设置，Oracle JDBC驱动包必须在calsspath中，为了确保如此你可以在karaf启动之前复制ojdbc14.jar到karaf的lib文件夹下。 说明：karaf.lock.jdbc.url 需要活动的SID，这意味着在使用这个特定的锁之前你必须手动创建数据库实例。 容器级锁定容器级锁定允许绑定预装到从内核的实例，以提供更快的故障转移性能。容器级锁被简单的文件和JDBC锁定机制支持。 为了实现容器及说定，添加如下内容到 $KARAF_HOME/etc/system.properties 文件中在每个系统的主从设置上： 123karaf.lock=truekaraf.lock.level=50karaf.lock.delay=10 Karaf.log.level属性告诉karaf实例引导过程带来的OSGI容器有多远。分配相同级别的绑定或者是更低的也会在那个karaf实例中被启动。 绑定开始级别在$KARAF_HOME/etc/startup.properties指定，以jar.name=levle的形式。核心系统绑定级数低于50，用户绑定级别大于50。 级别 行为1 一个“冷”的备用实例，核心绑定不会被加载到容器中，从实例将等待指导锁定需要启动服务器。&lt;50 一个“热”的备用实例，核心捆绑将被加载到容器，从实例等待指导锁需要启动用户级别绑定。控制台对于每个从实例可以在这个级别访问。 50 这个设置不建议作为用户捆绑被启动。注意：挡在同一主机上使用“热”备用，你需要设置JMX远程端口为唯一值以避免绑定冲突，你可以编辑karaf启动脚本以包括以下内容： 123DEFAULT_JAVA_OPTS=\"-server $DEFAULT_JAVA_OPTS -Dcom.sun.management.jmxremote.port=1100 -Dcom.sun.management.jmxremote.authenticate=false","link":"/2019/07/25/KARAF/"},{"title":"Mysql数据库自动写入创建时间以及更新时间","text":"12`create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',`update_time` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00' ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间' 如上图,第一个是创建时间,第二个是更新时间,数据插入时 不需要在对象中设置对应的时间值,数据库会自动插入创建时间,修改时会自动更新 ‘更新时间’,如果不放心 可以在代码中将这两个属性设置为null 123//数据库自动增加时间 tOrderFault.setCreateTime(null); tOrderFault.setUpdateTime(null); 如上图所示,也是可以的 创建时间 只有在该条数据插入后生成,更新时间只有当该条记录发生修改时数据库才会自动更新时间,第一次插入数据时,更新时间列会显示0000-00-00 00:00:00需要注意的是mysql5.6以前的版本是不支持同时有两个timestamp的时间类型,如果报错可以检查下数据库的版本","link":"/2019/09/17/Mysql写入更新时间/"},{"title":"OSGI Command","text":"OSGI中Command -控制台命令 在OSGI的中开发bundle，在Karaf容器中加载bundle后，往往需要获取bundle处理的中间信息，用于调试、故障定位等。而org.apache.karaf.shell.console提供一种可以以控制台命令的方式介入bundle。提供一种在运行时，以命令触发原代码中的逻辑功能。 具体开发，与开发bundle过程一样，有几点需要注意： 1、@Command @Option @Argument 的使用，父类OsgiCommandSupport和重载方法doExecute 2、pom中的build的plugin需要增加maven-scr-plugin 3、pom的build的maven-bundle-plugin显示Import org.apache.karaf.shell.* 123456789101112131415161718192021222324252627282930313233343536373839404142package com.zte.sdn.oscp.yang.adapter; import org.apache.karaf.shell.commands.Argument;import org.apache.karaf.shell.commands.Command;import org.apache.karaf.shell.commands.Option;import org.apache.karaf.shell.console.OsgiCommandSupport; /** * Created by sunquan on 2017/10/20. */@Command(scope = \"livio\", name = \"example\", description = \"livio exmaple command\")public class ExampleCommand extends OsgiCommandSupport { @Option(name = \"-n\", aliases = {\"--Name\"}, description = \"Show the information of specific shard module\", required = false, multiValued = false) private String shardModuleName = \"\"; @Argument(index = 0, name = \"command\", description = \"[help]\", required = true, multiValued = false) private String command = \"help\"; @Override protected Object doExecute() throws Exception { if (!shardModuleName.isEmpty()) { System.out.println(\"Name:\" + shardModuleName); } switch (command) { case \"help\": session.getConsole().println(\"help command output\"); break; case \"print\": session.getConsole().println(\"print command output\"); break; default: break; } return null; }} 其中multiValued为false，command字段则为字符串help或print，即输入命令中字面量但如果为true,表示该参数可以配置多个值command字段，会自动加上[],并以逗号分隔。如livio:example print help 则command字段为[print,help] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zte.sunquan.demo&lt;/groupId&gt; &lt;artifactId&gt;yang-adapter&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;bundle&lt;/packaging&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;!--&lt;dependency&gt;--&gt; &lt;!--&lt;groupId&gt;com.zte.sunquan.demo&lt;/groupId&gt;--&gt; &lt;!--&lt;artifactId&gt;yang-middle&lt;/artifactId&gt;--&gt; &lt;!--&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;--&gt; &lt;!--&lt;/dependency&gt;--&gt; &lt;dependency&gt; &lt;groupId&gt;org.osgi&lt;/groupId&gt; &lt;artifactId&gt;org.osgi.compendium&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.osgi&lt;/groupId&gt; &lt;artifactId&gt;org.osgi.core&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.felix&lt;/groupId&gt; &lt;artifactId&gt;org.apache.felix.scr&lt;/artifactId&gt; &lt;version&gt;1.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.felix&lt;/groupId&gt; &lt;artifactId&gt;org.apache.felix.scr.annotations&lt;/artifactId&gt; &lt;version&gt;1.9.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.karaf.shell&lt;/groupId&gt; &lt;artifactId&gt;org.apache.karaf.shell.console&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.zte.sunquan.demo&lt;/groupId&gt; &lt;artifactId&gt;yang-admodel&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.felix&lt;/groupId&gt; &lt;artifactId&gt;maven-bundle-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;extensions&gt;true&lt;/extensions&gt; &lt;configuration&gt; &lt;instructions&gt; &lt;Bundle-ManifestVersion&gt;2&lt;/Bundle-ManifestVersion&gt; &lt;Bundle-Name&gt;${project.description}&lt;/Bundle-Name&gt; &lt;Bundle-SymbolicName&gt;${project.groupId}.${project.artifactId}&lt;/Bundle-SymbolicName&gt; &lt;Bundle-Version&gt;${project.version}&lt;/Bundle-Version&gt; &lt;Bundle-Vendor&gt;${project.groupId}&lt;/Bundle-Vendor&gt; &lt;Import-Package&gt; !com.zte.sdn.oscp.commons.serialize.binary.protostuff*, org.apache.karaf.shell.*, com.zte.sdn.oscp.yang.gen.v1.ip.device.rev20170324.*, com.zte.sdn.oscp.yang.gen.v1.ip.device.rev20170324.NetconfState.*, *&lt;/Import-Package&gt; &lt;!--&lt;Private-Package&gt;&lt;/Private-Package&gt;--&gt; &lt;!--&lt;Embed-Dependecy&gt;&lt;/Embed-Dependecy&gt;--&gt; &lt;/instructions&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.felix&lt;/groupId&gt; &lt;artifactId&gt;maven-scr-plugin&lt;/artifactId&gt; &lt;version&gt;1.21.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;generate-scr-scrdescriptor&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;scr&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 在reources/OSGI-INF/blueprint目录中增加文件shell-config.xml并输入 123456789&lt;blueprint xmlns=\"http://www.osgi.org/xmlns/blueprint/v1.0.0\"&gt; &lt;command-bundle xmlns=\"http://karaf.apache.org/xmlns/shell/v1.1.0\"&gt; &lt;command&gt; &lt;action class=\"com.zte.sdn.oscp.yang.adapter.ExampleCommand\"/&gt; &lt;/command&gt; &lt;/command-bundle&gt; &lt;/blueprint&gt;","link":"/2019/07/27/OSGI-Command/"},{"title":"OSGI Command2","text":"OSGI中command的应用 在上一篇博文中，我们讲解了osgi中的blueprint，但并没有对此作出具体的运用，在本文及以后将会在讲osgi中其他应用的时候将blueprint串进来讲解，本文将要讲讲解的是osgi中的command，在编写的应用中，可能涉及到数据迁移或者其他一些操作，如果这个通过调用接口来进行操作的话，如果非相关人员获取到相关接口调用方式，可能会带来一些危害，所以在不得已的情况下不会使用接口的方式，这个时候command的作用就体现出来了，如果自己编写数个命令，然后在运行容器的command中执行命令，就可以直接完成我们所需要的操作，下面我们就开始使用相关api完成我们所用的command。 相关程序 使用osgi的command必须在相关类中实现Action接口，这是felix提供的一个interface，具体为import org.apache.felix.gogo.commands.Action在其实现的方法中写入我们所需要的程序逻辑，我们命令执行的内容都在这里，其中要提一下的是，并不只是可以s实现Action接口一种，还可以继承OsgiCommandSupport类，实现方法doExecute，也可以实现相同的效果，其中OsgiCommandSupport包名及类名为以下： import org.apache.karaf.shell.console.OsgiCommandSupport其实在这里面还有一种为:org.apache.karaf.shell.api.action.Action这个应该也是可以的，但是在使用的时候，karaf会报错，因此这个再次暂且不讲，等日后有机会搞清楚了再来说这个，我估计的话，现在最好实现的都是这一种，因为前两种都已经被不推荐使用了。在实现相应的方法后，还需要在类上加上以下注解： 123@Service@Log4j@Command(scope = \"test\", name = \"hello\", description = \"start for hello world\") @Log4j注解可有可无，在这主要是用来记录日志，加上@Service和@Command注解是必须的，标注为command和在karaf控制台中使用，里面具体参数使用可以查看到相应注解的源码，上面都有着详细的注释和说明。commmand基本就是这么多东西，其中源码为： 12345678910111213141516171819import lombok.extern.log4j.Log4j;import org.apache.felix.gogo.commands.Action;import org.apache.felix.service.command.CommandSession;import org.apache.karaf.shell.api.action.Command;import org.apache.karaf.shell.api.action.lifecycle.Service;/** * Created by xiaxuan on 16/4/20. */@Service@Log4j@Command(scope = \"test\", name = \"hello\", description = \"start for hello world\")public class HelloCommand implements Action{ public Object execute(CommandSession session) throws Exception { log.info(\"Hello word\"); return null; }} 这个完成之后，还需要在blueprint.xml文件中配置command，如下所示： 12345&lt;command-bundle xmlns=\"http://karaf.apache.org/xmlns/shell/v1.0.0\"&gt; &lt;command name=\"test/hello\"&gt; &lt;action class=\"cn.com.files.command.HelloCommand\"&gt;&lt;/action&gt; &lt;/command&gt; &lt;/command-bundle&gt; 程序运行在启动容器之后，在命令行中，打开karaf客户端，为以下： 首先输入list命令，确保bundle正常启动，本示例写在storage这个bundle中，为以下： id为13的bundle的状态为active，可以确定我们的bundle处于活跃状态，现在控制台输入test，为我们之前command的scope，按tab键，提示如下： 有多个提示，第一个可以不用理会，第二个即为我们编写的command命令，第三个为之后我将要讲述的。控制台输入test:hello，查看控制台输出：控制台得到正常输出结果，hello world,命令执行正常。 使用规范及其他 个人在使用command上，更多是使用在数据导入或者数据修复的时候使用，正常的业务逻辑一般不会使用command来实现。 我在以上使用的command实现的接口或者继承的类中，两个都是已经过时的接口或者抽象类，现在一般通常使用的应该是karaf的那一个Action，但是在使用上可能会有些问题，个人认为可能是在配置上和当前有所区别，应该和注解无关，在@Service注解中的注释中，还提到了karaf的这一个Action类。但是本文在此没有使用这个Action，望有使用过的可以和我讨论一下。 command可以实现接口或者继承类来做到，也可以自定义自己的command，然后在blueprint做相关的配置即可，要比上述编写的command稍微简单点，用法也多样化一点。 在上述command中，command可以传入参数，并且参数可以有多个，这个在下一篇博文中或许会有所提及。 以上编写的command，都是基于karaf容器来运行的。","link":"/2019/07/27/OSGI-Command2/"},{"title":"ONOS 开发指南","text":"搭建环境可以用官方提供的虚拟机viturebox https://www.virtualbox.org/ONOS tutorial OVA https://drive.google.com/open?id=1JcGUJJDTtbHNnbFzC7SUK52RmMDBVUry 重要命令的提示onos&gt; //表示你现在在onos的命令行mininet&gt; //表示现在的位置是在mininet下 设置集群点击桌面 Setup ONOS Cluster 图标 Launch ONOS GUI点击提供的ONOS GUI 图标ip:8181/onos/ui username: onospassword：rocks CLI AND SERVICE如果我们像提供服务给其他接口，我们需要先定义服务接口然后让其他模块实现接口1.定义一个接口官方案例写在了core工程下net里 1234567891011121314151617181920package org.onosproject.net.apps; import java.util.Map; import org.onosproject.net.HostId; /** * A demonstrative service for the intent reactive forwarding application to * export. */public interface ForwardingMapService { /** * Get the endpoints of the host-to-host intents that were installed. * * @return maps of source to destination */ public Map&lt;HostId, HostId&gt; getEndPoints(); } 2.实现接口@Component(immediate = true) 启动时加载为组件@Service 注册为服务 有了这个注解别的服务就可以通过@Reference(cardinality = ReferenceCardinality.MANDATORY_UNARY) 这个注解来引用了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package org.onosproject.ifwd; import java.util.Collections;import java.util.Map;import java.util.concurrent.ConcurrentHashMap;import java.util.concurrent.ConcurrentMap; import org.apache.felix.scr.annotations.Activate;import org.apache.felix.scr.annotations.Component;import org.apache.felix.scr.annotations.Deactivate;import org.apache.felix.scr.annotations.Reference;import org.apache.felix.scr.annotations.ReferenceCardinality;import org.apache.felix.scr.annotations.Service;import org.onosproject.core.ApplicationId;import org.onosproject.core.CoreService;import org.onosproject.net.Host;import org.onosproject.net.HostId;import org.onosproject.net.PortNumber;import org.onosproject.net.apps.ForwardingMapService;import org.onosproject.net.flow.DefaultTrafficSelector;import org.onosproject.net.flow.DefaultTrafficTreatment;import org.onosproject.net.flow.TrafficSelector;import org.onosproject.net.flow.TrafficTreatment;import org.onosproject.net.host.HostService;import org.onosproject.net.intent.HostToHostIntent;import org.onosproject.net.intent.IntentService;import org.onosproject.net.packet.DefaultOutboundPacket;import org.onosproject.net.packet.InboundPacket;import org.onosproject.net.packet.OutboundPacket;import org.onosproject.net.packet.PacketContext;import org.onosproject.net.packet.PacketProcessor;import org.onosproject.net.packet.PacketService;import org.onosproject.net.topology.TopologyService;import org.onlab.packet.Ethernet;import org.slf4j.Logger; import static org.slf4j.LoggerFactory.getLogger; @Component(immediate = true)@Servicepublic class IntentReactiveForwarding implements ForwardingMapService { private final Logger log = getLogger(getClass()); @Reference(cardinality = ReferenceCardinality.MANDATORY_UNARY) protected CoreService coreService; // ...&lt;snip&gt;... // Install a rule forwarding the packet to the specified port. private void setUpConnectivity(PacketContext context, HostId srcId, HostId dstId) { TrafficSelector selector = DefaultTrafficSelector.builder().build(); TrafficTreatment treatment = DefaultTrafficTreatment.builder().build(); HostToHostIntent intent = new HostToHostIntent(appId, srcId, dstId, selector, treatment); intentService.submit(intent); } // the new service method, to be filled out @Override public Map&lt;HostId, HostId&gt; getEndPoints() { return null; }} 实现这个服务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Component(immediate = true)@Servicepublic class IntentReactiveForwarding implements ForwardingMapService { // ...&lt;snip&gt;... private ApplicationId appId; // Map for storing found endpoints, for our service. It is protected // so that process() can access it. protected final ConcurrentMap&lt;HostId, HostId&gt; endPoints = new ConcurrentHashMap&lt;&gt;(); // ...&lt;snip&gt;... /** * Packet processor responsible for forwarding packets along their paths. */ private class ReactivePacketProcessor implements PacketProcessor { @Override public void process(PacketContext context) { // Stop processing if the packet has been handled, since we // can't do any more to it. if (context.isHandled()) { // ...&lt;snip&gt;... if (dst == null) { flood(context); return; } // Add found endpoints to map. endPoints.put(srcId, dstId); // Otherwise forward and be done with it. setUpConnectivity(context, srcId, dstId); forwardPacketToDst(context, dst); } } // ...&lt;snip&gt;... @Override public Map&lt;HostId, HostId&gt; getEndPoints() { // Return our map as a read-only structure. return Collections.unmodifiableMap(endPoints); }} 创建命令一般命令定义在${ONOS_ROOT}/cli/下命令分为两种：1系统配置和监控相关2网络配置和监控相关 @Command@Argument@Option 1234567891011121314151617181920212223package org.onosproject.cli.net; import org.apache.karaf.shell.commands.Argument;import org.apache.karaf.shell.commands.Command;import org.onosproject.cli.AbstractShellCommand;import org.onosproject.net.HostId;import org.onosproject.net.apps.ForwardingMapService; /** * A demo service that lists the endpoints for which intents are installed. */@Command(scope = \"onos\", name = \"fwdmap\", description = \"Lists the endpoints for which intents are installed\")public class ForwardingMapCommand extends AbstractShellCommand { @Argument(index = 0, name = \"hostId\", description = \"Host ID of source\", required = false, multiValued = false) private HostId hostId = null; @Override protected void execute() { }} 合并新服务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package org.onosproject.cli.net; import java.util.Map; import org.apache.karaf.shell.commands.Argument;import org.apache.karaf.shell.commands.Command;import org.onosproject.cli.AbstractShellCommand;import org.onosproject.net.HostId;import org.onosproject.net.apps.ForwardingMapService; /** * A demo service that lists the endpoints for which intents are installed. */@Command(scope = \"onos\", name = \"fwdmap\", description = \"Lists the endpoints for which intents are installed\")public class ForwardingMapCommand extends AbstractShellCommand { // formatted string for output to CLI private static final String FMT = \"src=%s, dst=%s\"; // the String to hold the optional argument @Argument(index = 0, name = \"hostId\", description = \"Host ID of source\", required = false, multiValued = false) private String hostId = null; // reference to our service private ForwardingMapService service; // to hold the service's response private Map&lt;HostId, HostId&gt; hmap; @Override protected void execute() { // get a reference to our service service = get(ForwardingMapService.class); /* * getEndPoints() returns an empty map even if it contains nothing, so * we don't need to check for null hmap here. */ hmap = service.getEndPoints(); // check for an argument, then display information accordingly if (hostId != null) { // we were given a hostId to filter on, print only those that match HostId host = HostId.hostId(hostId); for (Map.Entry&lt;HostId, HostId&gt; el : hmap.entrySet()) { if (el.getKey().equals(hostId)) { print(FMT, el.getKey(), el.getValue()); } } } else { // print everything we have for (Map.Entry&lt;HostId, HostId&gt; el : hmap.entrySet()) { print(FMT, el.getKey(), el.getValue()); } } }} 注册命令到karaf${ONOS_ROOT}/cli/src/main/resources/OSGI-INF/blueprint/ shell-config.xml中 123456789&lt;command&gt; &lt;!--Our command implementation's FQDN--&gt; &lt;action class=\"org.onosproject.cli.net.ForwardingMapCommand\"/&gt; &lt;!--A command completer for Host IDs--&gt; &lt;completers&gt; &lt;ref component-id=\"hostIdCompleter\"/&gt; &lt;null/&gt; &lt;/completers&gt;&lt;/command&gt; 重新编译启动ONOS 123$ cd ${ONOS_ROOT}$ mvn clean install$ karaf clean 测试 应用模板教程onos-create-app工具可以创建基础应用、命令、REST API 和GUI1选 ONOS 版本export ONOS_POM_VERSION=2.0.0 2创建工程 1） 创建基础工程 onos-create-app app org.foo foo-app 1.0-SNAPSHOT org.foo.app 编译安装激活 123$ cd foo-app$ mvn clean install$ onos-app localhost install! target/foo-app-1.0-SNAPSHOT.oar 2) 创建带命令的工程 onos-create-app cli org.foo foo-app 1.0-SNAPSHOT org.foo.app 重新编译安装 12$ mvn clean install$ onos-app localhost reinstall! target/foo-app-1.0-SNAPSHOT.oar 3) 创建REST API 工程 onos-create-app rest org.foo foo-app 1.0-SNAPSHOT org.foo.app 重新编译安装 12$ mvn clean install$ onos-app localhost reinstall! target/foo-app-1.0-SNAPSHOT.oar 打包完会自动创建 swagger ui http://localhost:8181/v1/docs/ 4）创建ui工程 onos-create-app ui org.foo foo-app 1.0-SNAPSHOT org.foo.app 带表格视图 onos-create-app uitab org.foo foo-app 1.0-SNAPSHOT org.foo.app 带拓扑图 onos-create-app uitopo org.foo foo-app 1.0-SNAPSHOT org.foo.app 重新编译安装 12mvn clean installonos-app localhost reinstall! org.foo.app target/foo-app-1.0-SNAPSHOT.oar 发布到本地仓库onos-publish -l CoreService 类 12345678910111213141516//核心应用名String CORE_APP_NAME = \"org.onosproject.core\";//核心“提供者”的标识符。 ProviderId CORE_PROVIDER_ID = new ProviderId(\"core\", CORE_APP_NAME);//返回产品版本 Version version();//返回当前注册的应用程序标识符集。 Set&lt;ApplicationId&gt; getAppIds();//从给定的ID返回现有的应用程序ID。ApplicationId getAppId(Short id);//从给定的ID返回现有的应用程序ID。ApplicationId getAppId(String name);//根据名字注册一个新应用ApplicationId registerApplication(String name, Runnable preDeactivate);//返回给定主题的ID生成器。IdGenerator getIdGenerator(String topic); 注意 如何获得一个AppIdappId = coreService.registerApplication(PROVIDER_ID);根据PROVIDER_ID产生再从coreService的registerApplication方法中得到appIdPROVIDER_IDstatic final String PROVIDER_ID = &quot;org.onosproject.provider.bgp.cfg&quot;; 如何设置配置文件首先自定义好配置类引入组件配置服务 12@Reference(cardinality = ReferenceCardinality.MANDATORY)protected ComponentConfigService componentConfigService; 组件配置服务注册配置文件 1componentConfigService.registerProperties(getClass()); 首先需要建立配置文件类 然后需要添加配置监听 `private final NetworkConfigListener configListener = new InternalConfigListener();` `configService.addListener(configListener);` 通过配置工厂ConfigFactory configFactory需要三个参数SubjectFactories.APP_SUBJECT_FACTORY, 配置类的字节码文件, key 1234567private final ConfigFactory configFactory = new ConfigFactory(SubjectFactories.APP_SUBJECT_FACTORY, BgpAppConfig.class, \"bgpapp\") { @Override public BgpAppConfig createConfig() { return new BgpAppConfig(); }}; 注册配置工厂configRegistry.registerConfigFactory(configFactory);读取配置readConfiguration(); 注意 以上方法需要在bundle初始化的时候执行即在activate（也有叫start）方法中执行： 12345678@Activatepublic void activate(ComponentContext context) { appId = coreService.registerApplication(PROVIDER_ID); configService.addListener(configListener); configRegistry.registerConfigFactory(configFactory); readConfiguration();//内部自己定义 log.info(\"BGP cfg provider started\");} 当bundle关闭不再需要的时候可以在deactivate（也有叫stop）方法中取消注册 12345@Deactivatepublic void deactivate(ComponentContext context) { configRegistry.unregisterConfigFactory(configFactory); configService.removeListener(configListener);} 2.net config配置类继承Config在组件中引入服务 12@Referenceprotected NetworkConfigRegistry configRegistry; 1234567private final ConfigFactory configFactory = new configFactory(SubjectFactories.APP_SUBJECT_FACTORY,配置类名,起个名字作为key){ @Override public 配置类 create配置类(){ return new 配置类(); }} activate 方法里注册配置文件 1configRegistry.registerConfigFactory(configFactory); onos中的日志onos通过slf4j 来记录日志 private static final Logger log = getLogger(本类的字节码); onos命令行开发主要用到几个注解 类上的 @Service @Component 方法上 @Option @Argument scop 作用域一般为应用名name 命令名description 命令的描述detaileddescription 细节描述 onos 的WEB 开发onos基于JAX-RS 做的传统的servlet rest 开发方式需要配置web.xml需配置servlet 和 servlet-mapping 一：配置web.xml ` Visual App REST API v1.0 &lt;security-constraint&gt; &lt;web-resource-collection&gt; &lt;web-resource-name&gt;Secured&lt;/web-resource-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/web-resource-collection&gt; &lt;auth-constraint&gt; &lt;role-name&gt;admin&lt;/role-name&gt; &lt;/auth-constraint&gt; &lt;/security-constraint&gt; &lt;security-role&gt; &lt;role-name&gt;admin&lt;/role-name&gt; &lt;/security-role&gt; &lt;login-config&gt; &lt;auth-method&gt;BASIC&lt;/auth-method&gt; &lt;realm-name&gt;karaf&lt;/realm-name&gt; &lt;/login-config&gt; &lt;servlet&gt; &lt;servlet-name&gt;JAX-RS Service&lt;/servlet-name&gt; &lt;servlet-class&gt;org.glassfish.jersey.servlet.ServletContainer&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;javax.ws.rs.Application&lt;/param-name&gt; &lt;!--&lt;param-name&gt;jersey.config.server.provider.classnames&lt;/param-name&gt;--&gt; &lt;param-value&gt;org.onosproject.visual.rest.VisualWebApplication&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;JAX-RS Service&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; ` 2.BUILD文件里 1234567891011121314151617osgi_jar_with_tests( api_description = \"REST API for Visual\", api_package = \"org.onosproject.visual.rest\", api_title = \"Visual Management\", api_version = \"1.0\", web_context = \"/ensbrain/visual\", karaf_command_packages = [\"org.onosproject.visual\"], deps = COMPILE_DEPS,)``` 必须要有的是web_context此值相当于访问路径的根 3.必须要有一个Application类并继承AbstractWebApplication例如访问类叫VisualWebResource那么VisualWebApplication： public class VisualWebApplication extends AbstractWebApplication { @Override public Set&lt;Class&lt;?&gt;&gt; getClasses() { return getClasses(VisualWebResource.class); }} 12 @Path(“query”)public class VisualWebResource extends AbstractWebResource{ // private VisualServerConfig esAgentConfig; @GET @Path(&quot;hello&quot;) @Produces(MediaType.APPLICATION_JSON) public Response getHistogram() { String result = &quot;hello welcome to enlink sdp visual of users&apos; behaviour &quot;; ObjectNode node = mapper().createObjectNode().put(&quot;response&quot;, result); return ok(node).build(); }1234567891011121314如果要传递对象到前台需要通过转成json字符串需要 1.实现JsonCodec&lt;dto类&gt; ``` 1.dto类 2.dto类的json转码类要继承JsonCodec&lt;dto类&gt; 3.重写里面的转码方法其实就是把属性都拿出来重新放到objectNode类中返回的就是objectNode 4.如果是多个对象就转成ArrayNode json数组返回arrayNode ``` 2.注册在激活时将实体类与json转换类形成映射关系 例如： @Component(immediate = true,service = VisualRegistryService.class)public class VisualRegistryService { private static final Logger log = getLogger(VisualRegistryService.class); @Reference(cardinality = ReferenceCardinality.MANDATORY) protected CoreService coreService; @Reference(cardinality = ReferenceCardinality.MANDATORY) protected CodecService codecService; private ApplicationId appId; @Activate public void active(){ appId = coreService.getAppId(&quot;org.onosproject.visual&quot;); codecService.registerCodec(SearchSQLDto.class,new SearchSQLDtoJsonCodec()); log.info(&quot;visual started&quot;); } @Deactivate public void deactive(){ log.info(&quot;Stopped&quot;); }}","link":"/2019/07/30/ONOS-开发指南/"},{"title":"OSGI Command3","text":"OSGI中自定义command前文 在上一篇博文中，我们讲述了什么是OSGI中的command，同时写了一个简单的command，这个command实现了org.apache.felix.gogo.commands.Action这个接口，同样可以实现相同功能的还有org.apache.karaf.shell.console.OsgiCommandSupport这一个抽象类，但是在本程序中，这两个接口或者抽象类上都标注了@Deprecated，因此已经不推荐使用了，而当前推荐使用的则是karaf中的一个接口，为org.apache.karaf.shell.api.action.Action，但是这个在使用过程中会出现一定的问题，这个稍后再说，本文主要讲解的是，不再使用karaf提供的@Service,@Command和继承或者实现相关接口来编写Command，而是在blueprint.xml的方式配置一个Command，这个文字不太好描述，下面就直接用相关程序来讲解本次编写的command。自定义command首先不多说，新建一个类，命名为SampleCommand，里面分别写三个方法，两个无参方法，一个有参，为以下： 1234567891011public void first() { System.out.println(\"first\"); } public void second() { System.out.println(\"second\"); } public void hello(String name) { System.out.println(\"hello \" + name); } 以上，等会再控制台中显示，上述两个无参方法，就是普通命令，而有参方法则是可以传入一个参数，并且会在控制台中打出，在以上完成之后，还要在blueprint.xml中配置相关命令，blueprint.xml中内容如下： 1234567891011121314151617&lt;bean class=\"cn.com.files.command.SampleCommand\" id=\"sampleCommand\"/&gt; &lt;service auto-export=\"all-classes\" ref=\"sampleCommand\"&gt; &lt;service-properties&gt; &lt;entry key=\"osgi.command.scope\"&gt; &lt;!-- define the prefix for you commands --&gt; &lt;value&gt;sample&lt;/value&gt; &lt;/entry&gt; &lt;entry key=\"osgi.command.function\"&gt; &lt;!-- declare the list of all the methods you want to expose --&gt; &lt;array value-type=\"java.lang.String\"&gt; &lt;value&gt;first&lt;/value&gt; &lt;value&gt;second&lt;/value&gt; &lt;value&gt;hello&lt;/value&gt; &lt;/array&gt; &lt;/entry&gt; &lt;/service-properties&gt; &lt;/service&gt; 我们首先进行了一个bean的配置，即将SampleCommand配置为bean，其次我们配置了一个service，并在osgi.command.scope中的value中指定值为sample，这就是我们先前写的scope，而在指定的osgi.command.function中指定的数组中的值，即为我们编写的几个方法名，如此配置完成之后，我们的command就基本完成了，现在只需要重新在控制台mvn clean install -U -DskipTests之后，再次打开karaf的控制台，观察当前bundle的启动状态，以及我们编写的command。运行打开karaf控制台，输入list，bundle运行状态如下所示： 其中ID为13的，name为Storage的这一个state为Active，说明Bundle正常启动，现在尝试运行我们编写的命令，输入sample，这是我们之前定义的scope，按下tab键，有以下提示: 在提示中，三个方法依次提示出来，我们执行sample:first,sample:second都将会在控制台中打出内容，如下所示： 以上依次执行我们方法中的内容，现在开始我们的方法3即sample:hello,在输入以上命令后，按下tab键，此时并没有什么提示，但是我们却是需要输入一个参数，这个提示在上一篇中的command的定义方法，并指定一个参数，并编写相关方法后确实是有提示的这个稍后再提，在我们输入sample:hello world后，控制台打出的内容为： 如此，带有参数的命令也正常执行了。 其他实现 其实在实现带有参数的命令中，还有另外一种实现，在此可以说一下，这里提供了另外一种注解，即为@Argument,将这个加在我们在命令中的参数上，就像是以下这样： 1234@Argument(index = 0, name = \"arg\", description = \"the command argument\", required = false, multiValued = false)String arg;``` 然后还需要一个completer类，在这作为对参数的提示，我编写的如下所示： /** Created by xiaxuan on 16/7/6. a simple Completer / public class SimpleCompeter implements Completer { public int complete(String buffer, int cursor, List candidates) { StringsCompleter delegate = new StringsCompleter(); delegate.getStrings().add(&quot;one&quot;); delegate.getStrings().add(&quot;two&quot;); delegate.getStrings().add(&quot;three&quot;); return delegate.complete(buffer, cursor, candidates); }} 1同时还需在blueprint.xml中的命令中配置相关的completer，如下： ``` 如此也是完成了一个带有参数的命令，但是问题在于，我在karaf控制台中执行这个命令的时候，按下tab键的时候确实是有相关的提示，就是我们在completer中编写的one、two、three。但是输入其中任何一个，在控制台中输出的始终是null，我在看karaf源码的时候，karaf中的任意一个模块基本都是使用了大量的命令，我使用的方法和他们的类似，但是输出却是出了一定的问题，这个如果有过研究的人希望能与我探讨一下这个问题。 总结 其实这种命令，在更多上，是属于自己的扩展，应用于制作第三方的客户端的较多，在平常应用的编写中，这种command编写的还是比较少，因为在生产环境中，多半已经把ssh登录限制了，所以基本不可能远程登录karaf控制台来执行自己的命令。 在命令的使用上，更多还是使用后续讲解这一种命令较好，这种官方提供更多的支持，并且提供了许多种注解可以使用，可以加上@Argument,@Option等等。 命令更多的只是对我们的应用程序的一种辅助，在真正的生产环境中，建议还是不要大量的运用这种命令，该用接口的还是使用接口实现。","link":"/2019/07/29/OSGI-Command3/"},{"title":"OSGI","text":"OSGI （面向Java的动态模型系统） OSGi（开放服务网关协议，Open Service Gateway Initiative）技术是Java动态化模块化系统的一系列规范。OSGi一方面指维护OSGi规范的OSGI官方联盟，另一方面指的是该组织维护的基于Java语言的服务（业务）规范。简单来说，OSGi可以认为是Java平台的模块层。OSGi服务平台向Java提供服务，这些服务使Java成为软件集成和软件开发的首选环境。SGi技术提供允许应用程序使用精炼、可重用和可协作的组件构建的标准化原语，这些组件能够组装进一个应用和部署中。开放服务网关协议有双重含义。一方面它指OSGi Alliance组织；另一方面指该组织制定的一个基于Java语言的服务规范——OSGi服务平台。 OSGi Alliance是一个由Sun Microsystems、IBM、爱立信等于1999年3月成立的开放的标准化组织，最初名为Connected Alliance。该组织及其标准原本主要目的在于使服务提供商通过住宅网关，为各种家庭智能设备提供各种服务。该平台逐渐成为一个为室内、交通工具、移动电话和其他环境下的所有类型的网络设备的应用程序和服务进行传递和远程管理的开放式服务平台。该规范和核心部分是一个框架，其中定义了应用程序的生命周期模式和服务注册。基于这个框架定义了大量的OSGi服务：日志、配置管理、偏好，HTTP（运行servlet）、XML分析、设备访问、软件包管理、许可管理、星级、用户管理、IO连接、连线管理、Jini和UPnP。这个框架实现了一个优雅、完整和动态的组件模型。应用程序（称为bundle）无需重新引导可以被远程安装、启动、升级和卸载（其中Java包/类的管理被详细定义）。API中还定义了运行远程下载管理政策的生命周期管理。服务注册允许bundles去检测新服务和取消的服务，然后相应配合。 OSGI框架一般具备的基础功能：支持模块化的动态部署。基于OSGI而构建的系统可以以模块化的方式动态地部署至框架中，从而增加、扩展或改变系统的功能。支持模块化的封装和交互。每个工程（模块）可通过声明Export-Package对外提供访问此工程的类和接口。支持模块的动态扩展。基于OSGI提供的面向服务的组件模型的设计方法，以及OSGI实现框架提供的扩展点方法可实现模块的动态扩展。模块化的设计。在OSGI中模块由一个或多个bundle构成，模块之间的交互通过Import-Package、Export-Package以及OSGI Service的方式实现。动态化的设计。动态化的设计是指系统中所有的模块必须支持动态的插拔和修改，“即插即用，即删即无”。可扩展的设计。通常使用定义扩展点的方式。按照Eclipse推荐的扩展点插件的标准格式定义bundle中的扩展点，其它要扩展的bundle可通过实现相应的扩展点来扩展该bundle的功能。每个bundle拥有独立的class loader，通过它来完成本bundle类的加载。 稳定、高效的系统。基于OSGI的系统采用的是微核机制，微核机制保证了系统的稳定性，微核机制的系统只要微核是稳定运行的，那么系统就不会崩溃，也就是说基于OSGI的系统不会受到运行在其中的bundle的影响，不会因为bundle的崩溃而导致整个系统的崩溃。 [1]OSGi服务平台提供在多种网络设备上无需重启的动态改变构造的功能。为了最小化耦合度和促使这些耦合度可管理，OSGi技术提供一种面向服务的架构，它能使这些组件动态地发现对方。OSGi联盟已经开发了例如像HTTP服务器、配置、日志、安全、用户管理、XML等很多公共功能标准组件接口。这些组件的兼容性插件实现可以从进行了不同优化和使用代价的不同计算机服务提供商得到。然而，服务接口能够基于专有权基础上开发。因为OSGi技术为集成提供了预建立和预测试的组件子系统，所以OSGi技术使你从改善产品上市时间和降低开发成本上获益。因为这些组件能够动态发布到设备上，所以OSGi技术也能降低维护成本和拥有新的配件市场机会。 OSGI规范的核心组件是OSGI框架。这个框架为应用程序（被叫做组件（bundle））提供了一个标准环境。整个框架可以划分为一些层次： OSGIL0：运行环境L1：模块L2：生命周期管理L3：服务注册还有一个无处不在的安全系统渗透到所有层。 L0层执行环境是Java环境的规范。Java2配置和子规范，像J2SE，CDC，CLDC，MIDP等等，都是有效的执行环境。OSGi平台已经标准化了一个执行环境，它是基于基础轮廓和在一个执行环境上确定了最小需求的一个小一些的变种，该执行环境对OSGi组件是有用的。L1模块层定义类的装载策略。OSGi框架是一个强大的具有严格定义的类装载模型。它基于Java之上，但是增加了模块化。在Java中，正常情况下有一个包含所有类和资源的类路径。OSGi模块层为一个模块增加了私有类同时有可控模块间链接。模块层同安全架构完全集成，可以选择部署到部署封闭系统，防御系统，或者由厂商决定的完全由用户管理的系统。L2生命周期层增加了能够被动态安装、开启、关闭、更新和卸载的bundles。这些bundles依赖于于具有类装载功能的模块层，但是增加了在运行时管理这些模块的API。生命周期层引入了正常情况下不属于一个应用程序的动态性。扩展依赖机制用于确保环境的操作正确。生命周期操作在安全架构保护之下，使其不受到病毒的攻击。L3层增加了服务注册。服务注册提供了一个面向bundles的考虑到动态性的协作模型。bundles能通过传统的类共享进行协作，但是类共享同动态安装和卸载代码不兼容。服务注册提供了一个在bundles间分享对象的完整模型。定义了大量的事件来处理服务的注册和删除。这些服务仅仅是能代表任何事物的Java对象。很多服务类似服务器对象，例如HTTP服务器，而另一些服务表示的是一个真实世界的对象，例如附近的一个蓝牙手机。这个服务模块提供了完整安全保障。该服务安全模块使用了一个很聪明的方式来保障bundles之间通信安全。 标准服务在该框架之上，OSGi联盟定义了很多服务。这些服务通过一个Java接口指定。bundles能够实 OSGI现这个接口，并在注册服务层注册该服务。服务的客户端在注册库中找到它，或者当它出现或者消失时做出响应。这个同SOA架构使用Web服务进行发布的方式相似。两者主要不同是Web服务总是需要传输层，这个使它比采用直接方法调用的OSGi服务慢几千倍。同时，OSGi组件能够对这些服务的出现和消失做出响应。更多的信息可以从OSGi服务平台发行版本4手册或者PDF下载中找到。需要注意的是每一种服务都是抽象定义的，与不同计算机服务商的实现相独立。","link":"/2019/07/24/OSGI/"},{"title":"OSGI 开发实例","text":"OSGI(Open Services Gateway Initiative)，或者通俗点说JAVA动态模块系统，定义了一套模块应用开发的框架。OSGI容器实现方案如Knopflerfish, Equinox, and Apache Felix允许你把你的应用分成多个功能模块，这样通过依赖管理这些功能会更加方便。 和Servlet和EJB规范类似，OSGI规范包含两大块：一个OSGI容器需要实现的服务集合；一种OSGI容器和应用之间通信的机制。开发OSGI平台意味着你需要使用OSGI API编写你的应用，然后将其部署到OSGI容器中。从开发者的视角来看，OSGI提供以下优势： 你可以动态地安装、卸载、启动、停止不同的应用模块，而不需要重启容器。 你的应用可以在同一时刻跑多个同一个模块的实例。 OSGI在SOA领域提供成熟的解决方案，包括嵌入式，移动设备和富客户端应用等。OK，你已经有个Servlet容器来做web 应用，有了EJB容器来做事务处理，你可能在想为什么你还需要一个新的容器？简单点说，OSGI容器被设计专门用来开发可分解为功能模块的复杂的Java应用。 企业应用领域的OSGI OSGI规范最初是由OSGI联盟在1999年3月发起。它的主要目的是成为向网络设备传输服务管理的开放规范。核心思想是一旦你向网络设备中添加了一个OSGI服务平台，你可以在网络中的任意位置管理该设备上的服务组件。这些服务组件可以任意安装，更新或移除而不会对设备产生影响。 多年来，OSGI技术只出现在嵌入式系统和网络设备市场。现在，Eclipse使OSGI在企业开发领域焕发出新的光彩。 OSGI受到越来越广泛的支持 2003年，Eclipse开发团队开始寻找一种使eclipse成为一种功能更动态、工具更模块化的富客户端平台。最终，他们的目光锁定在OSGI框架上。Eclipse3.0，2004年6月发布，是基于OSGI技术搭建的首个Eclipse版本。 几乎所有企业应用服务提供商支持或计划支持OSGI。Spring框架同样支持OSGI，通过Spring DM（Spring Dynamic Modules for OSGI Service Platforms）项目，可以让我们在Spring上更方便的应用OSGI。 开源OSGI容器 从企业应用开发者的角度看，OSGI容器侵入性非常小，你可以方便地将其嵌入一个企业应用。举个例子来说，假设你在开发一个复杂的web应用。你希望将这个应用分解成多个功能模块。一个View层模块，一个Model层模块，一个DAO模块。使用嵌入式OSGI容器来跨依赖地管理这些模块可以让你随时更新你的DAO模块却不需要重启你的服务器。 只要你的应用完全符合OSGI规范，它就可以在所有符合OSGI规范的容器内运行。现在，有三种流行的开源OSGI容器： Equinox是OSGI Service Platform Release 4的一个实现。是Eclipse 模块化运行时的核心。 Knopflerfish另一个选择。 Apache Felix是Apache软件基金会赞助的一个OSGI容器在这篇文章里我们使用Equinox作为我们的OSGI容器。 尝试开发一个Hello World bundle 在OSGI的领域，发布的软件是以bundle的形式出现。bundle由java class类和资源文件组成，向设备所有者提供功能，同时可以为其他的bundles提供服务。Eclipse对开发bundles提供了强大的支持。Eclipse不仅仅提供创建bundles的功能，它还集成了Equinox这个OSGI容器，你可以在其上开发和调试OSGI组件。其实所有的Eclipse插件都是使用Eclipse规范代码写的OSGI bundle。接下来，你将可以学到如何使用Eclipse IDE开发一个Hello world osgi bundle。 开始开发bundle 我们一步步的开始： 启动Eclipse，依次点 File --&gt; New --&gt; Project。 选择Plug-in Project，next。 输入Project Name项目名称，比如com.howard.sample.HelloWorld,Target Platform(目标平台)里的an OSGI framework，选择standard。 剩下的保持默认，next。 下个对话框也默认，next。 然后选择Hello OSGI Bundle作为模版。Finish。Eclipse会飞快的为你创建Hello world bundle的模版代码。主要包含两个文件：Activator.java和MANIFEST.MF。 Activator.java的代码如下所示： 12345678910import org.osgi.framework.BundleActivator;import org.osgi.framework.BundleContext;public class Activator implements BundleActivator { public void start(BundleContext context) throws Exception { System.out.println(\"Hello world\"); } public void stop(BundleContext context) throws Exception { System.out.println(\"Goodbye World\"); }} 如果你的bundle在启动和关闭的时候需要被通知，你可以考虑实现BundleActivator接口。以下是定义Activator的一些注意点： 你的Activator类需要一个公有的无参数构造函数。OSGI框架会通过类反射的方式来实例化一个Activator类。 容器启动bundle过程中负责调用你的Activator类的start方法。bundle可以在此初始化资源比如说初始化数据库连接。start方法需要一个参数，BundleContext对象。这个对象允许bundles以取得OSGI容器相关信息的方式和框架交互。如果某一个bundle有异常抛出，容器将对该bundle标记为stopped并不将其纳入service列表。 容器关闭的时候会调用你的Activator类方法stop(),你可以利用这个机会做一些清理的操作。MANIFEST.MF 这个文件是你的bundle的部署描述文件。格式和Jar里的MANIFEST.MF是一样的。包含的不少名值对，就像如下： 123456789Manifest-Version: 1.0Bundle-ManifestVersion: 2Bundle-Name: HelloWorld Plug-inBundle-SymbolicName: com.howard.sample.HelloWorldBundle-Version: 1.0.0Bundle-Activator: com.howard.sample.helloworld.ActivatorBundle-Vendor: HOWARDBundle-RequiredExecutionEnvironment: JavaSE-1.6Import-Package: org.osgi.framework;version=\"1.3.0\" 分别来看下：Bundle-ManifestVersion 数值为2意味着本bundle支持OSGI规范第四版；如果是1那就是支持OSGI规范第三版。 Bundle-Name 给bundle定义一个短名，方便人员阅读 Bundle-SymbolicName 给bundle定义一个唯一的非局部名。方便分辨。 Bundle-Activator 声明在start和stop事件发生时会被通知的监听类的名字。 Import-Package 定义bundle的导入包。 Hello World bundle完成了，接下来我们运行一下。执行bundle 点击Run --&gt; Run Configuration 在左边的OSGI Framework选项里右键 new ，创建一个新的OSGI Run Configuration 名字随便取好了，我们取个OSGi hello world。 你会注意到中间的窗口里Workspace项目里有一子项 com.howard.sample.HelloWorld,将其勾选上，其他的不用管。这时的状态应该如下图。 点击Run按钮。在控制台你应该可以看见点东西了。那是叫做OSGI控制台的东东。与子相伴，还有一个&quot;Hello world&quot;。 OSGI控制台OSGI控制台是一个OSGI容器的命令行界面。你可以利用它做些诸如启动，关闭，安装bundles，更新和删除bundles等操作。现在，点击OSGI控制台所在的位置，回车，你就会发现可以输入命令了。这时的OSGI控制台应该如下图：下面列出一些常用的OSGI命令，你可以试着和OSGI容器交互。 ss 显示已安装的bundles的状态信息，信息包括bundle ID，短名，状态等等。 start 启动一个bundle stop 关闭一个bundle update 载入一个新的JAR文件更新一个bundle install 安装一个新的bundle到容器中 uninstall 卸载一个已在容器中的bundle 依赖管理 OSGI规范允许你把你的应用分解成多个模块然后管理各个模块间的依赖关系。 这需要通过bundle scope来完成。默认情况下，一个bundle内的class对其他bundle来说是不可见的。那么，如果要让一个bundle访问另一个bundle里的class要怎么做？解决的方案就是从源bundle导出包，然后在目标bundle里导入。 接下来我们对此做一个例子。 首先，我们需要先创建一个com.howard.sample.HelloService bundle,我们将通过它导出一个包。 然后，我们在com.howard.sample.HelloWorld 这个bundle里导入包。 导出包 1、创建名为com.howard.sample.HelloService的bundle，创建步骤和前面一样。 2、在这个bundle内，添加一个com.howard.sample.service.HelloService.java 接口，代码如下： 123public interface HelloService { public String sayHello();} 创建一个com.howard.sample.service.impl.HelloServiceImpl.java类实现刚才的接口： 123456 public class HelloServiceImpl implements HelloService{ public String sayHello() { System.out.println(\"Inside HelloServiceImple.sayHello()\"); return \"Say Hello\"; }} 4、打开MANIFEST.MF,选择Runtime标签项，在Exported Packages选项栏，点击Add并且选择com.howard.sample.service这个包。然后MANIFEST.MF的代码应该如下： 1234567891011121314151617181920212223Manifest-Version: 1.0Bundle-ManifestVersion: 2Bundle-Name: HelloService Plug-inBundle-SymbolicName: com.howard.sample.HelloServiceBundle-Version: 1.0.0Bundle-Activator: com.howard.sample.helloservice.ActivatorBundle-Vendor: HOWARDBundle-RequiredExecutionEnvironment: JavaSE-1.6Import-Package: org.osgi.framework;version=\"1.3.0\"Export-Package: com.howard.sample.service``` 你可以看到，MANIFEST.MF文件和刚才的HelloWorld的那份很类似。唯一的区别就是这个多了Export-Package这个标记，对应的值就是我们刚才选择的com.howard.sample.service。Export-Package标记告诉OSGI容器在com.howard.sample.service包内的classes可以被外部访问。注意，我们仅仅暴露了HelloService接口，而不是直接暴露HelloServiceImpl实现。 导入包接下来我们要更新原来的HelloWorld bundle以导入com.howard.sample.service包。步骤如下：1、进入HelloWorld bundle，打开MANIFEST.MF，进入Dependencies标签页，在Imported Packages里添加com.howard.sample.service。MANIFEST.MF文件应该如下所示： Manifest-Version: 1.0Bundle-ManifestVersion: 2Bundle-Name: HelloWorld Plug-inBundle-SymbolicName: com.howard.sample.HelloWorldBundle-Version: 1.0.0Bundle-Activator: com.howard.sample.helloworld.ActivatorBundle-Vendor: HOWARDBundle-RequiredExecutionEnvironment: JavaSE-1.6Import-Package: com.howard.sample.service, org.osgi.framework;version=”1.3.0” 1234567891011121314151617181920212223242526272829303132没错，Import-package标记的值也就是导入的包名之间是用逗号隔开的。在这里导入了两个包om.howard.sample.service和org.osgi.framework。后者是使用Activator类时必须导入的包。 2、接下来，打开HelloWorld项目下的Activator.java文件，这时候你会发现可以使用HelloService这个接口了。但还是不能使用HelloServiceImpl实现类。Eclipse会告诉你：Access restriction（立入禁止）。Class级别可见域为什么OSGI容器可以做到让jar包中的一些classes可见而另一些又不可见呢。答案其实就是OSGI容器自定义了java class loader来有选择的加载类。OSGI容器为每一个bundle都创建了不同的class loader。因此，bundle可以访问的classes包括 Boot classpath：所有的java基础类。 Framework classpath：OSGI框架级别的classloader加载的类 Bundle classpath：Bundle本身引用的关系紧密的JAR的路径 Imported packages：就是在MANIFEST.MF里声明的导入包，一旦声明，在bundle内就可见了。 bundle级别的可见域允许你可以随时放心的更改HelloServiceImpl实现类而不需要去担心依赖关系会被破坏。 OSGI服务 OSGI框架是实现SOA的绝佳土壤。通过它可以实现bundles暴露服务接口给其他bundles消费而不需要让细节暴露。消费bundles甚至可以完全不知道提供服务的bundles。凭着可以良好的隐藏具体实现的能力，OSGI当之无愧是SOA的一种较完美的实现方案。 OSGI中，提供服务的bundle在OSGI容器上将一个POJO注册成一个service。消费者bundle请求OSGI容器中基于某个特殊接口的注册service。一旦找到，消费者bundle就会绑定它，然后就可以调用service中的方法了。举个例子会更容易说明。 导出services 1、确保com.howard.sample.HelloService里的MANIFEST.MF导入org.osgi.framework包2、创建com.howard.sample.service.impl.HelloServiceActivator.java,代码如下： public class HelloServiceActivator implements BundleActivator { ServiceRegistration helloServiceRegistration; @Override public void start(BundleContext context) throws Exception { HelloService helloService = new HelloServiceImpl(); helloServiceRegistration = context.registerService(HelloService.class .getName(), helloService, null); } @Override public void stop(BundleContext context) throws Exception { helloServiceRegistration.unregister(); } }12345678910OK，我们就是用BundleContext的registerService方法注册service的。这个方法需要三个参数。 service的接口名。如果service实现了多个接口，那样你需要传入一个包含所有接口名的String数组。在这里我们传入的是HelloService这个接口。 真正的service实现。在例子中我们传了一个HelloServiceImpl实现。 service属性。这个参数可以在有多个service实现同一个接口的情况下，消费者用来区分真正感兴趣的service。 3、最后一步就是修改HelloService的MANIFEST.MF文件，将Bundle-Activator改成com.howard.sample.service.impl.HelloServiceActivator 现在HelloService bundle已经随时准备将HelloServiceImpl服务发布了。OSGI容器启动HelloServie bundle的时候会让HelloServiceActivator运作，在那个时候将HelloServiceImpl注册到容器中，接下来就是创建消费者的问题了。 导入service 我们的消费者就是HelloWorld bundle，主要修改的就是其中的Activator.java,修改代码如下： public class Activator implements BundleActivator { ServiceReference helloServiceReference; public void start(BundleContext context) throws Exception { System.out.println(&quot;Hello World!!&quot;); helloServiceReference=context.getServiceReference(HelloService.class.getName()); HelloService helloService=(HelloService)context.getService(helloServiceReference); System.out.println(helloService.sayHello()); } public void stop(BundleContext context) throws Exception { System.out.println(&quot;Goodbye World!!&quot;); context.ungetService(helloServiceReference); } }123456789101112代码很简单，就不多说了。在运行之前我们在Run--&gt;Run Configurations对话框里，把HelloWorld和HelloService这两个bundle前面的钩都打上。然后运行时你会发现HelloServiceImpl.sayHello()方法已经被调用了。在OSGI控制台输入ss并回车，所有容器内的bundle状态一目了然。其中id为0的bundle是OSGI框架基础bundle，另两个就是HelloService和HelloWorld了，它俩的id是随机的，状态是ACTIVE也就是已启动状态。假设HelloService的id为7，HelloWorld为8。输入stop 8就可以暂停bundle的运行，容器内这个bundle还是存在的，只是状态变成了RESOLVED。再次启动使用start 8，然后就会看见HelloWorld bundle消费了HelloService的服务。创建服务工厂 刚才例子所示，我们会在HelloService bundle启动时初始化并注册service。然后不管存不存在消费端，这个service都会存在，而且消费端取得的service 实例其实都是同一个。OK，某些servie是比较耗费资源的主，我们不希望它一直占用资源，最好是在真正用它的时候创建不用的时候销毁就最好了。 解决如上问题的方案就是使用ServiceFactory接口的实现来代替原先service具体的实现到OSGI容器去注册。这样，以后只有当其他bundle请求该服务时，才会由ServiceFactory实现类来处理请求并返回一个新的service实例。 实例步骤如下：1、在HelloService bundle创建一个实现ServiceFactory接口的类HelloServiceFactory类，代码如下： public class HelloServiceFactory implements ServiceFactory { private int usageCounter = 0; @Override public Object getService(Bundle bundle, ServiceRegistration registration) { System.out.println(“Create object of HelloService for “ + bundle.getSymbolicName()); usageCounter++; System.out.println(“Number of bundles using service “ + usageCounter); HelloService helloService = new HelloServiceImpl(); return helloService; } @Override public void ungetService(Bundle bundle, ServiceRegistration registration, Object service) { System.out.println(“Release object of HelloService for “ + bundle.getSymbolicName()); usageCounter–; System.out.println(“Number of bundles using service “ + usageCounter); }} 1234ServiceFactory接口定义了两个方法： getService方法：特定的bundle在第一次调用BundleContext的getService方法时由OSGI框架调用，在实例代码中，我们用这个方法来返回一个新的HelloService的实现。OSGI框架会缓存这个返回的对象，如果同一个bundle在未来再次调用BundleContext的getService方法的话，会直接返回这个缓存中的对象。 ungetService方法：bundle释放service的时候由OSGI容器调用。 2、修改HelloServiceActivator.java的start方法，将ServiceFactory作为服务注册，代码如下： public class HelloServiceActivator implements BundleActivator { ServiceRegistration helloServiceRegistration; @Override public void start(BundleContext context) throws Exception { HelloServiceFactory helloServiceFactory = new HelloServiceFactory(); helloServiceRegistration = context.registerService(HelloService.class .getName(), helloServiceFactory, null); } @Override public void stop(BundleContext context) throws Exception { helloServiceRegistration.unregister(); } }1234567现在运行下试试看，你会发现HelloWorld bundle启动时才会初始化HelloService，控制台会打印出\"Number of bundles using service 1\"，当HelloWorld bundle暂停时会打印出\"Number of bundles using service 0\"。 services跟踪 某种情形下，我们可能需要在某个特殊的接口有新的服务注册或取消注册时通知消费端。这时我们可以使用ServiceTracker类。如下步骤所示： 1、在HelloWorld bundle里的MANIFEST.MF导入org.util.tracker包。 2、创建HelloServiceTracker类，代码如下： public class HelloServiceTracker extends ServiceTracker { public HelloServiceTracker(BundleContext context) { super(context, HelloService.class.getName(),null); } public Object addingService(ServiceReference reference) { System.out.println(&quot;Inside HelloServiceTracker.addingService &quot; + reference.getBundle()); return super.addingService(reference); } public void removedService(ServiceReference reference, Object service) { System.out.println(&quot;Inside HelloServiceTracker.removedService &quot; + reference.getBundle()); super.removedService(reference, service); } } 12345678910 我们在HelloServiceTracker的构造函数里将HelloService接口名传进去，ServiceTracker会跟踪实现这个接口的所有的注册services。ServiceTracker主要有两个重要方法： addingService方法：bundle注册一个基于给定接口的service时调用。 removeService方法：bundle取消注册一个基于给定接口的service时调用。 3、修改Activator类，使用刚刚创建的HelloServiceTracker来获取service： public class Activator implements BundleActivator { HelloServiceTracker helloServiceTracker; public void start(BundleContext context) throws Exception { System.out.println(&quot;Hello World!!&quot;); helloServiceTracker= new HelloServiceTracker(context); helloServiceTracker.open(); HelloService helloService=(HelloService)helloServiceTracker.getService(); System.out.println(helloService.sayHello()); } public void stop(BundleContext context) throws Exception { System.out.println(&quot;Goodbye World!!&quot;); helloServiceTracker.close(); } } 现在运行一下，可以发现只要HelloService bundle启动或是暂停都会导致HelloServiceTracker的对addingService或removedService方法的调用。 ServiceTracker不仅仅能跟踪Service的动向，它还能通过getService方法取得Service实例并返回。但是如果同一个接口下有多个service注册，这时返回哪个service呢？这时候就需要看service的等级哪个高了。这个等级是service注册时的property属性里的一项：SERVICE_RANKING。谁的SERVICE_RANKING高，就返回谁。 如果有两个一样高的呢？这时再看这两个service谁的PID更低了。 如果对OSGI的ClassLoader机制有疑问，可以看看这篇解释ClassLoader机制和自定义ClassLoader相关的文章： http://longdick.iteye.com/blog/442213 OSGI的基本原理和入门开发就到这里了。希望同学们能对OSGI开发有个简单而清晰的认识。 参考资料：Hello, OSGi, Part 1: Bundles for beginners","link":"/2019/07/30/OSGI-开发实例/"},{"title":"Mac 关闭开机声音","text":"苹果 Mac系统如何关闭开机声音？1、打开 “应用程序” ，打开“实用工具”，找到并打开[终端]程序，然后输入如下命令并回车（注意，复制的时候不要多复制了空格）： &nbsp;sudo nvram SystemAudioVolume=%80 2、之后会提示输入密码，输入你的用户密码，回车，如下图。这样之后再次启动Mac的时候就是完全静音的了。（注意：输入密码是看不到的，不像普通的输入密码那样能看到，你只管输就可以了，输入完按回车。） 3、如果想要恢复正常的开机声音，只要在终端中再次输入如下命令即可：sudo nvram -d SystemAudioVolume","link":"/2019/07/22/Mac-关闭开机声音/"},{"title":"Oauth2","text":"Oauth2授权模式 授权码模式（Authorization Code）隐士授权模式（Implicit）密码模式（Resource Owner Password Credentials）客户端模式（Client Credentials）授权码授权流程 客户端请求第三方授权用户（资源有者）同意给客户端授权客户端获取到授权码，请求认证服务器申请令牌认证服务器向客户端响应令牌客户端请求资源服务器的资源资源服务器返回受保护资源 申请授权码请求认证服务获取授权码get请求： 客户端ID就是存到数据库里的client_id的值 Spring security 会去数据库中查有没有这个服务 校验令牌token刷新令牌refresh_token JWT (json web token)简化认证流程 实现方式是通过RSA(公钥/私钥)完成签名资源服务器和认证服务器之间通过公钥私钥的方式jwt格式包含三个部分： Header部分1234{ \"\":\"\", \"typ\":\"\"} 他会用base64url进行编码成字符串 Payload部分（内容部分）12{} 也会进行base64url进行编码 signature部分签名部分防止篡改 12345HMACSHA256( base64urlEncode(header) + \".\" + base64UrlEncode(payload), secret) secret:签名使用的密钥整个再进行加密 生成密钥证书，采用rsa算法每个证书包含公钥和私钥生成私钥： 1keytool -genkeypair -alias xxx(密钥的别名) -keyalg RSA（采用的算法） -keypass xxx(密匙的访问密码) -keystore xxx.keystore(密钥库文件即存储密钥的文件) -storepass xxx(密钥库的访问密码) 参数解析：-alias 密钥的别名-alias 密钥的别名-keyalg 使用的hash算法-keypass 密匙的访问密码-keystore 密匙库文件名-storepass 密匙库的访问密码 导出公钥：需要openssl来导出公钥信息安装openssl到证书所在目录执行如下命令： 1keytool -list rfc --keystore xxx.keystore（密钥库文件名）| openssl x509 -inform pem -pubkey springsecurityoauth2流程","link":"/2019/07/21/Oauth2/"},{"title":"OSI","text":"计算机网络OSI 一、OSI参考模型自下而上：物理层（物理介质，比特流）、数据链路层（网卡、交换机）、网络层（IP协议）、传输层（TCP/UDP协议）、会话层（创建/建立/断开连接）、表示层（翻译，编码，压缩，加密）、应用层（HTTP协议）简化为TCP/IP模型：网络层（物理层、数据链路层、网络层）、传输层，会话层，应用层（表示层，应用层）二、相应功能 1.物理层主要设备：中继器、集线器物理层中双绞线的传输距离是有限的，信号会缩减，影响数据的传输。为了使传输的数据能够准确的传输，中继器是可以放大传输信号，保持原数据的准确。双绞线 的传输距离是100m，而超过100m则信号会衰减 pc——–pc中继器 在两台pc中间加一个中继器，则相当于两台pc到中继器的距离均为100m，有助于信号的增强。 pc—–中继器——-pc集线器和中继器的区别是：中继器只有两个以太网接口，而集线器相当于多个端口的中继器。注意：冲突域、广播域 冲突域：当两个比特流在同一介质上同时传输就是产生冲突，冲突域是指发送数据给一个单一目标（单播）所影响的范围广播域：发送数据给一个不明确的目标所影响的范围 集线器有一个冲突域和一个广播域 IP地址192.168.1.1ping1.2： ping 192.168.1.2（ping命令所用的协议有ICMP/ARP协议）返回数据说明两者是相通的，可以发送信息当1.1想向1.2发送数据时，发送报文时，将包发送到集线器，集线器将包广播发送给所有连接在集线器上的其他端口，当1.3，1.4发现该包不是发送给他们的，就将拒绝接收，而1.2发现是发送给它时，就做出应答，返回一个应答包，应答包先发到集线器，集线器又进行广播，然后再发送到1.1上。但是！！数据包向所有的端口发送，不安全，且所有的机器共享带宽，更容易产生拥塞，所以不能用于较大的网络集线器是物理设备，是个智障，只会广播的发发发，不是智能的，所以不具备学习能力，故每次发送数据只能使用广播的方式。 2.数据链路层功能：完成网络之间相邻结点的可靠传输，通过Mac地址负责主机之间的数据的可靠传输。物理层传输的是比特流，而数据链路层传输的是帧。主要设备：网卡、网桥、交换机网卡：网络适配器，连接计算机与网络的硬件设备，整理计算机发往网线的数据，将数据分解成大小的数据包之后向网络上发送Mac地址与IP地址的区别： 最重要的区别：公司内部，学校机房等都是一个局域网，如果想在局域网混，必须要Mac地址！！！，这样交换机才能识别，之后交换机数据传到路由器，想要在外界大的网络中混，必须得要IP地址！！，这样路由器才能识别，进行相应功能！ 这就是为什么我们的电脑有ip地址和mac地址！Mac地址：是厂商烧录在只读存储器上的，出厂厂商的唯一标识，且不可更改 IP地址：网络地址，相当于门牌号查看网卡的Mac地址(十六进制)命令： ipconfig /all Pysical Address :xx-xx-xx-xx-xx-xx 网桥：将两个LAN链接在一起，变成一个LAN,并按Mac地址转发；分割冲突域；例如：如何分割冲突域每个PC机网卡的Mac地址：AA-AA-AA-AA-AA-AA、BB…. 网桥更具Mac地址学习能力，目标Mac地址转发 IP地址192.168.1.1ping1.2： &gt;ping 192.168.1.2（ping命令所用的协议有ICMP/ARP地址解析协议） 过程： 1.1第一次发送ICMP数据包到集线器，集线器发给1.2和网桥，网桥接收到数据包后（工作原理是根据原Mac地址（1.1的Mac地址）学习，目标Mac地址（1.2Mac地址）进行转发，Eth0/1端口学到Mac地址），学到1.1Mac地址，网桥把包传输到下一个集线器，集线器会把包发给1.3,1.4，这两个会扔掉不属于它们的包，1.2接收到后会返回数据给集线器，集线器发送给网桥，这时网桥会学到1.2的Mac地址，由于网桥已经记录了1.1的Mac地址，则会直接发送给1.1而不会又进行广播发给1.3和1.4。 通过第一次发送数据包和接受目标的应答，网桥的每个端口都将学到也就是记录到两个Mac地址，原Mac地址和目标的Mac地址。 交换机：工作过程和网桥类似交换机有三种转发方式： （1）对已知单播帧只往对应端口进行转发 （2）对未知单播帧，即交换机还没有学习到的Mac进行广播转发，所有端口进行广播 （3）对广播帧或组播帧进行广播 IP地址192.168.1.1ping1.2： &gt;ping 192.168.1.2（ping命令所用的协议有ICMP/ARP地址解析协议） 工作过程： 工作过程与网桥一致，交换机的端口通过原Mac地址发送数据学到Mac地址，接收应答的数据包学到目标Mac地址，最终每个端口都学到Mac地址记录在交换机Mac地址表中 可以通过命令查找交换机的Mac地址表： show mac-address-tables 交换机有几个端口就有几个冲突域，且只有一个广播域交换机和网桥的区别： 网桥只有两个端口，交换机至少有四个端口也有8个、16个等端口，网桥基于软件转发，交换机基于硬件转发，可以通过命令查看Mac地址表，而网桥不行。且交换机的造价比网桥低。交换机分类：（1）传统二层交换机（2）VLAN交换机，有网管功能（3）三层交换机：VLAN交换机+路由器，属于网络层的设备，而不是数据链路层的设备 3.网络层完成网络中主机间的报文传输（物理层比特流，数据链路层帧），网络层识别的地址是IP地址。涉及的协议：IP/IPX主要设备：路由器路由器：通信中转站，例如快递的中转站，将不同网络和网段进行数据翻译，使其可以相互理解，构成一个大的网络。 1.2与1.1属于同一个网段，可以接收数据和应答 IP地址192.168.1.1ping1.2： &gt;ping 192.168.1.2 1.1与2.2显示超时，没有返回任何数据，两者不属于同一个网段，不能通信 IP地址192.168.1.1ping2.2： &gt;ping 192.168.2.2如何使两者进行通信？有两种方法： （1）添加一个路由器： 工作原理： 路由器之所以可以翻译不同网段之间的通信是因为其本身具备丰富的协议，可以连接不同的网段和网络。路由器可以判断网络地址以及选择路径的功能。 （2）修改子网掩码，这点后面IP地址再补充 物理层、数据链路层、网络层之间的区别： 4.传输层 是整个网络关键的部分，是实现两个用户进程间端到端的可靠通信，处理数据包的错误等传输问题。是向下通信服务最高层，向上用户功能最底层。即向网络层提供服务，向会话层提供独立于网络层的传送服务和可靠的透明数据传输。主要协议：TCP传输控制协议/UDP用户数据报协议，涉及服务使用的端口号，主机根据端口号识别服务，区分会话。TCP协议：解决数据是否完整传输，是否正确UDP协议：UDP协议实现了端口，从而使数据包传送到IP地址的基础上，还可以进一步将其送到具体的某一个端口上。UDP传输与IP传输相似，但IP协议是ip地址之间的通信，但通信需要多个通信通道，将每个通道分配给每一个进程使用，UDP则是实现端口的通信。服务与以及相对应的端口文件：C:\\Windows\\System32\\drivers\\etc\\servies例如：www服务端口号为805.会话层： 主要功能是在两个结点间建立、维护和释放面向用户的连接，并对会话进行管理和控制，保证会话数据可靠传送。例如：你通过秘书与对方建立联系，则你发出建立联系的请求相当于一个会话，秘书相当于传输层，然后秘书进行拨号联系对方，当对方接通对话，则会话的连接建立。6.表示层： 主要负责数据格式的转换，即翻译，压缩与解压缩，加密与解密。例如：你想下午两点出发去上海，你对上海的朋友说下午两点过来，朋友的理解却是你下午两点到上海，两个人两种理解，而表示层则是进行格式转换和信息的表示。7.应用层： 应用层是网络体系中最高的一层，也是唯一面向用户的一层，也可视为为用户提供常用的应用程序，例如电子邮件，上网浏览等网络服务都是应用层程序。对于这三层的定义还是不能理解，上网搜了以上比较通俗的解释。应用层的主要协议：HTTP,HTTPS,FTP（上传，下载）,SMTP（邮件）HTTP协议：超文本传输协议，包括url地址，域名，源地址，发送的数据方法（Get，Post等），浏览器信息等OSI模型的总结OSI模型上层（会话层，表示层，应用层）处理用户接口、数据格式、应用访问。OSI模型下层（物理层，数据链路层，网络层，传输层）处理数据在网络介质中的传送。三、数据封装过程封装： 应用层：发送数据———表示层：数据格式转换，加密，压缩等———-会话层：建立连接———–传输层：差错校验，流量控制，TCP/UDP传输，添加端口号信息（源端口，目标端口）+数据————网络层：分组，数据包（IP地址+数据）——–数据链路层：帧（帧头（帧头包含Mac地址）+帧数据）————物理层：比特流（0,1）解封装： J 物理层：比特流（0,1）———数据链路层：帧（帧头（帧头包含Mac地址）+帧数据）——–网络层：分组，数据包（IP地址+数据）——-传输层：差错校验，流量控制等；UDP/TCP(传送和接收端口信息+数据)———会话层：建立连接———–表示层：数据格式转换，解密，解压缩等———–应用层：接收的数据","link":"/2019/07/18/OSI/"},{"title":"ParseYaml","text":"yaml解析 1、使用spring自带的yaml解析工具： 12345678910111213141516171819202122232425public class YamlUtils { private static final Logger logger = LogManager.getLogger(YamlUtils.class); public static Map&lt;String, Object&gt; yaml2Map(String yamlSource) { try { YamlMapFactoryBean yaml = new YamlMapFactoryBean(); yaml.setResources(new ClassPathResource(yamlSource)); return yaml.getObject(); } catch (Exception e) { logger.error(\"Cannot read yaml\", e); return null; } } public static Properties yaml2Properties(String yamlSource) { try { YamlPropertiesFactoryBean yaml = new YamlPropertiesFactoryBean(); yaml.setResources(new ClassPathResource(yamlSource)); return yaml.getObject(); } catch (Exception e) { logger.error(\"Cannot read yaml\", e); return null; } }} spring提供的yaml工具只能够解析出map和properties，如果想解析生成java bean就有点力不从心，不方便。2、使用snakeyaml 在maven中引入 12345&lt;dependency&gt; &lt;groupId&gt;org.yaml&lt;/groupId&gt; &lt;artifactId&gt;snakeyaml&lt;/artifactId&gt; &lt;version&gt;1.17&lt;/version&gt; &lt;/dependency&gt; Yaml yaml = new Yaml();LibraryPolicyDto libraryPolicyDto = yaml.loadAs(inputStream, LibraryPolicyDto.class); 附yaml文件： 1234systemLibraryId: 1 relativeLibraryId: xxx readerPolicyIndex: 1 bookPolicyIndex: 1 otherPublicApi: 12345678910111213name: showApi desc: 查看api url: 127.0.0.1:8080/api/showApi port: 8080 inputParams: - id:int - libraryId:String outputParams: - name:String - desc:String - url:String - inputParams:String - ouputParams:String 附LibraryPolicyDto 123456789public class LibraryPolicyDto { private int systemLibraryId; private String relativeLibraryId; private int readerPolicyIndex; private int bookPolicyIndex; private List&lt;ApiDto&gt; otherPublicApi; //getter //setter}","link":"/2019/07/12/ParseYaml/"},{"title":"@SessionAttributes","text":"@SessionAttributes原理默认情况下Spring MVC将模型中的数据存储到request域中。当一个请求结束后，数据就失效了。如果要跨页面使用。那么需要使用到session。而@SessionAttributes注解就可以使得模型中的数据存储一份到session域中。 @SessionAttributes参数 1、names：这是一个字符串数组。里面应写需要存储到session中数据的名称。 2、types：根据指定参数的类型，将模型中对应类型的参数存储到session中 3、value：其实和names是一样的。 123456@RequestMapping(\"/test\") public String test(Map&lt;String,Object&gt; map){ map.put(\"names\", Arrays.asList(\"caoyc\",\"zhh\",\"cjx\")); map.put(\"age\", 18); return \"hello\";} 123451、request中names:${requestScope.names}&lt;br/&gt;2、request中age:${requestScope.age}&lt;br/&gt;&lt;hr/&gt;3、session中names:${sessionScope.names }&lt;br/&gt;4、session中age:${sessionScope.age }&lt;br/&gt; 【总结】:上面代码没有指定@SessionAttributes，所有在session域总无法获取到对应的数据。 下面我们加上@SessionAttributes注解 1234567891011@SessionAttributes(value={\"names\"},types={Integer.class}) @Controller public class Test { @RequestMapping(\"/test\") public String test(Map&lt;String,Object&gt; map){ map.put(\"names\", Arrays.asList(\"caoyc\",\"zhh\",\"cjx\")); map.put(\"age\", 18); return \"hello\"; }} 再次访问页面： 可以看到session域中值已存在 【注意】：@SessionAttributes注解只能在类上使用，不能在方法上使用","link":"/2019/09/24/SessionAttributes/"},{"title":"@PostConstruct","text":"简介Java EE5 引入了@PostConstruct和@PreDestroy这两个作用于Servlet生命周期的注解，实现Bean初始化之前和销毁之前的自定义操作。此文主要说明@PostConstruct。 API使用说明 PostConstruct 注释用于在依赖关系注入完成之后需要执行的方法上，以执行任何初始化。此方法必须在将类放入服务之前调用。支持依赖关系注入的所有类都必须支持此注释。即使类没有请求注入任何资源，用 PostConstruct 注释的方法也必须被调用。只有一个方法可以用此注释进行注释。应用 PostConstruct 注释的方法必须遵守以下所有标准：该方法不得有任何参数，除非是在 EJB 拦截器 (interceptor) 的情况下，根据 EJB 规范的定义，在这种情况下它将带有一个 InvocationContext 对象 ；该方法的返回类型必须为 void；该方法不得抛出已检查异常；应用 PostConstruct 的方法可以是 public、protected、package private 或 private；除了应用程序客户端之外，该方法不能是 static；该方法可以是 final；如果该方法抛出未检查异常，那么不得将类放入服务中，除非是能够处理异常并可从中恢复的 EJB。 ** 总结为一下几点： ** 只有一个方法可以使用此注释进行注解；被注解方法不得有任何参数；被注解方法返回值为void；被注解方法不得抛出已检查异常；被注解方法需是非静态方法；此方法只会被执行一次； Servlet执行流程图两个注解加入只会，Servlet执行流程图： 在具体Bean的实例化过程中，@PostConstruct注释的方法，会在构造方法之后，init方法之前进行调用。 实例基于Spring boot编写的可执行方法见github：https://github.com/HappySecondBrother/exampleUserService方法（提供缓存数据）： 12345678910111213141516171819202122package com.secbro.service;import org.springframework.stereotype.Service;import java.util.ArrayList;import java.util.List;/** * @author 二师兄 * @date 2016/8/10 */@Servicepublic class UserService { public List&lt;String&gt; getUser(){ List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(\"张三\"); list.add(\"李四\"); return list; }} BusinessService方法，通过@PostConstruct调用UserService： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.secbro.service;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import javax.annotation.PostConstruct;import java.util.List;/** * @author 二师兄 * @date 2016/8/10 */@Servicepublic class BusinessService { @Autowired private UserService userService; private List&lt;String&gt; list = null; /** * 构造方法执行之后，调用此方法 */ @PostConstruct public void init(){ System.out.println(\"@PostConstruct方法被调用\"); // 实例化类之前缓存获得用户信息 List&lt;String&gt; list = userService.getUser(); this.list = list; if(list != null &amp;&amp; !list.isEmpty()){ for(String user : list){ System.out.println(\"用户：\" + user); } } } public BusinessService(){ System.out.println(\"构造方法被调用\"); } public List&lt;String&gt; getList() { return list; } public void setList(List&lt;String&gt; list) { this.list = list; }} 执行结果： 1234构造方法被调用@PostConstruct方法被调用用户：张三用户：李四 项目应用** 在项目中@PostConstruct主要应用场景是在初始化Servlet时加载一些缓存数据等。** 注意事项使用此注解时会影响到服务的启动时间。服务器在启动时会扫描WEB-INF/classes的所有文件和WEB-INF/lib下的所有jar包。","link":"/2019/09/05/PostConstruct/"},{"title":"RestTemplate","text":"SpringBoot26 RestTemplate、WebClientSpringboot — 用更优雅的方式发HTTP请求(RestTemplate详解)RestTemplate是Spring提供的用于访问Rest服务的客户端，RestTemplate提供了多种便捷访问远程Http服务的方法,能够大大提高客户端的编写效率。 我之前的HTTP开发是用apache的HttpClient开发，代码复杂，还得操心资源回收等。代码很复杂，冗余代码多，稍微截个图，这是我封装好的一个post请求工具： 本教程将带领大家实现Spring生态内RestTemplate的Get请求和Post请求还有exchange指定请求类型的实践和RestTemplate核心方法源码的分析，看完你就会用优雅的方式来发HTTP请求。 1.简述RestTemplate 是Spring用于同步client端的核心类，简化了与http服务的通信，并满足RestFul原则，程序代码可以给它提供URL，并提取结果。默认情况下，RestTemplate默认依赖jdk的HTTP连接工具。当然你也可以 通过setRequestFactory属性切换到不同的HTTP源，比如Apache HttpComponents、Netty和OkHttp。 RestTemplate能大幅简化了提交表单数据的难度，并且附带了自动转换JSON数据的功能，但只有理解了HttpEntity的组成结构（header与body），且理解了与uriVariables之间的差异，才能真正掌握其用法。这一点在Post请求更加突出，下面会介绍到。 该类的入口主要是根据HTTP的六个方法制定： 此外，exchange和excute可以通用上述方法。 在内部，RestTemplate默认使用HttpMessageConverter实例将HTTP消息转换成POJO或者从POJO转换成HTTP消息。默认情况下会注册主mime类型的转换器，但也可以通过setMessageConverters注册其他的转换器。 其实这点在使用的时候是察觉不到的，很多方法有一个responseType 参数，它让你传入一个响应体所映射成的对象，然后底层用HttpMessageConverter将其做映射 12HttpMessageConverterExtractor&lt;T&gt; responseExtractor = new HttpMessageConverterExtractor&lt;&gt;(responseType, getMessageConverters(), logger); HttpMessageConverter.java源码： 12345678910111213141516171819public interface HttpMessageConverter&lt;T&gt; { //指示此转换器是否可以读取给定的类。 boolean canRead(Class&lt;?&gt; clazz, @Nullable MediaType mediaType); //指示此转换器是否可以写给定的类。 boolean canWrite(Class&lt;?&gt; clazz, @Nullable MediaType mediaType); //返回List&lt;MediaType&gt; List&lt;MediaType&gt; getSupportedMediaTypes(); //读取一个inputMessage T read(Class&lt;? extends T&gt; clazz, HttpInputMessage inputMessage) throws IOException, HttpMessageNotReadableException; //往output message写一个Object void write(T t, @Nullable MediaType contentType, HttpOutputMessage outputMessage) throws IOException, HttpMessageNotWritableException; } 在内部，RestTemplate默认使用SimpleClientHttpRequestFactory和DefaultResponseErrorHandler来分别处理HTTP的创建和错误，但也可以通过setRequestFactory和setErrorHandler来覆盖。 2.get请求实践 2.1.getForObject()方法 1234567public &lt;T&gt; T getForObject(String url, Class&lt;T&gt; responseType, Object... uriVariables){}public &lt;T&gt; T getForObject(String url, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables)public &lt;T&gt; T getForObject(URI url, Class&lt;T&gt; responseType)``` getForObject()其实比getForEntity()多包含了将HTTP转成POJO的功能，但是getForObject没有处理response的能力。因为它拿到手的就是成型的pojo。省略了很多response的信息。2.1.1 POJO: public class Notice { private int status; private Object msg; private List data;} public class DataBean { private int noticeId; private String noticeTitle; private Object noticeImg; private long noticeCreateTime; private long noticeUpdateTime; private String noticeContent;} 12示例：2.1.2 不带参的get请求 /** * 不带参的get请求 */ @Test public void restTemplateGetTest(){ RestTemplate restTemplate = new RestTemplate(); Notice notice = restTemplate.getForObject(“http://xxx.top/notice/list/1/5&quot; , Notice.class); System.out.println(notice); } 1控制台打印： INFO 19076 — [ main] c.w.s.c.w.c.HelloControllerTestStarted HelloControllerTest in 5.532 seconds (JVM running for 7.233)Notice{status=200, msg=null, data=[DataBean{noticeId=21, noticeTitle=’aaa’, noticeImg=null,noticeCreateTime=1525292723000, noticeUpdateTime=1525292723000, noticeContent=’aaa‘},DataBean{noticeId=20, noticeTitle=’ahaha’, noticeImg=null, noticeCreateTime=1525291492000,noticeUpdateTime=1525291492000, noticeContent=’ah…….’ 1示例：2.1.3 带参数的get请求1 Notice notice = restTemplate.getForObject(“http://fantj.top/notice/list/{1}/{2}&quot; , Notice.class,1,5); 123明眼人一眼能看出是用了占位符{1}。示例：2.1.4 带参数的get请求2 Map&lt;String,String&gt; map = new HashMap(); map.put(“start”,”1”); map.put(“page”,”5”); Notice notice = restTemplate.getForObject(“http://fantj.top/notice/list/&quot; , Notice.class,map); 123明眼人一看就是利用map装载参数，不过它默认解析的是PathVariable的url形式。2.2 getForEntity()方法 public ResponseEntity getForEntity(String url, Class responseType, Object… uriVariables){}public ResponseEntity getForEntity(String url, Class responseType, Map&lt;String, ?&gt; uriVariables){}public ResponseEntity getForEntity(URI url, Class responseType){} 12345678910111213141516171819202122232425262728293031与getForObject()方法不同的是返回的是ResponseEntity对象，如果需要转换成pojo，还需要json工具类的引入，这个按个人喜好用。不会解析json的可以百度FastJson或者Jackson等工具类。然后我们就研究一下ResponseEntity下面有啥方法。ResponseEntity、HttpStatus、BodyBuilder结构ResponseEntity.javapublic HttpStatus getStatusCode(){}public int getStatusCodeValue(){}public boolean equals(@Nullable Object other) {}public String toString() {}public static BodyBuilder status(HttpStatus status) {}public static BodyBuilder ok() {}public static &lt;T&gt; ResponseEntity&lt;T&gt; ok(T body) {}public static BodyBuilder created(URI location) {}... HttpStatus.javapublic enum HttpStatus {public boolean is1xxInformational() {}public boolean is2xxSuccessful() {}public boolean is3xxRedirection() {}public boolean is4xxClientError() {}public boolean is5xxServerError() {}public boolean isError() {}} BodyBuilder.java public interface BodyBuilder extends HeadersBuilder { //设置正文的长度，以字节为单位，由Content-Length标头 BodyBuilder contentLength(long contentLength); //设置body的MediaType 类型 BodyBuilder contentType(MediaType contentType); //设置响应实体的主体并返回它。 ResponseEntity body(@Nullable T body);｝ 123可以看出来，ResponseEntity包含了HttpStatus和BodyBuilder的这些信息，这更方便我们处理response原生的东西。示例： @Testpublic void rtGetEntity(){ RestTemplate restTemplate = new RestTemplate(); ResponseEntity entity = restTemplate.getForEntity(“http://fantj.top/notice/list/1/5&quot; , Notice.class); HttpStatus statusCode = entity.getStatusCode(); System.out.println(&quot;statusCode.is2xxSuccessful()&quot;+statusCode.is2xxSuccessful()); Notice body = entity.getBody(); System.out.println(&quot;entity.getBody()&quot;+body); ResponseEntity.BodyBuilder status = ResponseEntity.status(statusCode); status.contentLength(100); status.body(&quot;我在这里添加一句话&quot;); ResponseEntity&lt;Class&lt;Notice&gt;&gt; body1 = status.body(Notice.class); Class&lt;Notice&gt; body2 = body1.getBody(); System.out.println(&quot;body1.toString()&quot;+body1.toString()); } 1234567891011statusCode.is2xxSuccessful()trueentity.getBody()Notice{status=200, msg=null, data=[DataBean{noticeId=21, noticeTitle='aaa', ...body1.toString()&lt;200 OK,class com.waylau.spring.cloud.weather.pojo.Notice,{Content-Length=[100]}&gt; 当然，还有getHeaders()等方法没有举例。3. post请求实践同样的,post请求也有postForObject和postForEntity。 public T postForObject(String url, @Nullable Object request, Class responseType, Object… uriVariables) throws RestClientException {}public T postForObject(String url, @Nullable Object request, Class responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException {}public T postForObject(URI url, @Nullable Object request, Class responseType) throws RestClientException {} 123示例我用一个验证邮箱的接口来测试。 @Testpublic void rtPostObject(){ RestTemplate restTemplate = new RestTemplate(); String url = “http://47.xxx.xxx.96/register/checkEmail&quot;; HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_FORM_URLENCODED); MultiValueMap&lt;String, String&gt; map= new LinkedMultiValueMap&lt;&gt;(); map.add(“email”, “844072586@qq.com“); HttpEntity&lt;MultiValueMap&lt;String, String&gt;&gt; request = new HttpEntity&lt;&gt;(map, headers); ResponseEntity&lt;String&gt; response = restTemplate.postForEntity( url, request , String.class ); System.out.println(response.getBody());} 12345678910111213141516执行结果：{\"status\":500,\"msg\":\"该邮箱已被注册\",\"data\":null}Springboot -- 用更优雅的方式发HTTP请求(RestTemplate详解)代码中，MultiValueMap是Map的一个子类，它的一个key可以存储多个value，简单的看下这个接口：public interface MultiValueMap&lt;K, V&gt; extends Map&lt;K, List&lt;V&gt;&gt; {...} 为什么用MultiValueMap?因为HttpEntity接受的request类型是它。public HttpEntity(@Nullable T body, @Nullable MultiValueMap&lt;String, String&gt; headers){}//我这里只展示它的一个construct,从它可以看到我们传入的map是请求体，headers是请求头。 为什么用HttpEntity是因为restTemplate.postForEntity方法虽然表面上接收的request是@Nullable Object request类型，但是你追踪下去会发现，这个request是用HttpEntity来解析。核心代码如下： if (requestBody instanceof HttpEntity) { this.requestEntity = (HttpEntity&lt;?&gt;) requestBody;}else if (requestBody != null) { this.requestEntity = new HttpEntity&lt;&gt;(requestBody);}else { this.requestEntity = HttpEntity.EMPTY;} 12345678910我曾尝试用map来传递参数，编译不会报错，但是执行不了，是无效的url request请求(400 ERROR)。其实这样的请求方式已经满足post请求了，cookie也是属于header的一部分。可以按需求设置请求头和请求体。其它方法与之类似。4.使用exchange指定调用方式exchange()方法跟上面的getForObject()、getForEntity()、postForObject()、postForEntity()等方法不同之处在于它可以指定请求的HTTP类型。但是你会发现exchange的方法中似乎都有@Nullable HttpEntity requestEntity这个参数，这就意味着我们至少要用HttpEntity来传递这个请求体，之前说过源码所以建议就使用HttpEntity提高性能。示例 @Test public void rtExchangeTest() throws JSONException { RestTemplate restTemplate = new RestTemplate(); String url = “http://xxx.top/notice/list&quot;; HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_FORM_URLENCODED); JSONObject jsonObj = new JSONObject(); jsonObj.put(“start”,1); jsonObj.put(“page”,5); HttpEntity&lt;String&gt; entity = new HttpEntity&lt;&gt;(jsonObj.toString(), headers); ResponseEntity&lt;JSONObject&gt; exchange = restTemplate.exchange(url, HttpMethod.GET, entity, JSONObject.class); System.out.println(exchange.getBody()); } 1234567891011121314151617181920这次可以看到，我使用了JSONObject对象传入和返回。当然，HttpMethod方法还有很多，用法类似。5.excute()指定调用方式excute()的用法与exchange()大同小异了，它同样可以指定不同的HttpMethod，不同的是它返回的对象是响应体所映射成的对象，而不是ResponseEntity。需要强调的是，execute()方法是以上所有方法的底层调用。随便看一个：``` @Override @Nullable public &lt;T&gt; T postForObject(String url, @Nullable Object request, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException { RequestCallback requestCallback = httpEntityCallback(request, responseType); HttpMessageConverterExtractor&lt;T&gt; responseExtractor = new HttpMessageConverterExtractor&lt;&gt;(responseType, getMessageConverters(), logger); return execute(url, HttpMethod.POST, requestCallback, responseExtractor, uriVariables); } 1 RestTemplate RestTemplate是在客户端访问 Restful 服务的一个核心类；RestTemplate通过提供回调方法和允许配置信息转换器来实现个性化定制RestTemplate的功能，通过RestTemplate可以封装请求对象，也可以对响应对象进行解析。 技巧01：RestTemplate默认使用JDK提供的包去建立HTTP连接，当然，开发者也可以使用诸如 Apache HttpComponents, Netty, and OkHttp 去建立HTTP连接。 技巧02：RestTemplate内部默认使用HttpMessageConverter来实现HTTTP messages 和 POJO 之间的转换，可以通过RestTemplate的成员方 法 setMessageConverters(java.util.List&lt;org.springframework.http.converter.HttpMessageConverter&lt;?&gt;&gt;). 去修改默认的转换器。 技巧03：RestTemplate内部默认使用SimpleClientHttpRequestFactory and DefaultResponseErrorHandler 去创建HTTP连接和处理HTTP错误，可以通过HttpAccessor.setRequestFactory(org.springframework.http.client.ClientHttpRequestFactory) and setErrorHandler(org.springframework.web.client.ResponseErrorHandler)去做相应的修改。 ##### 1.1 RestTemplate中方法概览 RestTemplate为每种HTTP请求都实现了相关的请求封装方法技巧01：这些方法的命名是有讲究的，方法名的第一部分表示HTTP请求类型，方法名的第二部分表示响应类型 例如：getForObject 表示执行GET请求并将响应转化成一个Object类型的对象 技巧02：利用RestTemplate封装客户端发送HTTP请求时，如果出现异常就会抛出 RestClientException 类型的异常；可以通过在创建RestTemplate对象的时候指定一个ResponseErrorHandler类型的异常处理类来处理这个异常 技巧03：exchange 和 excute 这两个方法是通用的HTTP请求方法，而且这两个方法还支持额外的HTTP请求类型【PS: 前提是使用的HTTP连接包也支持这些额外的HTTP请求类型】 技巧04：每种方法都有3个重载方法，其中两个接收String类型的请求路径和响应类型、参数；另外一个接收URI类型的请求路径和响应类型。 技巧05：使用String类型的请求路径时，RestTemplate会自动进行一次编码，所以为了避免重复编码问题最好使用URI类型的请求路径例如：restTemplate.getForObject(“http://example.com/hotel list”) becomes”http://example.com/hotel%20list&quot;技巧06：URI 和URL 知识点扫盲 技巧06：利用接收URI参数的RestTemplate.getForObject方法发送Get请求 ##### 1.2 常用构造器 技巧01：利用无参构造器创建RestTemplate实例时，什么都是使用默认的【即：使用HttpMessageConverter来实现HTTTP messages 和 POJO 之间的转换、使用SimpleClientHttpRequestFactory and DefaultResponseErrorHandler 去创建HTTP连接和处理HTTP错误】 技巧02：利用 RestTemplate(ClientHttpRequestFactory requestFactory) 创建RestTemplate实例时使用自定义的requestFactory去创建HTTP连接 技巧03：利用 RestTemplate(java.util.List&lt;HttpMessageConverter&lt;?&gt;&gt; messageConverters) 创建RestTemplate实例时使用自定义的转换器列表实现HTTTP messages 和 POJO 之间的转换 1.3 GET相关方法 技巧01：本博文使用的是SpringBoot项目，利用了一个配置文件来将RestTemplate注入的容器中 #######1.3.1 public T getForObject(String url, Class responseType, Object… uriVariables)1》远程服务代码【不带请求参数的】 1》模拟客户端代码【不带请求参数的】 2》远程服务代码【带请求参数的】 技巧01：HTTP请求中url路径？后面的参数就是请求参数格式以 key=value 的形式传递；后台需要用@RequestParam注解，如果前端的 key 和 后台方法的参数名称一致时可以不用@RequestParam注解【因为@RequestParam注解时默认的参数注解】 技巧02：对于请求参数，最好在服务端利用@RequestParam注解设置该请求参数为非必传参数并设定默认值 2》模拟客户端代码【带请求参数的】 3》远程服务代码【带路径参数的】 技巧01：HTTP请求的路径可以成为路径参数，前提是服务端进行路径配置【即：需要配合@RequestMapping和@PathVariable一起使用】 技巧02：由于路径参数不能设置默认是，所以在后台通过@PathVariable将路径参数设置成必传可以减少出错率 技巧03：@PathVariable可以设置正则表达式【详情参见：https://www.cnblogs.com/NeverCtrl-C/p/8185576.html】 3》模拟客户端代码【带路径参数的】 4》远程服务代码【带路径参数和请求参数的】 技巧01： @PathVariable和@RequestParam都可以设定是否必传【默认必传】 技巧02：@PathVariable不可以设定默认值，@RequestParam可以设定默认值【默认值就是不传入的时候代替的值】 技巧03： @PathVariable如果设置必传为true，前端不传入时就会报错【技巧：开启必传】 技巧04：@RequestParam如果设置必传为true，前端不传入还是也会报错【技巧：关闭必传，开启默认值】 技巧05：@PathVariable可以设置正则表达式【详情参见：https://www.cnblogs.com/NeverCtrl-C/p/8185576.html】 4》模拟客户端代码【带路径参数和请求参数的】 ####### 1.3.2 public T getForObject(String url, Class responseType, Map&lt;String, ?&gt; uriVariables) 1》远程服务代码【带请求参数的】 1》模拟客户端代码【带请求参数的】 ####### 1.3.3 public ResponseEntity getForEntity(String url, Class responseType, Object… uriVariables)技巧01：getForObject 和 getForEntity 的区别：后者可以获取到更多的响应信息，前者这可以获取到响应体的数据1》远程服务代码【不带请求参数的】 1》模拟客户端代码【不带请求参数的】 2》远程服务代码【带请求参数的】 2》模拟客户端代码【带请求参数的】 3》远程服务代码【带路径参数的】3》模拟客户端代码【带路径参数的】4》远程服务代码【带路径参数和请求参数的】4》模拟客户端代码【带路径参数和请求参数的】 ####### 1.3.4 public ResponseEntity getForEntity(String url, Class responseType, Map&lt;String, ?&gt; uriVariables)1》远程服务代码【带请求参数的】 1》模拟客户端代码【带请求参数的】 ##### 1.4 POST服务端源代码：点击前往 ####### 1.4.1 public T postForObject(String url, @Nullable Object request, Class responseType, Object… uriVariables) 参数解释： url -&gt; String类型的请求路径 request -&gt; 请求体对象 responseType -&gt; 响应数据类型 uriVariables -&gt; 请求参数 ####### 1.4.2 public T postForObject(String url, @Nullable Object request, Class responseType, Map&lt;String, ?&gt; uriVariables) 参数解释： url -&gt; String类型的请求路径 request -&gt; 请求体对象 responseType -&gt; 响应数据类型 uriVariables -&gt; 请求参数 ####### 1.4.3 public T postForObject(URI url, @Nullable Object request, Class responseType) 参数解释： url -&gt; URI类型的请求路径 request -&gt; 请求体对象 responseType -&gt; 响应数据类型 ####### 1.4.4 请求体对象说明 技巧01：请求体对象（@Nullable Object request）可以直接传一个实体，服务端利用@RequestBody接收这个实体即可 技巧02：请求体对象（@Nullable Object request）也可以传入一个 HttpEntity 的实例，服务端的代码不变；创建 HttpEntity 实例时可以设定请求体数据和请求头数据（详情请参见 HttpEntity 的相关构造函数） 1.5 其他请求和GET、POST类似 待更新…… 2 WebClient WebClient 是一个非阻塞、响应式的HTTP客户端，它以响应式被压流的方式执行HTTP请求；WebClient默认使用 Reactor Netty 作为HTTP连接器，当然也可以通过 ClientHttpConnector修改其它的HTTP连接器。 技巧01：使用WebClient需要进入Spring5的相关依赖，如果使用的是SpringBoot项目的话直接引入下面的依赖就可以啦 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;&lt;/dependency&gt; WebClient源码 ##### 2.1 创建WebClient实例的方式 技巧01：从WebClient的源码中可以看出，WebClient接口提供了三个静态方法来创建WebClient实例 @Bean注入 123456789@Bean public RestTemplate restTemplate(){ RestTemplate restTemplate = new RestTemplate();; restTemplate.setRequestFactory(new HttpComponentsClientHttpRequestFactory()); MappingJackson2HttpMessageConverter converter = new MappingJackson2HttpMessageConverter(); converter.setSupportedMediaTypes(Collections.singletonList(MediaType.ALL)); restTemplate.getMessageConverters().add(converter); return restTemplate; } 使用： 12345 MultiValueMap&lt;String, Object&gt; headers = new LinkedMultiValueMap&lt;String, Object&gt;();headers.add(\"Accept\", \"application/json\");headers.add(\"Content-Type\", \"application/json\");HttpEntity request = new HttpEntity(requestBody, headers);Object sqlResult = restTemplate.postForObject(logConf.getAccessAddr() + \"/_sql\", request, Object.class);","link":"/2019/07/18/RestTemplate/"},{"title":"RabbitMq入门以及使用教程","text":"RabbitMq入门以及使用教程 [TOCM] [TOC] 一、简介MQ全称为Message Queue, 消息队列（MQ）是一种应用程序对应用程序的通信方法。应用程序通过读写出入队列的消息（针对应用程序的数据）来通信，而无需专用连接来链接它们。消息传递指的是程序之间通过在消息中发送数据进行通信，而不是通过直接调用彼此来通信，直接调用通常是用于诸如远程过程调用的技术。排队指的是应用程序通过 队列来通信。队列的使用除去了接收和发送应用程序同时执行的要求。其中较为成熟的MQ产品有IBM WEBSPHERE MQ等等… 二、使用场景在项目中，将一些无需即时返回且耗时的操作提取出来，进行了异步处理，而这种异步处理的方式大大的节省了服务器的请求响应时间，从而提高了系统的吞吐量。 三、相关名称介绍1、ConnectionFactory、Connection、ChannelConnectionFactory、Connection、Channel都是RabbitMQ对外提供的API中最基本的对象。 Connection是RabbitMQ的socket链接，它封装了socket协议相关部分逻辑。 ConnectionFactory为Connection的制造工厂。 Channel是我们与RabbitMQ打交道的最重要的一个接口，我们大部分的业务操作是在Channel这个接口中完成的，包括定义Queue、定义Exchange、绑定Queue与Exchange、发布消息等。 2、QueueQueue（队列）是RabbitMQ的内部对象，用于存储消息，用下图表示。 RabbitMQ中的消息都只能存储在Queue中，生产者（下图中的P）生产消息并最终投递到Queue中，消费者（下图中的C）可以从Queue中获取消息并消费。 多个消费者可以订阅同一个Queue，这时Queue中的消息会被平均分摊给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理。 3、Message acknowledgment在实际应用中，可能会发生消费者收到Queue中的消息，但没有处理完成就宕机（或出现其他意外）的情况，这种情况下就可能会导致消息丢失。为了避免这种情况发生，我们可以要求消费者在消费完消息后发送一个回执给RabbitMQ，RabbitMQ收到消息回执（Message acknowledgment）后才将该消息从Queue中移除；如果RabbitMQ没有收到回执并检测到消费者的RabbitMQ连接断开，则RabbitMQ会将该消息发送给其他消费者（如果存在多个消费者）进行处理。这里不存在timeout概念，一个消费者处理消息时间再长也不会导致该消息被发送给其他消费者，除非它的RabbitMQ连接断开。 这里会产生另外一个问题，如果我们的开发人员在处理完业务逻辑后，忘记发送回执给RabbitMQ，这将会导致严重的bug——Queue中堆积的消息会越来越多；消费者重启后会重复消费这些消息并重复执行业务逻辑… 另外pub message是没有ack的。 4、Message durability如果我们希望即使在RabbitMQ服务重启的情况下，也不会丢失消息，我们可以将Queue与Message都设置为可持久化的（durable），这样可以保证绝大部分情况下我们的RabbitMQ消息不会丢失。但依然解决不了小概率丢失事件的发生（比如RabbitMQ服务器已经接收到生产者的消息，但还没来得及持久化该消息时RabbitMQ服务器就断电了），如果我们需要对这种小概率事件也要管理起来，那么我们要用到事务。由于这里仅为RabbitMQ的简单介绍，所以这里将不讲解RabbitMQ相关的事务。 5、Prefetch count前面我们讲到如果有多个消费者同时订阅同一个Queue中的消息，Queue中的消息会被平摊给多个消费者。这时如果每个消息的处理时间不同，就有可能会导致某些消费者一直在忙，而另外一些消费者很快就处理完手头工作并一直空闲的情况。我们可以通过设置prefetchCount来限制Queue每次发送给每个消费者的消息数，比如我们设置prefetchCount=1，则Queue每次给每个消费者发送一条消息；消费者处理完这条消息后Queue会再给该消费者发送一条消息。 6、Exchange在上一节我们看到生产者将消息投递到Queue中，实际上这在RabbitMQ中这种事情永远都不会发生。实际的情况是，生产者将消息发送到Exchange（交换器，下图中的X），由Exchange将消息路由到一个或多个Queue中（或者丢弃）。 Exchange是按照什么逻辑将消息路由到Queue的？这个将在下面的8、Binding中介绍。 RabbitMQ中的Exchange有四种类型，不同的类型有着不同的路由策略，这将在下面的10、Exchange Types中介绍。 7、routing key生产者在将消息发送给Exchange的时候，一般会指定一个routing key，来指定这个消息的路由规则，而这个routing key需要与Exchange Type及binding key联合使用才能最终生效。 在Exchange Type与binding key固定的情况下（在正常使用时一般这些内容都是固定配置好的），我们的生产者就可以在发送消息给Exchange时，通过指定routing key来决定消息流向哪里。RabbitMQ为routing key设定的长度限制为255 bytes。 8、BindingRabbitMQ中通过Binding将Exchange与Queue关联起来，这样RabbitMQ就知道如何正确地将消息路由到指定的Queue了。 9、Binding key在绑定（Binding）Exchange与Queue的同时，一般会指定一个binding key；消费者将消息发送给Exchange时，一般会指定一个routing key；当binding key与routing key相匹配时，消息将会被路由到对应的Queue中。这个将在Exchange Types章节会列举实际的例子加以说明。 在绑定多个Queue到同一个Exchange的时候，这些Binding允许使用相同的binding key。binding key 并不是在所有情况下都生效，它依赖于Exchange Type，比如fanout类型的Exchange就会无视binding key，而是将消息路由到所有绑定到该Exchange的Queue。 ###10、Exchange TypesRabbitMQ常用的Exchange Type有fanout、direct、topic、headers这四种（AMQP规范里还提到两种Exchange Type，分别为system与自定义，这里不予以描述），下面分别进行介绍。 fanoutfanout类型的Exchange路由规则非常简单，它会把所有发送到该Exchange的消息路由到所有与它绑定的Queue中。 上图中，生产者（P）发送到Exchange（X）的所有消息都会路由到图中的两个Queue，并最终被两个消费者（C1与C2）消费。 directdirect类型的Exchange路由规则也很简单，它会把消息路由到那些binding key与routing key完全匹配的Queue中。 以上图的配置为例，我们以routingKey=”error”发送消息到Exchange，则消息会路由到Queue1（amqp.gen-S9b…，这是由RabbitMQ自动生成的Queue名称）和Queue2（amqp.gen-Agl…）；如果我们以routingKey=”info”或routingKey=”warning”来发送消息，则消息只会路由到Queue2。如果我们以其他routingKey发送消息，则消息不会路由到这两个Queue中。 topic前面讲到direct类型的Exchange路由规则是完全匹配binding key与routing key，但这种严格的匹配方式在很多情况下不能满足实际业务需求。topic类型的Exchange在匹配规则上进行了扩展，它与direct类型的Exchage相似，也是将消息路由到binding key与routing key相匹配的Queue中，但这里的匹配规则有些不同，它约定： routing key为一个句点号“. ”分隔的字符串（我们将被句点号“. ”分隔开的每一段独立的字符串称为一个单词），如“stock.usd.nyse”、“nyse.vmw”、“quick.orange.rabbit” binding key与routing key一样也是句点号“. ”分隔的字符串。 binding key中可以存在两种特殊字符“*”与“#”，用于做模糊匹配，其中“*”用于匹配一个单词，“#”用于匹配多个单词（可以是零个）。 以上图中的配置为例，routingKey=”quick.orange.rabbit”的消息会同时路由到Q1与Q2，routingKey=”lazy.orange.fox”的消息会路由到Q1与Q2，routingKey=”lazy.brown.fox”的消息会路由到Q2，routingKey=”lazy.pink.rabbit”的消息会路由到Q2（只会投递给Q2一次，虽然这个routingKey与Q2的两个bindingKey都匹配）；routingKey=”quick.brown.fox”、routingKey=”orange”、routingKey=”quick.orange.male.rabbit”的消息将会被丢弃，因为它们没有匹配任何bindingKey。 headersheaders类型的Exchange不依赖于routing key与binding key的匹配规则来路由消息，而是根据发送的消息内容中的headers属性进行匹配。 在绑定Queue与Exchange时指定一组键值对；当消息发送到Exchange时，RabbitMQ会取到该消息的headers（也是一个键值对的形式），对比其中的键值对是否完全匹配Queue与Exchange绑定时指定的键值对；如果完全匹配则消息会路由到该Queue，否则不会路由到该Queue。 该类型的Exchange没有用到过（不过也应该很有用武之地），所以不做介绍。 11、RPCMQ本身是基于异步的消息处理，前面的示例中所有的生产者（P）将消息发送到RabbitMQ后不会知道消费者（C）处理成功或者失败（甚至连有没有消费者来处理这条消息都不知道）。 但实际的应用场景中，我们很可能需要一些同步处理，需要同步等待服务端将我的消息处理完成后再进行下一步处理。这相当于RPC（Remote Procedure Call，远程过程调用）。在RabbitMQ中也支持RPC。 RabbitMQ中实现RPC的机制是： 客户端发送请求（消息）时，在消息的属性（MessageProperties，在AMQP协议中定义了14中properties，这些属性会随着消息一起发送）中设置两个值replyTo（一个Queue名称，用于告诉服务器处理完成后将通知我的消息发送到这个Queue中）和correlationId（此次请求的标识号，服务器处理完成后需要将此属性返还，客户端将根据这个id了解哪条请求被成功执行了或执行失败）； 服务器端收到消息并处理； 服务器端处理完消息后，将生成一条应答消息到replyTo指定的Queue，同时带上correlationId属性； 客户端之前已订阅replyTo指定的Queue，从中收到服务器的应答消息后，根据其中的correlationId属性分析哪条请求被执行了，根据执行结果进行后续业务处理。 四、总结本文介绍了RabbitMQ中个人认为最重要的概念，充分利用RabbitMQ提供的这些功能就可以处理我们绝大部分的异步业务了。","link":"/2019/12/06/RabbitMq入门/"},{"title":"@SessionAttribute与@SessionAttributes","text":"@SessionAttribute与@SessionAttributes @SessionAttributes@SessionAttributes作用于处理器类上，用于在多个请求之间传递参数，类似于Session的Attribute，但不完全一样，一般来说@SessionAttributes设置的参数只用于暂时的传递，而不是长期的保存，长期保存的数据还是要放到Session中。通过@SessionAttributes注解设置的参数有3类用法：（1）在视图中通过request.getAttribute或session.getAttribute获取（2）在后面请求返回的视图中通过session.getAttribute或者从model中获取（3）自动将参数设置到后面请求所对应处理器的Model类型参数或者有 @ModelAttribute注释的参数里面。将一个参数设置到SessionAttributes中需要满足两个条件：（1）在@SessionAttributes注解中设置了参数的名字或者类型（2）在处理器中将参数设置到了model中。@SessionAttributes用户后可以调用SessionStatus.setComplete来清除，这个方法只是清除SessionAttribute里的参数，而不会应用Session中的参数。示例如下：注解@SessionAttribute中设置book、description和 types={Double}，这样值会被放到@SessionAttributes中，但Redirect跳转时就可以重新获得这些数据了，接下来操作sessionStatus.setComplete()，则会清除掉所有的数据，这样再次跳转时就无法获取数据了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@Controller @RequestMapping(\"/book\") @SessionAttributes(value ={\"book\",\"description\"},types={Double.class}) public class RedirectController { @RequestMapping(\"/index\") public String index(Model model){ model.addAttribute(\"book\", \"金刚经\"); model.addAttribute(\"description\",\"不擦擦擦擦擦擦擦车\"); model.addAttribute(\"price\", new Double(\"1000.00\")); //跳转之前将数据保存到book、description和price中，因为注解@SessionAttribute中有这几个参数 return \"redirect:get.action\"; } @RequestMapping(\"/get\") public String get(@ModelAttribute (\"book\") String book,ModelMap model, SessionStatus sessionStatus){ //可以获得book、description和price的参数 System.out.println(model.get(\"book\")+\";\"+model.get(\"description\")+\";\"+model.get(\"price\")); sessionStatus.setComplete(); return \"redirect:complete.action\"; } @RequestMapping(\"/complete\") public String complete(ModelMap modelMap){ //已经被清除，无法获取book的值 System.out.println(modelMap.get(\"book\")); modelMap.addAttribute(\"book\", \"妹纸\"); return \"sessionAttribute\"; } } @SessionAttribute① 使用@SessionAttribute来访问预先存在的全局会话属性 如果你需要访问预先存在的、以全局方式管理的会话属性的话，比如在控制器之外（比如通过过滤器）可能或不可能存在在一个方法参数上使用注解 12345678910111213141516171819202122232425262728293031323334353637383940414243 @SessionAttribute： /** * 在处理请求 /helloWorld/jump 的时候，会在会话中添加一个 sessionStr 属性。 * &lt;p/&gt; * 这里可以通过@SessionAttribute 获取到 */ @RequestMapping(\"/sesAttr\") public String handleSessionAttr(@SessionAttribute(value = \"sessionStr\") String sessionStr, Model model) { System.out.println(\"--&gt; sessionStr : \" + sessionStr); model.addAttribute(\"sth\", sessionStr); return \"/examples/targets/test1\"; } 为了使用这些需要添加或移除会话属性的情况，考虑注入org.springframework.web.context.request.WebRequest或javax.servlet.http.HttpSession到一个控制器方法中。 对于暂存在会话中的用作控制器工作流一部分的模型属性，要像“使用 @SessionAttributes 存储模型属性到请求共享的HTTP会话”一节中描述的那样使用SessionAttributes。@RequestAttribute② 使用@RequestAttribute访问请求属性就像@SessionAttribute一样，注解@RequestAttribute可以被用于访问由过滤器或拦截器创建的、预先存在的请求属性： 12345678910111213@RequestMapping(\"/reqAttr\") public String handle(@RequestAttribute(\"reqStr\") String str, Model model) { System.out.println(\"--&gt; reqStr : \" + str); model.addAttribute(\"sth\", str); return \"/examples/targets/test1\"; } 可以使用下面的过滤器进行测试： 1234567891011121314151617181920212223242526272829@WebFilter(filterName = \"myFilter\", description = \"测试过滤器\", urlPatterns = { \"/*\" })public class MyFilter implements Filter{ @Override public void init(FilterConfig filterConfig) throws ServletException {} @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\"--&gt; MyFilter Do.\"); request.setAttribute(\"reqStr\", \"万万没想到，啦啦啦啦啦！\"); chain.doFilter(request, response); } @Override public void destroy() {} }","link":"/2019/07/17/SessionAttribute/"},{"title":"Spring Cloud Stream与RabbitMQ集成","text":"[TOCM] [TOC] 简述Spring Cloud Stream是一个建立在Spring Boot和Spring Integration之上的框架，有助于创建事件驱动或消息驱动的微服务。在本文中，我们将通过一些简单的例子来介绍Spring Cloud Stream的概念和构造。 步骤1 Maven依赖在开始之前，我们需要添加Spring Cloud Stream与RabbitMQ消息中间件的依赖。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;&lt;/dependency&gt; 同时为支持Junit单元测试，在pom.xml文件中添加 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-test-support&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 2 主要概念微服务架构遵循“智能端点和哑管道”的原则。端点之间的通信由消息中间件（如RabbitMQ或Apache Kafka）驱动。服务通过这些端点或信道发布事件来进行通信。 让我们通过下面这个构建消息驱动服务的基本范例，来看看Spring Cloud Stream框架的一些主要概念。 2.1 服务类通过Spring Cloud Stream建立一个简单的应用，从Input通道监听消息然后返回应答到Output通道。 12345678910111213@SpringBootApplication@EnableBinding(Processor.class)public class MyLoggerServiceApplication { public static void main(String[] args) { SpringApplication.run(MyLoggerServiceApplication.class, args); } @StreamListener(Processor.INPUT) @SendTo(Processor.OUTPUT) public LogMessage enrichLogMessage(LogMessage log) { return new LogMessage(String.format(\"[1]: %s\", log.getMessage())); }} 注解@EnableBinding声明了这个应用程序绑定了2个通道：INPUT和OUTPUT。这2个通道是在接口Processor中定义的（Spring Cloud Stream默认设置）。所有通道都是配置在一个具体的消息中间件或绑定器中。 让我们来看下这些概念的定义： Bindings — 声明输入和输出通道的接口集合。Binder — 消息中间件的实现，如Kafka或RabbitMQChannel — 表示消息中间件和应用程序之间的通信管道StreamListeners — bean中的消息处理方法，在中间件的MessageConverter特定事件中进行对象序列化/反序列化之后，将在信道上的消息上自动调用消息处理方法。Message Schemas — 用于消息的序列化和反序列化，这些模式可以静态读取或者动态加载，支持对象类型的演变。将消息发布到指定目的地是由发布订阅消息模式传递。发布者将消息分类为主题，每个主题由名称标识。订阅方对一个或多个主题表示兴趣。中间件过滤消息，将感兴趣的主题传递给订阅服务器。订阅方可以分组，消费者组是由组ID标识的一组订户或消费者，其中从主题或主题的分区中的消息以负载均衡的方式递送。 2.2 测试类测试类是一个绑定器的实现，允许与通道交互和检查消息。让我们向上面的enrichLogMessage 服务发送一条消息，并检查响应中是否包含文本“[ 1 ]：”： 12345678910111213141516171819@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = MyLoggerServiceApplication.class)@DirtiesContextpublic class MyLoggerApplicationIntegrationTest { @Autowired private Processor pipe; @Autowired private MessageCollector messageCollector; @Test public void whenSendMessage_thenResponseShouldUpdateText() { pipe.input().send(MessageBuilder.withPayload(new LogMessage(\"This is my message\")).build()); Object payload = messageCollector.forChannel(pipe.output()).poll().getPayload(); assertEquals(\"[1]: This is my message\", payload.toString()); }} 2.3 RabbitMQ配置我们需要在工程src/main/resources目录下的application.yml文件里增加RabbitMQ绑定器的配置。 12345678910111213141516171819202122spring: cloud: stream: bindings: input: destination: queue.log.messages binder: local_rabbit group: logMessageConsumers output: destination: queue.pretty.log.messages binder: local_rabbit binders: local_rabbit: type: rabbit environment: spring: rabbitmq: host: localhost port: 5672 username: guest password: guest virtual-host: / input绑定使用名为queue.log.messages的消息交换机，output绑定使用名为queue.pretty.log.messages的消息交换机。所有的绑定都使用名为local_rabbit的绑定器。请注意，我们不需要预先创建RabbitmQ交换机或队列。运行应用程序时，两个交换机都会自动创建。 3 自定义通道在上面的例子里，我们使用Spring Cloud提供的Processor接口，这个接口有一个input通道和一个output通道。 如果我们想创建一些不同，比如说一个input通道和两个output通道，可以新建一个自定义处理器。 123456789101112public interface MyProcessor { String INPUT = \"myInput\"; @Input SubscribableChannel myInput(); @Output(\"myOutput\") MessageChannel anOutput(); @Output MessageChannel anotherOutput();} 3.1 服务类Spring将为我们提供这个接口的实现。通道的名称可以通过使用注解来设定，比如@Output(“myOutput”)。如果没有设置的话，Spring将使用方法名来作为通道名称。因此这里有三个通道：myInput， myOutput， anotherOutput。 现在我们可以增加一些路由规则，如果接收到的值小于10则走一个output通道；如果接收到的值大于等于10则走另一个output通道。 123456789101112131415@Autowiredprivate MyProcessor processor;@StreamListener(MyProcessor.INPUT)public void routeValues(Integer val) { if (val &lt; 10) { processor.anOutput().send(message(val)); } else { processor.anotherOutput().send(message(val)); }}private static final &lt;T&gt; Message&lt;T&gt; message(T val) { return MessageBuilder.withPayload(val).build();} 3.2 测试类发送不同的消息，判断返回值是否是通过不同的通道获得。 12345678910111213141516171819202122232425262728293031@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = MultipleOutputsServiceApplication.class)@DirtiesContextpublic class MultipleOutputsServiceApplicationIntegrationTest { @Autowired private MyProcessor pipe; @Autowired private MessageCollector messageCollector; @Test public void whenSendMessage_thenResponseIsInAOutput() { whenSendMessage(1); thenPayloadInChannelIs(pipe.anOutput(), 1); } @Test public void whenSendMessage_thenResponseIsInAnotherOutput() { whenSendMessage(11); thenPayloadInChannelIs(pipe.anotherOutput(), 11); } private void whenSendMessage(Integer val) { pipe.myInput().send(MessageBuilder.withPayload(val).build()); } private void thenPayloadInChannelIs(MessageChannel channel, Integer expectedValue) { Object payload = messageCollector.forChannel(channel).poll().getPayload(); assertEquals(expectedValue, payload); }} 4 根据条件分派使用@StreamListener 注释，我们还可以使用自定义的SpEL表达式来过滤用户期望的消息。下面这个例子，我们使用条件调度将消息路由到不同的输出。 12345678910111213141516@Autowiredprivate MyProcessor processor; @StreamListener( target = MyProcessor.INPUT, condition = \"payload &lt; 10\")public void routeValuesToAnOutput(Integer val) { processor.anOutput().send(message(val));} @StreamListener( target = MyProcessor.INPUT, condition = \"payload &gt;= 10\")public void routeValuesToAnotherOutput(Integer val) { processor.anotherOutput().send(message(val));} 5 总结在本教程中，我们介绍了Spring Cloud Stream的主要概念，并展示了如何通过RabbitMQ上的一些简单示例来使用它。工程代码可参看","link":"/2019/12/10/Spring Cloud Stream与RabbitMQ集成/"},{"title":"Spring Boot集成FastDFS","text":"https://blog.csdn.net/qq_756589808/article/details/82882589 1、建立一个springboot的工程，下面的是我的目录结构： 2、pom文件引入依赖： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;commons-lang3.version&gt;3.3.2&lt;/commons-lang3.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- 支持 @ConfigurationProperties 注解 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;!-- 1.5的版本默认采用的连接池技术是jedis 2.0以上版本默认连接池是lettuce, 在这里采用jedis，所以需要排除lettuce的jar --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;io.lettuce&lt;/groupId&gt; &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-redis&lt;/artifactId&gt; &lt;version&gt;1.4.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 添加jedis客户端 --&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/ojdbc/ojdbc --&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle&lt;/groupId&gt; &lt;artifactId&gt;ojdbc6&lt;/artifactId&gt; &lt;version&gt;11.2.0.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- http://repo1.maven.org/maven2/com/github/pagehelper/pagehelper/--&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;5.1.4&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Apache工具组件 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;${commons-lang3.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--spring2.0集成redis所需common-pool2--&gt; &lt;!-- 必须加上，jedis依赖此 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- alibaba的druid数据库连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.9&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 集成fastjson--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.47&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 集成Swagger2形成API文档请求接口--&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 文件服务器--&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.tobato&lt;/groupId&gt; &lt;artifactId&gt;fastdfs-client&lt;/artifactId&gt; &lt;version&gt;1.26.1-RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-codec&lt;/groupId&gt; &lt;artifactId&gt;commons-codec&lt;/artifactId&gt; &lt;version&gt;1.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.3.13.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.zaxxer&lt;/groupId&gt; &lt;artifactId&gt;HikariCP&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-quartz&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-beanutils&lt;/groupId&gt; &lt;artifactId&gt;commons-beanutils-core&lt;/artifactId&gt; &lt;version&gt;1.8.3&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 这个是方便在yml中自定义参数同过@Value的形式注入 这个依赖确保是1.26.1以上的不然SpringBoot2.0集成会报错 按照截图创建包以及建立类： 3、编写FdfsConfig 类 123456789101112131415161718192021222324252627282930313233343536package com.saliai.logsystem.common.conf; import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component; /** * @author: Martin * @Date: 2018/10/12 * @Description: * @Modify By: */@Componentpublic class FdfsConfig { @Value(\"${fdfs.res-host}\") private String resHost; @Value(\"${fdfs.storage-port}\") private String storagePort; public String getResHost() { return resHost; } public void setResHost(String resHost) { this.resHost = resHost; } public String getStoragePort() { return storagePort; } public void setStoragePort(String storagePort) { this.storagePort = storagePort; } } 4、编写 FdfsConfiguration 类 1234567891011121314151617181920package com.saliai.logsystem.common.conf; /** * @author: Martin * @Date: 2018/10/12 * @Description: * @Modify By: */ import com.github.tobato.fastdfs.FdfsClientConfig;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.EnableMBeanExport;import org.springframework.context.annotation.Import;import org.springframework.jmx.support.RegistrationPolicy; @Configuration@Import(FdfsClientConfig.class)@EnableMBeanExport(registration = RegistrationPolicy.IGNORE_EXISTING)public class FdfsConfiguration {} 5、建立constants包并建立GlobalConstants类 1234567891011package com.saliai.logsystem.common.constants; /** * @author: Martin * @Date: 2018/10/25 * @Description: * @Modify By: */public class GlobalConstants { public final static String HTTP_FILEURL = \"http://baihoomall.100healths.cn\";} 6、创建截图中的FastDFSClientWrapper类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107package com.saliai.logsystem.common.utils; /** * @author: Martin * @Date: 2018/9/28 * @Description: * @Modify By: */ import com.github.tobato.fastdfs.domain.StorePath;import com.github.tobato.fastdfs.exception.FdfsUnsupportStorePathException;import com.github.tobato.fastdfs.proto.storage.DownloadByteArray;import com.github.tobato.fastdfs.service.FastFileStorageClient;import com.saliai.logsystem.common.conf.FdfsConfig;import com.saliai.logsystem.common.constants.GlobalConstants;import lombok.extern.slf4j.Slf4j;import org.apache.commons.io.FilenameUtils;import org.apache.commons.lang3.StringUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import org.springframework.web.multipart.MultipartFile; import java.io.ByteArrayInputStream;import java.io.IOException;import java.io.InputStream; /** * 功能描述: 文件处理类 * * @author Martin * @version V1.0 * @date 2018/10/12 */@Component@Slf4jpublic class FastDFSClientWrapper { @Autowired private FastFileStorageClient storageClient; @Autowired private FdfsConfig fdfsConfig; public String uploadFile(MultipartFile file) throws IOException { StorePath storePath = storageClient.uploadFile((InputStream) file.getInputStream(), file.getSize(), FilenameUtils.getExtension(file.getOriginalFilename()), null); log.info(\"storePath:\" + storePath); return getResAccessUrl(storePath); } /** * 封装文件完整URL地址 * * @param storePath * @return */ private String getResAccessUrl(StorePath storePath) { //GlobalConstants.HTTP_PRODOCOL + String fileUrl = GlobalConstants.HTTP_FILEURL + \":\" + fdfsConfig.getStoragePort() + \"/\" + storePath.getFullPath(); log.info(\"fileUrl:\" + fileUrl); return fileUrl; } /** * 功能描述: 删除文件 * * @param fileUrl * @return void * @author Martin * @date 2018/10/12 * @version V1.0 */ public void deleteFile(String fileUrl) { log.info(\"删除的文件的url:\" + fileUrl); if (StringUtils.isEmpty(fileUrl)) { return; } try { StorePath storePath = StorePath.praseFromUrl(fileUrl); log.info(\"groupName:\"+storePath.getGroup()+\"------\"+\"文件路径path：\"+storePath.getPath()); storageClient.deleteFile(storePath.getGroup(), storePath.getPath()); } catch (FdfsUnsupportStorePathException e) { log.warn(e.getMessage()); } } /** * 功能描述: 下载文件 * * @param fileUrl * @return java.io.InputStream * @author Martin * @date 2018/10/12 * @version V1.0 */ public InputStream downFile(String fileUrl) { try { StorePath storePath = StorePath.praseFromUrl(fileUrl); byte[] fileByte = storageClient.downloadFile(storePath.getGroup(), storePath.getPath(), new DownloadByteArray()); InputStream ins = new ByteArrayInputStream(fileByte); return ins; } catch (Exception e) { log.error(\"Non IO Exception: Get File from Fast DFS failed\", e); } return null; }} 7、接下来编写controller进行测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.saliai.logsystem.controller; import com.saliai.logsystem.common.utils.FastDFSClientWrapper;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.*;import org.springframework.web.multipart.MultipartFile;import org.springframework.web.servlet.mvc.support.RedirectAttributes; import java.io.IOException; /** * @author: Martin * @Date: 2018/9/28 * @Description: * @Modify By: */@Controller@RequestMapping(\"/upload\")public class UploadController { @Autowired private FastDFSClientWrapper dfsClient; @GetMapping(\"/\") public String index() { return \"upload/upload\"; } @PostMapping(\"/fdfs_upload\") public String fdfsUpload(@RequestParam(\"file\") MultipartFile file, RedirectAttributes redirectAttributes) { if (file.isEmpty()) { redirectAttributes.addFlashAttribute(\"message\", \"Please select a file to upload\"); return \"redirect:/upload/uploadStatus\"; } try { String fileUrl = dfsClient.uploadFile(file); redirectAttributes.addFlashAttribute(\"message\", \"You successfully uploaded '\" + fileUrl + \"'\"); } catch (IOException e) { e.printStackTrace(); } return \"redirect:/upload/uploadStatus\"; } @GetMapping(\"/uploadStatus\") public String uploadStatus() { return \"upload/uploadStatus\"; } @RequestMapping(\"/deleteFile\") @ResponseBody public String deleteFile(@RequestParam(value = \"fileUrl\") String fileUrl) { dfsClient.deleteFile(fileUrl); return \"Success\"; }} 8、建立页面测试上传功能。这里使用freemarker upload.ftl: 123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;body&gt; &lt;h1&gt;Spring Boot file upload example&lt;/h1&gt; &lt;form method=\"POST\" action=\"/upload/fdfs_upload\" enctype=\"multipart/form-data\"&gt; &lt;input type=\"file\" name=\"file\" /&gt;&lt;br/&gt;&lt;br/&gt; &lt;input type=\"submit\" value=\"Submit\" /&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; uploadStatus.ftl: 123456789101112131415161718192021&lt;!DOCTYPE html&gt; &lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt; &lt;body&gt; &lt;h1&gt;Spring Boot - Upload Status&lt;/h1&gt; &lt;div&gt; &lt;h1&gt;${message}&lt;/h1&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 在启动类中加上@EnableAutoConfiguration注解 在配置文件中加入这两个截图的东西： 9、启动项目，进行测试 提交，进行文件上传： 返回文件的地址，直接拷贝到浏览器访问，看看是否能访问。","link":"/2019/09/27/Spring-Boot集成FastDFS/"},{"title":"Springboot整合Logback日志","text":"（一）添加依赖 Springboot2.0自动整合了logback和log4j2，所以无需引入相关依赖。 （二）创建logback配置文件 首先，官方推荐使用的xml名字的格式为：logback-spring.xml而不是logback.xml，至于为什么，因为带spring后缀的可以使用这个标签（PS：这个标签用于切换“开发环境”和“生产环境”）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!-- 日志级别从低到高分为TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL，如果设置为WARN，则低于WARN的信息都不会输出 --&gt;&lt;!-- scan:当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true --&gt;&lt;!-- scanPeriod:设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。 --&gt;&lt;!-- debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 --&gt;&lt;configuration scan=\"true\" scanPeriod=\"10 seconds\"&gt; &lt;contextName&gt;logback&lt;/contextName&gt; &lt;!-- name的值是变量的名称，value的值时变量定义的值。通过定义的值会被插入到logger上下文中。定义变量后，可以使“${}”来使用变量。 --&gt; &lt;!-- 日志文件的输出路径 --&gt; &lt;property name=\"log.path\" value=\"E:/log\" /&gt; &lt;!-- 彩色日志 --&gt; &lt;!-- 彩色日志依赖的渲染类 --&gt; &lt;conversionRule conversionWord=\"clr\" converterClass=\"org.springframework.boot.logging.logback.ColorConverter\" /&gt; &lt;conversionRule conversionWord=\"wex\" converterClass=\"org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter\" /&gt; &lt;conversionRule conversionWord=\"wEx\" converterClass=\"org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter\" /&gt; &lt;!-- 彩色日志格式 --&gt; &lt;property name=\"CONSOLE_LOG_PATTERN\" value=\"${CONSOLE_LOG_PATTERN:-%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}\"/&gt; &lt;!--输出到控制台--&gt; &lt;appender name=\"CONSOLE\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;!--此日志appender是为开发使用，只配置最底级别，控制台输出的日志级别是大于或等于此级别的日志信息--&gt; &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt; &lt;level&gt;debug&lt;/level&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;!--彩色日志--&gt; &lt;!--&lt;Pattern&gt;${CONSOLE_LOG_PATTERN}&lt;/Pattern&gt;--&gt; &lt;!--普通日志--&gt; &lt;Pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/Pattern&gt; &lt;!-- 设置字符集 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--输出到文件--&gt; &lt;!-- 时间滚动输出 level为 DEBUG 日志 --&gt; &lt;appender name=\"DEBUG_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;file&gt;${log.path}/log_debug.log&lt;/file&gt; &lt;!--日志文件输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 设置字符集 --&gt; &lt;/encoder&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;!-- 日志归档 --&gt; &lt;fileNamePattern&gt;${log.path}/debug/log-debug-%d{yyyy-MM-dd}.%i.log&lt;/fileNamePattern&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\"&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;!--日志文件保留天数--&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 此日志文件只记录debug级别的 --&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;level&gt;debug&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 时间滚动输出 level为 INFO 日志 --&gt; &lt;appender name=\"INFO_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;file&gt;${log.path}/log_info.log&lt;/file&gt; &lt;!--日志文件输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;!-- 每天日志归档路径以及格式 --&gt; &lt;fileNamePattern&gt;${log.path}/info/log-info-%d{yyyy-MM-dd}.%i.log&lt;/fileNamePattern&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\"&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;!--日志文件保留天数--&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 此日志文件只记录info级别的 --&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;level&gt;info&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 时间滚动输出 level为 WARN 日志 --&gt; &lt;appender name=\"WARN_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;file&gt;${log.path}/log_warn.log&lt;/file&gt; &lt;!--日志文件输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 此处设置字符集 --&gt; &lt;/encoder&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;${log.path}/warn/log-warn-%d{yyyy-MM-dd}.%i.log&lt;/fileNamePattern&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\"&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;!--日志文件保留天数--&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 此日志文件只记录warn级别的 --&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;level&gt;warn&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 时间滚动输出 level为 ERROR 日志 --&gt; &lt;appender name=\"ERROR_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;file&gt;${log.path}/log_error.log&lt;/file&gt; &lt;!--日志文件输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 此处设置字符集 --&gt; &lt;/encoder&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;${log.path}/error/log-error-%d{yyyy-MM-dd}.%i.log&lt;/fileNamePattern&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\"&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;!--日志文件保留天数--&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 此日志文件只记录ERROR级别的 --&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- &lt;logger&gt;用来设置某一个包或者具体的某一个类的日志打印级别、 以及指定&lt;appender&gt;。&lt;logger&gt;仅有一个name属性， 一个可选的level和一个可选的addtivity属性。 name:用来指定受此logger约束的某一个包或者具体的某一个类。 level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF， 还有一个特俗值INHERITED或者同义词NULL，代表强制执行上级的级别。 如果未设置此属性，那么当前logger将会继承上级的级别。 addtivity:是否向上级logger传递打印信息。默认是true。 --&gt; &lt;!--&lt;logger name=\"org.pc.web\" level=\"info\"/&gt;--&gt; &lt;!--&lt;logger name=\"org.pc.scheduling.annotation.ScheduledAnnotationBeanPostProcessor\" level=\"INFO\"/&gt;--&gt; &lt;!-- 使用mybatis的时候，sql语句是debug下才会打印，而这里我们只配置了info，所以想要查看sql语句的话，有以下两种操作： 第一种把&lt;root level=\"info\"&gt;改成&lt;root level=\"DEBUG\"&gt;这样就会打印sql，不过这样日志那边会出现很多其他消息 第二种就是单独给dao下目录配置debug模式，代码如下，这样配置sql语句会打印，其他还是正常info级别： --&gt; &lt;!--&lt;logger name=\"org.pc.dao\" level=\"info\"/&gt;--&gt; &lt;!--开发环境:打印控制台 start--&gt; &lt;springProfile name=\"dev\"&gt; &lt;logger name=\"org.pc\" level=\"debug\"/&gt; &lt;/springProfile&gt; &lt;!-- root节点是必选节点，用来指定最基础的日志输出级别，只有一个level属性 level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF， 不能设置为INHERITED或者同义词NULL。默认是DEBUG 可以包含零个或多个元素，标识这个appender将会添加到这个logger。 --&gt; &lt;root level=\"info\"&gt; &lt;appender-ref ref=\"CONSOLE\" /&gt; &lt;appender-ref ref=\"DEBUG_FILE\" /&gt; &lt;appender-ref ref=\"INFO_FILE\" /&gt; &lt;appender-ref ref=\"WARN_FILE\" /&gt; &lt;appender-ref ref=\"ERROR_FILE\" /&gt; &lt;/root&gt; &lt;!--开发环境:打印控制台 end--&gt; &lt;!--生产环境:输出到文件--&gt; &lt;!--&lt;springProfile name=\"pro\"&gt;--&gt; &lt;!--&lt;root level=\"info\"&gt;--&gt; &lt;!--&lt;appender-ref ref=\"CONSOLE\" /&gt;--&gt; &lt;!--&lt;appender-ref ref=\"DEBUG_FILE\" /&gt;--&gt; &lt;!--&lt;appender-ref ref=\"INFO_FILE\" /&gt;--&gt; &lt;!--&lt;appender-ref ref=\"ERROR_FILE\" /&gt;--&gt; &lt;!--&lt;appender-ref ref=\"WARN_FILE\" /&gt;--&gt; &lt;!--&lt;/root&gt;--&gt; &lt;!--&lt;/springProfile&gt;--&gt;&lt;/configuration&gt; 定义好几种级别的日志分别输出到哪些文件中，然后在root标签里加入","link":"/2019/09/20/Springboot整合Logback日志/"},{"title":"Sprong-boot Mongodb使用GridFs做小型文件服务器","text":"话不多说，直接正题： 12compile(\"org.springframework.boot:spring-boot-starter-data-mongodb\")compile group: 'commons-codec', name: 'commons-codec', version: '1.10' 配置文件： 12345678spring: data: mongodb: database: *** host: *** username: *** password: *** port: *** 上传文件代码： 1234567891011121314151617@ResponseBody@PostMapping(value = \"/upload/file\")public String uploadSample(@RequestBody MultipartFile file) { try { String md5 = DigestUtils.md5Hex(file.getInputStream()); QueryBuilder builder = new QueryBuilder(); builder.and(\"md5\").is(md5); GridFSDBFile gridFSDBFile = gridFsTemplate.findOne(new BasicQuery(builder.get())); if (gridFSDBFile != null) { return gridFSDBFile.getId().toString(); } GridFSFile gridFSFile = gridFsTemplate.store(file.getInputStream(), file.getOriginalFilename(), file.getContentType()); returngridFSFile.getId().toString(); } catch (Exception e) { throw new RuntimeException(\"\"); }} 访问图片代码: 12345678@GetMapping(value = \"/get/file/{id}\")public void get(@PathVariable Object id, HttpServletResponse response) throws IOException { QueryBuilder builder = new QueryBuilder(); builder.and(\"_id\").is(id); GridFSDBFile file = gridFsTemplate.findOne(new BasicQuery(builder.get())); response.setContentType(file.getContentType()); file.writeTo(response.getOutputStream());} spring boot中默认配置可以满足基本需求，文件存储可以使用文件md5去重，非常方便，需要注意的是mongodb存储对文件大小的限制，这个具体数值请自行测试。 另一篇参考文档：https://blog.csdn.net/szs860806/article/details/72528618 https://www.songliguo.com/gridfs-mongodb-distributed-file-storage-system.html","link":"/2019/09/26/Sprong-boot-Mongodb使用GridFs做小型文件服务器/"},{"title":"SqlMapConfig.xml","text":"12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt; &lt;!-- mybatis的主配置文件 --&gt;&lt;configuration&gt; &lt;!-- 配置环境 (可以随便起个名但约定成俗就叫mysql)--&gt; &lt;environments default=\"mysql\"&gt; &lt;!-- 配置mysql的环境--&gt; &lt;environment id=\"mysql\"&gt; &lt;!-- 配置事务的类型--&gt; &lt;transactionManager type=\"JDBC\"&gt;&lt;/transactionManager&gt; &lt;!-- 配置数据源（连接池） --&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;!-- 配置连接数据库的4个基本信息 --&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/my_mybatis\"/&gt; &lt;property name=\"username\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"1234\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 指定映射配置文件的位置，映射配置文件指的是每个dao独立的配置文件 --&gt; &lt;mappers&gt; &lt;mapper resource=\"com/enlink/dao/IUserDao.xml\"/&gt; &lt;!-- 通过resource或者class来区分判断用的是配置文件还是注解方式 &lt;mapper class=\"com.enlink.dao.IUserDao\"/&gt; --&gt; &lt;/mappers&gt; &lt;/configuration&gt;","link":"/2019/07/10/SqlMapConfig-xml/"},{"title":"StringRedisTemplate  & redisTemplate","text":"RedisTemplate和StringRedisTemplate的区别 两者的关系是StringRedisTemplate继承RedisTemplate。 两者的数据是不共通的；也就是说StringRedisTemplate只能管理StringRedisTemplate里面的数据，RedisTemplate只能管理RedisTemplate中的数据。 SDR默认采用的序列化策略有两种，一种是String的序列化策略，一种是JDK的序列化策略。 StringRedisTemplate默认采用的是String的序列化策略，保存的key和value都是采用此策略序列化保存的。 RedisTemplate默认采用的是JDK的序列化策略，保存的key和value都是采用此策略序列化保存的。 RedisTemplate使用的序列类在在操作数据的时候，比如说存入数据会将数据先序列化成字节数组然后在存入Redis数据库，这个时候打开Redis查看的时候，你会看到你的数据不是以可读的形式展现的，而是以字节数组显示，类似下面 当然从Redis获取数据的时候也会默认将数据当做字节数组转化，这样就会导致一个问题，当需要获取的数据不是以字节数组存在redis当中而是正常的可读的字符串的时候，比如说下面这种形式的数据 注：使用的软件是RedisDesktopManager RedisTemplate就无法获取到数据，这个时候获取到的值就是NULL。这个时候StringRedisTempate就派上了用场。 当Redis当中的数据值是以可读的形式显示出来的时候，只能使用StringRedisTemplate才能获取到里面的数据。 所以当你使用RedisTemplate获取不到数据的时候请检查一下是不是Redis里面的数据是可读形式而非字节数组 另外我在测试的时候即使把StringRedisTemplate的序列化类修改成RedisTemplate的JdkSerializationRedisSerializer 最后还是无法获取被序列化的对象数据，即使是没有转化为对象的字节数组，代码如下 1234567891011121314151617181920212223242526272829@Test public void testRedisSerializer(){ User u = new User(); u.setName(\"java\"); u.setSex(\"male\"); redisTemplate.opsForHash().put(\"user:\",\"1\",u); /*查看redisTemplate 的Serializer*/ System.out.println(redisTemplate.getKeySerializer()); System.out.println(redisTemplate.getValueSerializer()); /*查看StringRedisTemplate 的Serializer*/ System.out.println(stringRedisTemplate.getValueSerializer()); System.out.println(stringRedisTemplate.getValueSerializer()); /*将stringRedisTemplate序列化类设置成RedisTemplate的序列化类*/ stringRedisTemplate.setKeySerializer(new JdkSerializationRedisSerializer()); stringRedisTemplate.setValueSerializer(new JdkSerializationRedisSerializer()); /*即使在更换stringRedisTemplate的的Serializer和redisTemplate一致的 * JdkSerializationRedisSerializer * 最后还是无法从redis中获取序列化的数据 * */ System.out.println(stringRedisTemplate.getValueSerializer()); System.out.println(stringRedisTemplate.getValueSerializer()); User user = (User) redisTemplate.opsForHash().get(\"user:\",\"1\"); User user2 = (User) stringRedisTemplate.opsForHash().get(\"user:\",\"1\"); System.out.println(\"dsd\"); } Debug结果 总结： 当你的redis数据库里面本来存的是字符串数据或者你要存取的数据就是字符串类型数据的时候，那么你就使用StringRedisTemplate即可，但是如果你的数据是复杂的对象类型，而取出的时候又不想做任何的数据转换，直接从Redis里面取出一个对象，那么使用RedisTemplate是更好的选择。","link":"/2019/07/18/StringRedisTemplate/"},{"title":"@SuppressWarning","text":"@SuppressWarnings(“rawtypes”) 是什么含义 简介：java.lang.SuppressWarnings是J2SE 5.0中标准的Annotation之一。可以标注在类、字段、方法、参数、构造方法，以及局部变量上。作用：告诉编译器忽略指定的警告，不用在编译完成后出现警告信息。使用：@SuppressWarnings(“”)@SuppressWarnings({})@SuppressWarnings(value={})根据sun的官方文档描述：value - 将由编译器在注释的元素中取消显示的警告集。允许使用重复的名称。忽略第二个和后面出现的名称。出现未被识别的警告名不是 错误：编译器必须忽略无法识别的所有警告名。但如果某个注释包含未被识别的警告名，那么编译器可以随意发出一个警告。各编译器供应商应该将它们所支持的警告名连同注释类型一起记录。鼓励各供应商之间相互合作，确保在多个编译器中使用相同的名称。示例：· @SuppressWarnings(“unchecked”)告诉编译器忽略 unchecked 警告信息，如使用List，ArrayList等未进行参数化产生的警告信息。· @SuppressWarnings(“serial”)如果编译器出现这样的警告信息：The serializable class WmailCalendar does not declare a static final serialVersionUID field of type long 使用这个注释将警告信息去掉。· @SuppressWarnings(“deprecation”)如果使用了使用@Deprecated注释的方法，编译器将出现警告信息。 使用这个注释将警告信息去掉。· @SuppressWarnings(“unchecked”, “deprecation”)告诉编译器同时忽略unchecked和deprecation的警告信息。· @SuppressWarnings(value={“unchecked”, “deprecation”})等同于@SuppressWarnings(“unchecked”, “deprecation”) SuppressWarnings压制警告，即去除警告rawtypes是说传参时也要传递带泛型的参数@SuppressWarnings(“unchecked”)告诉编译器忽略 unchecked 警告信息，如使用List，ArrayList等未进行参数化产生的警告信息。","link":"/2019/07/25/SuppressWarning/"},{"title":"Spring的@ExceptionHandler和@ControllerAdvice统一处理异常","text":"前言 之前敲代码的时候,避免不了各种try…catch, 如果业务复杂一点, 就会发现全都是try…catch 123456789try{ ..........}catch(Exception1 e){ ..........}catch(Exception2 e){ ...........}catch(Exception3 e){ ...........} 这样其实代码既不简洁好看 ,我们敲着也烦, 一般我们可能想到用拦截器去处理, 但是既然现在Spring这么火,AOP大家也不陌生, 那么Spring一定为我们想好了这个解决办法.果然: @ExceptionHandler源码 123456789//该注解作用对象为方法@Target({ElementType.METHOD})//在运行时有效@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface ExceptionHandler { //value()可以指定异常类 Class&lt;? extends Throwable&gt;[] value() default {};} @ControllerAdvice源码 123456789101112131415161718@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented//bean对象交给spring管理生成@Componentpublic @interface ControllerAdvice { @AliasFor(\"basePackages\") String[] value() default {}; @AliasFor(\"value\") String[] basePackages() default {}; Class&lt;?&gt;[] basePackageClasses() default {}; Class&lt;?&gt;[] assignableTypes() default {}; Class&lt;? extends Annotation&gt;[] annotations() default {};} 从名字上可以看出大体意思是控制器增强 所以结合上面我们可以知道,使用@ExceptionHandler，可以处理异常, 但是仅限于当前Controller中处理异常, @ControllerAdvice可以配置basePackage下的所有controller. 所以结合两者使用,就可以处理全局的异常了. 一、代码 这里需要声明的是，这个统一异常处理类，也是基于ControllerAdvice，也就是控制层切面的，如果是过滤器抛出的异常，不会被捕获!!! 在@ControllerAdvice注解下的类，里面的方法用@ExceptionHandler注解修饰的方法，会将对应的异常交给对应的方法处理。 12345@ExceptionHandler({IOException.class})public Result handleException(IOExceptione) { log.error(\"[handleException] \", e); return ResultUtil.failureDefaultError(); } 比如这个，就是捕获IO异常并处理。 废话不多说，代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223package com.zgd.shop.core.exception;import com.zgd.shop.core.error.ErrorCache;import com.zgd.shop.core.result.Result;import com.zgd.shop.core.result.ResultUtil;import lombok.extern.slf4j.Slf4j;import org.apache.commons.lang3.StringUtils;import org.springframework.http.HttpStatus;import org.springframework.http.converter.HttpMessageNotReadableException;import org.springframework.validation.BindException;import org.springframework.validation.BindingResult;import org.springframework.validation.FieldError;import org.springframework.web.HttpMediaTypeNotSupportedException;import org.springframework.web.HttpRequestMethodNotSupportedException;import org.springframework.web.bind.MethodArgumentNotValidException;import org.springframework.web.bind.MissingServletRequestParameterException;import org.springframework.web.bind.annotation.ControllerAdvice;import org.springframework.web.bind.annotation.ExceptionHandler;import org.springframework.web.bind.annotation.ResponseBody;import org.springframework.web.bind.annotation.ResponseStatus;import org.springframework.web.method.annotation.MethodArgumentTypeMismatchException;import javax.validation.ConstraintViolation;import javax.validation.ConstraintViolationException;import javax.validation.ValidationException;import java.util.Set;/** * GlobalExceptionHandle * 全局的异常处理 * * @author zgd * @date 2019/7/19 11:01 */@ControllerAdvice@ResponseBody@Slf4jpublic class GlobalExceptionHandle { /** * 请求参数错误 */ private final static String BASE_PARAM_ERR_CODE = \"BASE-PARAM-01\"; private final static String BASE_PARAM_ERR_MSG = \"参数校验不通过\"; /** * 无效的请求 */ private final static String BASE_BAD_REQUEST_ERR_CODE = \"BASE-PARAM-02\"; private final static String BASE_BAD_REQUEST_ERR_MSG = \"无效的请求\"; /** * 顶级的异常处理 * * @param e * @return */ @ResponseStatus(HttpStatus.OK) @ExceptionHandler({Exception.class}) public Result handleException(Exception e) { log.error(\"[handleException] \", e); return ResultUtil.failureDefaultError(); } /** * 自定义的异常处理 * * @param ex * @return */ @ResponseStatus(HttpStatus.OK) @ExceptionHandler({BizServiceException.class}) public Result serviceExceptionHandler(BizServiceException ex) { String errorCode = ex.getErrCode(); String msg = ex.getErrMsg() == null ? \"\" : ex.getErrMsg(); String innerErrMsg; String outerErrMsg; if (BASE_PARAM_ERR_CODE.equalsIgnoreCase(errorCode)) { innerErrMsg = \"参数校验不通过：\" + msg; outerErrMsg = BASE_PARAM_ERR_MSG; } else if (ex.isInnerError()) { innerErrMsg = ErrorCache.getInternalMsg(errorCode); outerErrMsg = ErrorCache.getMsg(errorCode); if (StringUtils.isNotBlank(msg)) { innerErrMsg = innerErrMsg + \"，\" + msg; outerErrMsg = outerErrMsg + \"，\" + msg; } } else { innerErrMsg = msg; outerErrMsg = msg; } log.info(\"【错误码】：{}，【错误码内部描述】：{}，【错误码外部描述】：{}\", errorCode, innerErrMsg, outerErrMsg); return ResultUtil.failure(errorCode, outerErrMsg); } /** * 缺少servlet请求参数抛出的异常 * * @param e * @return */ @ResponseStatus(HttpStatus.BAD_REQUEST) @ExceptionHandler({MissingServletRequestParameterException.class}) public Result handleMissingServletRequestParameterException(MissingServletRequestParameterException e) { log.warn(\"[handleMissingServletRequestParameterException] 参数错误: \" + e.getParameterName()); return ResultUtil.failure(BASE_PARAM_ERR_CODE, BASE_PARAM_ERR_MSG); } /** * 请求参数不能正确读取解析时，抛出的异常，比如传入和接受的参数类型不一致 * * @param e * @return */ @ResponseStatus(HttpStatus.OK) @ExceptionHandler({HttpMessageNotReadableException.class}) public Result handleHttpMessageNotReadableException(HttpMessageNotReadableException e) { log.warn(\"[handleHttpMessageNotReadableException] 参数解析失败：\", e); return ResultUtil.failure(BASE_PARAM_ERR_CODE, BASE_PARAM_ERR_MSG); } /** * 请求参数无效抛出的异常 * * @param e * @return */ @ResponseStatus(HttpStatus.BAD_REQUEST) @ExceptionHandler({MethodArgumentNotValidException.class}) public Result handleMethodArgumentNotValidException(MethodArgumentNotValidException e) { BindingResult result = e.getBindingResult(); String message = getBindResultMessage(result); log.warn(\"[handleMethodArgumentNotValidException] 参数验证失败：\" + message); return ResultUtil.failure(BASE_PARAM_ERR_CODE, BASE_PARAM_ERR_MSG); } private String getBindResultMessage(BindingResult result) { FieldError error = result.getFieldError(); String field = error != null ? error.getField() : \"空\"; String code = error != null ? error.getDefaultMessage() : \"空\"; return String.format(\"%s:%s\", field, code); } /** * 方法请求参数类型不匹配异常 * * @param e * @return */ @ResponseStatus(HttpStatus.BAD_REQUEST) @ExceptionHandler({MethodArgumentTypeMismatchException.class}) public Result handleMethodArgumentTypeMismatchException(MethodArgumentTypeMismatchException e) { log.warn(\"[handleMethodArgumentTypeMismatchException] 方法参数类型不匹配异常: \", e); return ResultUtil.failure(BASE_PARAM_ERR_CODE, BASE_PARAM_ERR_MSG); } /** * 请求参数绑定到controller请求参数时的异常 * * @param e * @return */ @ResponseStatus(HttpStatus.BAD_REQUEST) @ExceptionHandler({BindException.class}) public Result handleHttpMessageNotReadableException(BindException e) { BindingResult result = e.getBindingResult(); String message = getBindResultMessage(result); log.warn(\"[handleHttpMessageNotReadableException] 参数绑定失败：\" + message); return ResultUtil.failure(BASE_PARAM_ERR_CODE, BASE_PARAM_ERR_MSG); } /** * javax.validation:validation-api 校验参数抛出的异常 * * @param e * @return */ @ResponseStatus(HttpStatus.BAD_REQUEST) @ExceptionHandler({ConstraintViolationException.class}) public Result handleServiceException(ConstraintViolationException e) { Set&lt;ConstraintViolation&lt;?&gt;&gt; violations = e.getConstraintViolations(); ConstraintViolation&lt;?&gt; violation = violations.iterator().next(); String message = violation.getMessage(); log.warn(\"[handleServiceException] 参数验证失败：\" + message); return ResultUtil.failure(BASE_PARAM_ERR_CODE, BASE_PARAM_ERR_MSG); } /** * javax.validation 下校验参数时抛出的异常 * * @param e * @return */ @ResponseStatus(HttpStatus.BAD_REQUEST) @ExceptionHandler({ValidationException.class}) public Result handleValidationException(ValidationException e) { log.warn(\"[handleValidationException] 参数验证失败：\", e); return ResultUtil.failure(BASE_PARAM_ERR_CODE, BASE_PARAM_ERR_MSG); } /** * 不支持该请求方法时抛出的异常 * * @param e * @return */ @ResponseStatus(HttpStatus.METHOD_NOT_ALLOWED) @ExceptionHandler({HttpRequestMethodNotSupportedException.class}) public Result handleHttpRequestMethodNotSupportedException(HttpRequestMethodNotSupportedException e) { log.warn(\"[handleHttpRequestMethodNotSupportedException] 不支持当前请求方法: \", e); return ResultUtil.failure(BASE_BAD_REQUEST_ERR_CODE, BASE_BAD_REQUEST_ERR_MSG); } /** * 不支持当前媒体类型抛出的异常 * * @param e * @return */ @ResponseStatus(HttpStatus.UNSUPPORTED_MEDIA_TYPE) @ExceptionHandler({HttpMediaTypeNotSupportedException.class}) public Result handleHttpMediaTypeNotSupportedException(HttpMediaTypeNotSupportedException e) { log.warn(\"[handleHttpMediaTypeNotSupportedException] 不支持当前媒体类型: \", e); return ResultUtil.failure(BASE_BAD_REQUEST_ERR_CODE, BASE_BAD_REQUEST_ERR_MSG); } 至于返回值，就可以理解为controller层方法的返回值，可以返回@ResponseBody，或者页面。我这里是一个@ResponseBody的Result&lt;&gt;，前后端分离。 我们也可以自己根据需求，捕获更多的异常类型。 包括我们自定义的异常类型。比如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.zgd.shop.core.exception;import lombok.Data;/** * BizServiceException * 业务抛出的异常 * @author zgd * @date 2019/7/19 11:04 */@Datapublic class BizServiceException extends RuntimeException{ private String errCode; private String errMsg; private boolean isInnerError; public BizServiceException(){ this.isInnerError=false; } public BizServiceException(String errCode){ this.errCode =errCode; this.isInnerError = false; } public BizServiceException(String errCode,boolean isInnerError){ this.errCode =errCode; this.isInnerError = isInnerError; } public BizServiceException(String errCode,String errMsg){ this.errCode =errCode; this.errMsg = errMsg; this.isInnerError = false; } public BizServiceException(String errCode,String errMsg,boolean isInnerError){ this.errCode =errCode; this.errMsg = errMsg; this.isInnerError = isInnerError; }}","link":"/2019/10/15/Spring的@ExceptionHandler和@ControllerAdvice统一处理异常/"},{"title":"TkMybatis","text":"https://www.cnblogs.com/panchanggui/p/10748986.html 前提: 基于SpringBoot项目,正常集成Mybatis后,为了简化sql语句的编写,甚至达到无mapper.xml文件。在本篇总结教程,不在进行SpringBoot集成Mybatis的概述。如有需要,请查看我另一篇文章 SpringBoot集成MyBatis，这里不再赘述。 一. 实现步骤 引入TkMybatis的Maven依赖 实体类的相关配置,@Id,@Table Mapper继承tkMabatis的Mapper接口 启动类Application或自定义Mybatis配置类上使用@MapperScan注解扫描Mapper接口 在application.properties配置文件中,配置mapper.xml文件指定的位置[可选] 使用TkMybatis提供的sql执行方法 PS : 1. TkMybatis默认使用继承Mapper接口中传入的实体类对象去数据库寻找对应的表,因此如果表名与实体类名不满足对应规则时,会报错,这时使用@Table为实体类指定表。(这种对应规则为驼峰命名规则) 2. 使用TkMybatis可以无xml文件实现数据库操作,只需要继承tkMybatis的Mapper接口即可。 3. 如果有自定义特殊的需求,可以添加mapper.xml进行自定义sql书写,但路径必须与步骤4对应。 如有需要,实现mapper.xml自定义sql语句二. 实现细节 1 引入TkMybatis的Maven依赖&lt;!-- https://mvnrepository.com/artifact/tk.mybatis/mapper --&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;version&gt;4.0.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/tk.mybatis/mapper-spring-boot-starter --&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.3&lt;/version&gt; &lt;/dependency&gt; 2 实体类的配置TkMybatis默认使用继承Mapper接口中传入的实体类对象去数据库寻找对应的表,因此如果表名与实体类名不满足对应规则时,会报错,这时使用@Table为实体类指定表。(这种对应规则为驼峰命名规则)下面以一个实体类Custoemr为例： package cn.invengo.middleware.base.model; import java.util.Date; import javax.persistence.GeneratedValue;import javax.persistence.GenerationType;import javax.persistence.Id;import javax.persistence.Table; // @Table指定该实体类对应的表名,如表名为base_customer,类名为BaseCustomer可以不需要此注解@Table(name = “t_base_customer”)public class Customer { // @Id表示该字段对应数据库表的主键id // @GeneratedValue中strategy表示使用数据库自带的主键生成策略. // @GeneratedValue中generator配置为&quot;JDBC&quot;,在数据插入完毕之后,会自动将主键id填充到实体类中.类似普通mapper.xml中配置的selectKey标签 @Id @GeneratedValue(strategy = GenerationType.IDENTITY,generator = &quot;JDBC&quot;) private Long id; private String name; private String code; private String status; private String linkman; private String linkmanPhone; private String remark; private String attr01; private String attr02; private String attr03; private Date createDate; private Date lastUpdate; private Long creater; private Long lastUpdateMan; getter(),setter()方法省略...} 2.3 Mapper继承tkMabatis的Mapper接口import cn.base.model.Customer;import tk.mybatis.mapper.common.Mapper; /** @ClassName: CustomerMapper @Description:TODO(Customer数据库操作层) @author: wwj @date: 2018年8月31日 上午10:12:20 / public interface CustomerMapper extends Mapper {} 01.png2.4 启动类Application或自定义Mybatis配置类上使用@MapperScan注解扫描Mapper接口import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.builder.SpringApplicationBuilder;import org.springframework.boot.context.web.SpringBootServletInitializer; import tk.mybatis.spring.annotation.MapperScan; @SuppressWarnings(“deprecation”)@SpringBootApplication@MapperScan(“cn.base.mapper”)public class MiddlewareApplication extends SpringBootServletInitializer { private static Logger logger = LoggerFactory.getLogger(MiddlewareApplication.class); public static void main(String[] args) { SpringApplication.run(MiddlewareApplication.class, args); logger.info(&quot;Application Start Success!&quot;); } // 当SpringBoot项目打成war包发布时,需要继承SpringBootServletInitializer接口实现该方法 @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) { return builder.sources(Application.class); }} 2.5 application.properties配置mapper.xml配置文件的扫描路径mybatis.mapperLocations=classpath:cn/base/mapper/.xml2.6 使用TkMybatis提供的sql执行方法import java.util.List;import java.util.Objects; import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional; import cn.invengo.middleware.base.mapper.CustomerMapper;import cn.invengo.middleware.base.model.Customer;import cn.invengo.middleware.base.service.BaseCustomerService;import tk.mybatis.mapper.entity.Example;import tk.mybatis.mapper.entity.Example.Criteria; @Service@Transactionalpublic class BaseCustomerServiceImpl implements BaseCustomerService { @Autowired private CustomerMapper customerMapper; @Override public List&lt;Customer&gt; selectByEntity(Customer customer) { if(Objects.isNull(customer)) { customer = new Customer(); } Example example = new Example(Customer.class); Criteria criteria = example.createCriteria(); criteria.andEqualTo(customer); return customerMapper.selectByExample(example); } @Override public int insertSelective(Customer customer) { return customerMapper.insertSelective(customer); } @Override public int updateSelectiveById(Customer record) { return customerMapper.updateByPrimaryKeySelective(record); }} 02.png2.7 如有需要,自定义mapper.xml配置文件,完成自定义sql编写ps: 大多数复杂的需求,都能通过TkMyBatis的组合完成操作。这里以联表查询为例,自定义mapper.xml的sql编写。 该mapper.xml与以往普通方式的mapper.xml文件不同之处在于,这里不需要使用resultMap进行字段的映射。当然如果想在返回的Map中新增返回字段映射直接添加新的字段即可。 使用tkmybatis,在数据模型修改之后,修改代码也较为简便,只需要修改对应实体类中的字段即可。 eg:ContainerMapper.xml: SELECT c.`code` FROM `t_base_container` c LEFT OUTER JOIN t_base_device d ON c.id = d.container_id WHERE d.id = #{deviceId,jdbcType=BIGINT}; ps:这里需要注意的是,不要自己在mapper.xml中在书写tkMybatis已经有的一些基础方法了,否则会报错提示方法重复。本篇总结到此结束。","link":"/2020/03/03/TkMybatis/"},{"title":"TCP UDP","text":"1、TCP和UDP的区别（1）TCP面向连接；UDP面向无连接（2）TCP保证数据正确性；UDP可能丢包（3）TCP传输速度慢；UDP速度快（4）每一条TCP连接只能是点到点的；UDP支持一对一，一对多，多对一和多对多的交互通信（5）TCP对系统资源要求较多，UDP对系统资源要求较少。2、三次握手三次握手的目的是建立可靠的通信信道。确认自己与对方的发送与接收机能正常。TCP三次握手过程:1、主机A通过向主机B 发送一个含有同步序列号的标志位的数据段给主机B ，向主机B请求建立连接,通过这个数据段,主机A告诉主机B两件事：我想要和你通信、你可以用哪个序列号作为起始数据段来回应我。2、主机B收到主机A的请求后,用一个带有确认应答(ACK)和同步序列号(SYN)标志位的数据段响应主机A,也告诉主机A两件事：我已经收到你的请求了,你可以传输数据了、你要用哪个序列号作为起始数据段来回应我。3、主机A收到这个数据段后，再发送一个确认应答，确认已收到主机B的数据段：我已收到回复，我现在要开始传输实际数据了。这样3次握手就完成了,主机A和主机B 就可以传输数据了.这里写图片描述为什么需要三次握手：三次握手能确认双发收发功能都正常，缺一不可。第一次握手：Client什么都不能确认；Server确认了对方发送正常。第二次握手：Client确认了：自己发送、接收正常，对方发送、接收正常；Server确认了：自己接收正常，对方发送正常。第三次握手：Client确认了：自己发送、接收正常，对方发送、接收正常；Server确认了：自己发送、接收正常，对方发送接收正常。三次握手的另一个目标是确认确认双方都支持TCP，告知对方用TCP传输。第一次握手：Server猜测Client可能要建立TCP请求，但不确定，因为也可能是Client乱发了一个数据包给自己。第二次握手：通过ack=J+1，Client知道Server是支持TCP的，且理解了自己要建立TCP连接的意图。第三次握手：通过ack=K+1，Server知道Client是支持TCP的，且确实是要建立TCP连接。3、四次挥手1、第一次挥手：主机1（可以使客户端，也可以是服务器端），设置Sequence Number和Acknowledgment Number，向主机2发送一个FIN报文段；此时，主机1进入FIN_WAIT_1状态；这表示主机1没有数据要发送给主机2了。2、第二次挥手：主机2收到了主机1发送的FIN报文段，向主机1回一个ACK报文段，Acknowledgment Number为Sequence Number加1；主机1进入FIN_WAIT_2状态；主机2告诉主机1，我“同意”你的关闭请求。3、第三次挥手：主机2向主机1发送FIN报文段，请求关闭连接，同时主机2进入LAST_ACK状态。4、第四次挥手：主机1收到主机2发送的FIN报文段，向主机2发送ACK报文段，然后主机1进入TIME_WAIT状态；主机2收到主机1的ACK报文段以后，就关闭连接；此时，主机1等待2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，主机1也可以关闭连接了。为什么要time wait，为什么是2msl这里写图片描述为了保证A发送的最后一个ACK报文能够到达B。这个ACK报文段有可能丢失，因而使处在LAST-ACK状态的B收不到对已发送的FIN+ACK报文段的确认。B会超时重传这个FIN+ACK报文段，而A就能在2MSL时间内收到这个重传的FIN+ACK报文段。如果A在TIME-WAIT状态不等待一段时间，而是在发送完ACK报文段后就立即释放连接，就无法收到B重传的FIN+ACK报文段，因而也不会再发送一次确认报文段。这样，B就无法按照正常的步骤进入CLOSED状态。MSL指的是任何IP数据报能够在因特网上存活的最长时间。假设现在一个MSL的时候，接收端需要发送一个应答，这时候，我们也必须等待这个应答的消失，这个应答的消失也是需要一个MSL，所以我们需要等待2MSL。","link":"/2019/07/22/TCP-UDP/"},{"title":"@Table 注解详解","text":"spring @Table注解作用是 ： 声明此对象映射到数据库的数据表，通过它可以为实体指定表(talbe) 常用的两个属性：1、name 用来命名 当前实体类 对应的数据库 表的名字@Table(name = &quot;tab_user&quot;） 2、uniqueConstraints 用来批量命名唯一键其作用等同于多个：@Column(unique = true) @Table(name = &quot;tab_user&quot;,uniqueConstraints = {@UniqueConstraint(columnNames={&quot;uid&quot;,&quot;email&quot;})})","link":"/2019/10/11/able-注解详解/"},{"title":"abstract interface","text":"abstract interface区别 含有 abstract 修饰符 class 即为抽象类，抽象类不能创建实际对象，含有抽象方法的抽象类必须定义为 abstract class。 接口可以说成是一种特殊的抽象类，接口中的所有方法都必须是抽象的，接口中的方法定义默认为 public abstract 类型，接口中的成员产量类型默认为 public static final。 两者的区别: a. 抽象类可以有构造方法，接口中不能有构造方法。 b. 抽象类中可以有普通成员变量，接口中没有普通成员变量。 c. 抽象类中可以包含非抽象普通方法，接口中的所有方法必须都是抽象的，不能有非抽象的方法。 d. 抽象类中的抽象方法的访问权限可以是 public、protected 和(默认类型，虽然 eclipse 不报错，但也不能用，默认类型子类不能继承)，接口中的抽象方法只能是 public 类型的，并且默认即为 public abstract 类型。 e. 抽象类中可以包含静态方法，在 JDK1.8 之前接口中不能不包含静态方法，JDK1.8 以后可以包含。 f. 抽象类和接口中都可以包含静态成员变量，抽象类中的静态成员变量的访问权限可以是任意的，但接口中定义的变量只能是 public static final 类型的，并且默认即为 public static final 类型。 g. 一个类可以实现多个接口，用逗号隔开，但只能继承一个抽象类，接口不可以实现接口，但可以继承接口，并且可以继承多个接口，用逗号隔开。","link":"/2019/07/27/abstract-interface/"},{"title":"WebMvcConfigurer","text":"WebMvcConfigurer配置类其实是Spring内部的一种配置方式，采用JavaBean的形式来代替传统的xml配置文件形式进行针对框架个性化定制。基于java-based方式的spring mvc配置，需要创建一个配置类并实现WebMvcConfigurer 接口，WebMvcConfigurerAdapter 抽象类是对WebMvcConfigurer接口的简单抽象（增加了一些默认实现），但在在SpringBoot2.0及Spring5.0中WebMvcConfigurerAdapter已被废弃 。官方推荐直接实现WebMvcConfigurer或者直接继承WebMvcConfigurationSupport，方式一实现WebMvcConfigurer接口（推荐），方式二继承WebMvcConfigurationSupport类，具体实现可看这篇文章。https://blog.csdn.net/fmwind/article/details/82832758 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//// Source code recreated from a .class file by IntelliJ IDEA// (powered by Fernflower decompiler)// package org.springframework.web.servlet.config.annotation; import java.util.List;import org.springframework.format.FormatterRegistry;import org.springframework.http.converter.HttpMessageConverter;import org.springframework.validation.MessageCodesResolver;import org.springframework.validation.Validator;import org.springframework.web.method.support.HandlerMethodArgumentResolver;import org.springframework.web.method.support.HandlerMethodReturnValueHandler;import org.springframework.web.servlet.HandlerExceptionResolver; public interface WebMvcConfigurer { void configurePathMatch(PathMatchConfigurer var1); void configureContentNegotiation(ContentNegotiationConfigurer var1); void configureAsyncSupport(AsyncSupportConfigurer var1); void configureDefaultServletHandling(DefaultServletHandlerConfigurer var1); void addFormatters(FormatterRegistry var1); void addInterceptors(InterceptorRegistry var1); void addResourceHandlers(ResourceHandlerRegistry var1); void addCorsMappings(CorsRegistry var1); void addViewControllers(ViewControllerRegistry var1); void configureViewResolvers(ViewResolverRegistry var1); void addArgumentResolvers(List&lt;HandlerMethodArgumentResolver&gt; var1); void addReturnValueHandlers(List&lt;HandlerMethodReturnValueHandler&gt; var1); void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; var1); void extendMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; var1); void configureHandlerExceptionResolvers(List&lt;HandlerExceptionResolver&gt; var1); void extendHandlerExceptionResolvers(List&lt;HandlerExceptionResolver&gt; var1); Validator getValidator(); MessageCodesResolver getMessageCodesResolver();} 接下来我们着重找几个方法讲解一下： 12345678910111213141516 /* 拦截器配置 */void addInterceptors(InterceptorRegistry var1);/* 视图跳转控制器 */void addViewControllers(ViewControllerRegistry registry);/** *静态资源处理**/void addResourceHandlers(ResourceHandlerRegistry registry);/* 默认静态资源处理器 */void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer);/** * 这里配置视图解析器 **/void configureViewResolvers(ViewResolverRegistry registry);/* 配置内容裁决的一些选项*/void configureContentNegotiation(ContentNegotiationConfigurer configurer); 1、addInterceptors(InterceptorRegistry registry)此方法用来专门注册一个Interceptor，如HandlerInterceptorAdapter 12345678@Configurationpublic class MyWebMvcConfigurer implements WebMvcConfigurer { @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(new MyInterceptor()).addPathPatterns(\"/**\").excludePathPatterns(\"/emp/toLogin\",\"/emp/login\",\"/js/**\",\"/css/**\",\"/images/**\"); }} addPathPatterns(“/**”)对所有请求都拦截，但是排除了/toLogin和/login请求的拦截。 当spring boot版本升级为2.x时，访问静态资源就会被HandlerInterceptor拦截,网上有很多处理办法都是如下写法 .excludePathPatterns(“/index.html”,”/“,”/user/login”,”/static/**”);可惜本人在使用时一直不起作用，查看请求的路径里并没有/static/如图： 于是我改成了”/js/“,”/css/“,”/images/**”这样页面内容就可以正常访问了，我的项目结构如下： 页面跳转addViewControllers以前写SpringMVC的时候，如果需要访问一个页面，必须要写Controller类，然后再写一个方法跳转到页面，感觉好麻烦，其实重写WebMvcConfigurer中的addViewControllers方法即可达到效果了 123456789/** * 以前要访问一个页面需要先创建个Controller控制类，再写方法跳转到页面 * 在这里配置后就不需要那么麻烦了，直接访问http://localhost:8080/toLogin就跳转到login.jsp页面了 * @param registry */ @Override public void addViewControllers(ViewControllerRegistry registry) { registry.addViewController(\"/toLogin\").setViewName(\"login\"); } 值的指出的是，在这里重写addViewControllers方法，并不会覆盖WebMvcAutoConfiguration中的addViewControllers（在此方法中，Spring Boot将“/”映射至index.html），这也就意味着我们自己的配置和Spring Boot的自动配置同时有效，这也是我们推荐添加自己的MVC配置的方式。 自定义资源映射addResourceHandlers比如，我们想自定义静态资源映射目录的话，只需重写addResourceHandlers方法即可。 注：如果继承WebMvcConfigurationSupport类实现配置时必须要重写该方法，具体见其它文章 123456789101112@Configurationpublic class MyWebMvcConfigurerAdapter implements WebMvcConfigurer { /** * 配置静态访问资源 * @param registry */ @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\"/my/**\").addResourceLocations(\"classpath:/my/\"); } } 通过addResourceHandler添加映射路径，然后通过addResourceLocations来指定路径。我们访问自定义my文件夹中的elephant.jpg 图片的地址为 http://localhost:8080/my/elephant.jpg 如果你想指定外部的目录也很简单，直接addResourceLocations指定即可，代码如下： 1234@Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\"/my/**\").addResourceLocations(\"file:E:/my/\");} addResourceLocations指的是文件放置的目录，addResoureHandler指的是对外暴露的访问路径 configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) 用法： 1234@Override public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) { configurer.enable(); configurer.enable(\"defaultServletName\"); 此时会注册一个默认的Handler：DefaultServletHttpRequestHandler，这个Handler也是用来处理静态文件的，它会尝试映射/。当DispatcherServelt映射/时（/ 和/ 是有区别的），并且没有找到合适的Handler来处理请求时，就会交给DefaultServletHttpRequestHandler 来处理。注意：这里的静态资源是放置在web根目录下，而非WEB-INF 下。 可能这里的描述有点不好懂（我自己也这么觉得），所以简单举个例子，例如：在webroot目录下有一个图片：1.png 我们知道Servelt规范中web根目录（webroot）下的文件可以直接访问的，但是由于DispatcherServlet配置了映射路径是：/ ，它几乎把所有的请求都拦截了，从而导致1.png 访问不到，这时注册一个DefaultServletHttpRequestHandler 就可以解决这个问题。其实可以理解为DispatcherServlet破坏了Servlet的一个特性（根目录下的文件可以直接访问），DefaultServletHttpRequestHandler是帮助回归这个特性的。 5、configureViewResolvers(ViewResolverRegistry registry) 从方法名称我们就能看出这个方法是用来配置视图解析器的，该方法的参数ViewResolverRegistry 是一个注册器，用来注册你想自定义的视图解析器等。ViewResolverRegistry 常用的几个方法： １)．enableContentNegotiation() 1234 /** 启用内容裁决视图解析器*/public void enableContentNegotiation(View... defaultViews) { initContentNegotiatingViewResolver(defaultViews); } 该方法会创建一个内容裁决解析器ContentNegotiatingViewResolver ，该解析器不进行具体视图的解析，而是管理你注册的所有视图解析器，所有的视图会先经过它进行解析，然后由它来决定具体使用哪个解析器进行解析。具体的映射规则是根据请求的media types来决定的。2). UrlBasedViewResolverRegistration() 123456 public UrlBasedViewResolverRegistration jsp(String prefix, String suffix) {InternalResourceViewResolver resolver = new InternalResourceViewResolver();resolver.setPrefix(prefix);resolver.setSuffix(suffix);this.viewResolvers.add(resolver);return new UrlBasedViewResolverRegistration(resolver); 该方法会注册一个内部资源视图解析器InternalResourceViewResolver 显然访问的所有jsp都是它进行解析的。该方法参数用来指定路径的前缀和文件后缀，如： registry.jsp(&quot;/WEB-INF/jsp/&quot;, &quot;.jsp&quot;); 对于以上配置，假如返回的视图名称是example，它会返回/WEB-INF/jsp/example.jsp给前端，找不到则报404。 3). beanName() 123public void beanName() { BeanNameViewResolver resolver = new BeanNameViewResolver(); this.viewResolvers.add(resolver); 该方法会注册一个BeanNameViewResolver 视图解析器，这个解析器是干嘛的呢？它主要是将视图名称解析成对应的bean。什么意思呢？假如返回的视图名称是example，它会到spring容器中找有没有一个叫example的bean，并且这个bean是View.class类型的？如果有，返回这个bean。 4). viewResolver() 1234567 public void viewResolver(ViewResolver viewResolver) { if (viewResolver instanceof ContentNegotiatingViewResolver) { throw new BeanInitializationException( \"addViewResolver cannot be used to configure a ContentNegotiatingViewResolver. Please use the method enableContentNegotiation instead.\"); } this.viewResolvers.add(viewResolver);} 这个方法想必看名字就知道了，它就是用来注册各种各样的视图解析器的，包括自己定义的。 configureContentNegotiation(ContentNegotiationConfigurer configurer) 上面我们讲了configureViewResolvers 方法，假如在该方法中我们启用了内容裁决解析器，那么configureContentNegotiation(ContentNegotiationConfigurer configurer) 这个方法是专门用来配置内容裁决的一些参数的。这个比较简单，我们直接通过一个例子看： 12345678910111213public void configureContentNegotiation(ContentNegotiationConfigurer configurer) { /* 是否通过请求Url的扩展名来决定media type */ configurer.favorPathExtension(true) /* 不检查Accept请求头 */ .ignoreAcceptHeader(true) .parameterName(\"mediaType\") /* 设置默认的media yype */ .defaultContentType(MediaType.TEXT_HTML) /* 请求以.html结尾的会被当成MediaType.TEXT_HTML*/ .mediaType(\"html\", MediaType.TEXT_HTML) /* 请求以.json结尾的会被当成MediaType.APPLICATION_JSON*/ .mediaType(\"json\", MediaType.APPLICATION_JSON);} 到这里我们就可以举个例子来进一步熟悉下我们上面讲的知识了，假如我们MVC的配置如下： 1234567891011121314151617181920@EnableWebMvc @Configuration public class MyWebMvcConfigurerAdapte extends WebMvcConfigurerAdapter { @Override public void configureViewResolvers(ViewResolverRegistry registry) { registry.jsp(\"/WEB-INF/jsp/\", \".jsp\"); registry.enableContentNegotiation(new MappingJackson2JsonView()); } @Override public void configureContentNegotiation(ContentNegotiationConfigurer configurer) { configurer.favorPathExtension(true) .ignoreAcceptHeader(true) .parameterName(\"mediaType\") .defaultContentType(MediaType.TEXT_HTML) .mediaType(\"html\", MediaType.TEXT_HTML) .mediaType(\"json\", MediaType.APPLICATION_JSON); }} controller的代码如下: 123456789@Controller public class ExampleController { @RequestMapping(\"/test\") public ModelAndView test() { Map&lt;String, String&gt; map = new HashMap(); map.put(\"哈哈\", \"哈哈哈哈\"); map.put(\"呵呵\", \"呵呵呵呵\"); return new ModelAndView(\"test\", map); } 在WEB-INF/jsp目录下创建一个test.jsp文件，内容随意。现在启动tomcat，在浏览器输入以下链接：http://localhost:8080/test.json，浏览器内容返回如下： 1234{ \"哈哈\":\"哈哈哈哈\", \"呵呵\":\"呵呵呵呵\"} 在浏览器输入http://localhost:8080/test 或者http://localhost:8080/test.html，内容返回如下：this is test.jsp 显然，两次使用了不同的视图解析器，那么底层到底发生了什么？在配置里我们注册了两个视图解析器：ContentNegotiatingViewResolver 和 InternalResourceViewResolver，还有一个默认视图：MappingJackson2JsonView。controller执行完毕之后返回一个ModelAndView，其中视图的名称为example1。 1231.返回首先会交给ContentNegotiatingViewResolver 进行视图解析处理，而ContentNegotiatingViewResolver 会先把视图名example1交给它持有的所有ViewResolver尝试进行解析（本实例中只有InternalResourceViewResolver），2.根据请求的mediaType，再将example1.mediaType（这里是example1.json 和example1.html）作为视图名让所有视图解析器解析一遍，两步解析完毕之后会获得一堆候选的List&lt;View&gt; 再加上默认的MappingJackson2JsonView ，3.根据请求的media type从候选的List&lt;View&gt; 中选择一个最佳的返回，至此视图解析完毕。 现在就可以理解上例中为何请求链接加上.json 和不.json 结果会不一样。当加上.json 时，表示请求的media type 为MediaType.APPLICATION_JSON，而InternalResourceViewResolver 解析出来的视图的ContentType与其不符，而与MappingJackson2JsonView 的ContentType相符，所以选择了MappingJackson2JsonView 作为视图返回。当不加.json 请求时，默认的media type 为MediaType.TEXT_HTML，所以就使用了InternalResourceViewResolver解析出来的视图作为返回值了。我想看到这里你已经大致可以自定义视图了。","link":"/2019/08/29/WebMvcConfigurer/"},{"title":"annotation ParameterizedTpye","text":"12345678910111213141516171819202122232425262728private static Map&lt;String,Mapper&gt; loadMapperAnnotation(String daoClassPath){ //得到dao接口的字节码对象 Class daoClass = Class.forName(daoClassPath); //得到dao接口中的方法数组 Method[] methods = daoClass.getMethods(); for(Method method : methods){ //取出注解的属性值 Select xxxAnno = method.getAnnotation(xxx.class); String queryString = xxxAnno.value(); mapper.setQueryString(queryString); //获取返回值的泛型类型 Type type = method.getGenericReturnType(); //判断type是不是参数化的类型 if(type instanceof ParamerizedType){ //强转 ParamerizedType ptype = (ParamerizedType)type; //参数化类型获取真实类型 Type[] types = ptype.getActualTypeArguments(); //取出第一个 Class domainClass = (Class)types[0]; String resultType = domainClass.getName(); mapper.setResultType(resultType); } String methodName = method.getName(); String ClassName = method.getDeclaringClass().getName(); String key = className + \".\" + methodName; mappers.put(key,mapper); }","link":"/2019/07/17/annotation-ParameterizedTpye/"},{"title":"ali系统","text":"双十一大概会产生多大的数据量呢，可能大家没概念，举个例子央视拍了这么多年电视新闻节目，几十年下来他存了大概80P的数据。而今年双11一天，阿里要处理970P的数据，做为一个IT人，笔者认为今年”双十一“阿里最大的技术看点有有以下两个： 阿里的数据库，也就是刚刚拿下TPC冠军的OcceanBase，处理峰值也达到了骇人听闻的6100万次/秒，阿里核心系统百分百上云了。如果把信息系统比做一个武林高手，那么如此之大的交易量代表了他的刚猛威武，而全面触云又代表他灵动飘逸。而能把刚猛和灵活完美结合是简直是神才能达到的境界。 上云虽好，但不适合大规模计算，由于底层与用户之间多了一个虚拟化层，所以云计算平台一般都会产生10%左右的调度损耗，而这10%的损耗也让很多密集计算型的应用场景不太合适使用云平台。所以站在IT视角，云计算也不太合适双十一的场景。那么阿里刚猛兼顾灵活的武功是如何练成的呢？ 乾坤大挪移-Tair 通过阿里的官宣来看在Tair之前还有一个LVS的负载均衡层，不过那些都不是国产的自研技术，也不细表了。 Tair是阿里自研的开源缓存服务中间件（Github地址：。https://github.com/alibaba/tair）提供快速访问的内存（MDB引擎）/持久化（LDB引擎）存储服务，基于高性能、高可用的分布式集群架构，满足读写性能要求高及容量可弹性伸缩的业务需求，在双十一秒杀的体系内完成乾坤大挪移般的加速工作。 通常情况下，一个 Tair 集群中包含2台 Configserver 及多台 DataServer。其中两台 Configserver 互为主备。通过和 Dataserver 之间的心跳检测获取集群中存活可用的 Dataserver，构建数据在集群中的分布信息。Dataserver 负责数据的存储，并按照 Configserver 的指示完成数据的复制和迁移工作。Client 在启动的时候，从 Configserver 获取数据分布信息，根据数据分布信息，和相应的 Dataserver 进行交互，完成用户的请求。 其核心的模块就是Configserver，具体的代码在 https://github.com/alibaba/tair/blob/master/src/configserver/conf_server_table_manager.cpp 以初始化函数为例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455using namespace std; void conf_server_table_manager::init() { flag = client_version = server_version = server_bucket_count = server_copy_count = plugins_version = area_capacity_version = last_load_config_time = NULL;//初始化标志位 migrate_block_count = NULL; hash_table = m_hash_table = d_hash_table = NULL; file_name = \"\"; hash_table_deflate_data_for_client = hash_table_deflate_data_for_data_server = NULL; hash_table_deflate_data_for_client_size = hash_table_deflate_data_for_data_server_size = 0; file_opened = false; if(transition_version != NULL)//如不为空，则删除 { delete transition_version; } if(recovery_version != NULL)//如不为空，则删除 { delete recovery_version; } if(recovery_block_count != NULL) //如不为空，则删除 { delete recovery_block_count; } transition_version = new uint32_t(); recovery_version = new uint32_t(); recovery_block_count = new int32_t(); *transition_version = 0; *recovery_version = 0; *recovery_block_count = -1; } 凌波微步–SOFAStack：SOFAStack（Scalable Open Financial Architecture Stack）是阿里研发的一套开源的用于构建微服务的分布式中间件（Github地址：https://github.com/sofastack），微服务最大的优势就是方便灵活，与凌波微步的武功有异曲同工之妙。它包含了构建微服务体系的众多组件，包括研发框架、RPC 框架，服务注册中心，分布式链路追踪，Metrics监控度量、分布式事务框架、服务治理平台等，结合社区优秀的开源产品，可以快速搭建一套完善的微服务体系。：https://blog.csdn.net/BEYONDMA/article/details/103213493 使用SOFAStack可以快速的构建出架构完整的微服务体系： 九阳神功-OceanBase:勇夺TPC冠军的OceanBase也是阿里自研的金融级关系型数据库，由于是其集群化的特性使其具备了有如九阳神功般取之不尽，用之不竭的内力。 笔者在前文《200行代码解读国产数据库阿里OceanBase的速度之源》 《揭秘OceanBase的王者攻略》已经对于这个数据库做了详尽的介绍，这里不再赘述。 武穆遗书-飞天操作系统：飞天（Apsara）是由阿里云自主研发、服务全球的超大规模通用计算操作系统。正所谓韩信点兵，多多益善，飞天能将百万级服务器连成一台超级计算机，还能有条不紊的通过云计算向用户提供计算能力。 我们看到在飞天的基础公共模块之上，有两个最核心的服务，一个是盘古，另一个是伏羲。盘古是存储管理服务，伏羲是资源调度服务，飞天内核之上应用的存储和资源的分配都是由盘古和伏羲管理。具体见下图： 飞天最底层是遍布全球的几十个数据中心，成百上千万台服务器，把这么多服务器连成一片变成一个整体，绝对是上乘的兵法才能做到的，所以飞天堪称是阿里“双十一“的武穆兵法。 真武七截阵-神龙云服务器: 在《倚天屠龙记》中真武七截阵是由张三丰创始的阵法，人数越多威力越强，而神龙云服务器也其最大的特点就是把虚拟化层的损耗几乎为零，随着物理服务数量的增多，性能却一点也不打折，堪称IT界的真武七截阵。 其最大亮点是阿里自研的MOC芯片，MOC是专门用于虚拟化层的调度服务，将宝贵的CPU与内存资源由复杂的云调度中解放出来，开创了一种新型的云服务器形式，神龙能与阿里云产品家族中的其他计算产品无缝对接。比如存储、网络、数据库等产品，完全兼容ECS云服务器实例的镜像系统，可以自由地在普通ECS实例以及神龙云服务器实例间变配，从而更多元化地结合客户的业务场景进行资源构建。 所以阿里的“双十一”既有顶级的武学又有强悍的阵法、兵法加持，所以才能达到如此的高度，不久前阿里的平头哥芯片公司发布了无剑平台，这可能也代表着阿里的技术体系已经到了既刚且灵的境界。不仅如此纵阿里的技术体系最令人惊喜的是这些技术全部出于国产自研，甚至大部分已经开源，相信今后国产的底层软件也会在阿里等龙头的带领下迎来爆发。","link":"/2020/01/17/ali系统/"},{"title":"@Autowired @Resource @Qualifier的区别","text":"@Autowired @Resource @Qualifier的区别spring不但支持自己定义的@Autowired注解，还支持几个由JSR-250规范定义的注解，它们分别是@Resource、@PostConstruct以及@PreDestroy。 @Resource的作用相当于@Autowired，只不过@Autowired按byType自动注入，而@Resource默认按 byName自动注入罢了。@Resource有两个属性是比较重要的，分是name和type，Spring将@Resource注解的name属性解析为bean的名字，而type属性则解析为bean的类型。所以如果使用name属性，则使用byName的自动注入策略，而使用type属性时则使用byType自动注入策略。如果既不指定name也不指定type属性，这时将通过反射机制使用byName自动注入策略。 @Resource装配顺序 1. 如果同时指定了name和type，则从Spring上下文中找到唯一匹配的bean进行装配，找不到则抛出异常 2. 如果指定了name，则从上下文中查找名称（id）匹配的bean进行装配，找不到则抛出异常 3. 如果指定了type，则从上下文中找到类型匹配的唯一bean进行装配，找不到或者找到多个，都会抛出异常 4. 如果既没有指定name，又没有指定type，则自动按照byName方式进行装配；如果没有匹配，则回退为一个原始类型进行匹配，如果匹配则自动装配； @Autowired 与@Resource的区别： 1、 @Autowired与@Resource都可以用来装配bean. 都可以写在字段上,或写在setter方法上。 2、 @Autowired默认按类型装配（这个注解是属业spring的），默认情况下必须要求依赖对象必须存在，如果要允许null值，可以设置它的required属性为false，如：@Autowired(required=false) ，如果我们想使用名称装配可以结合@Qualifier注解进行使用，如下： 123@Autowired()@Qualifier(\"baseDao\")privateBaseDao baseDao; 3、@Resource（这个注解属于J2EE的），默认按照名称进行装配，名称可以通过name属性进行指定，如果没有指定name属性，当注解写在字段上时，默认取字段名进行安装名称查找，如果注解写在setter方法上默认取属性名进行装配。当找不到与名称匹配的bean时才按照类型进行装配。但是需要注意的是，如果name属性一旦指定，就只会按照名称进行装配。 12@Resource(name=\"baseDao\")privateBaseDao baseDao; 推荐使用：@Resource注解在字段上，这样就不用写setter方法了，并且这个注解是属于J2EE的，减少了与spring的耦合。这样代码看起就比较优雅。 spring @Qualifier注解 @Autowired是根据类型进行自动装配的。如果当Spring上下文中存在不止一个UserDao类型的bean时，就会抛出BeanCreationException异常;如果Spring上下文中不存在UserDao类型的bean，也会抛出BeanCreationException异常。我们可以使用@Qualifier配合@Autowired来解决这些问题。如下：①可能存在多个UserDao实例 123@Autowired @Qualifier(\"userServiceImpl\") public IUserService userService; 或者 1234@Autowired public void setUserDao(@Qualifier(\"userDao\") UserDao userDao) { this.userDao = userDao; } 这样Spring会找到id为userServiceImpl和userDao的bean进行装配。 ②可能不存在UserDao实例 12@Autowired(required = false) public IUserService userService 个人总结： @Autowired//默认按type注入 @Qualifier(“cusInfoService”)//一般作为@Autowired()的修饰用 @Resource(name=”cusInfoService”)//默认按name注入，可以通过name和type属性进行选择性注入不属于spring 一般@Autowired和@Qualifier一起用，@Resource单独用。 当然没有冲突的话@Autowired也可以单独用 常用注解–定义Bean的注解 @Controller@Controller(“Bean的名称”)定义控制层Bean,如Action @Service@Service(“Bean的名称”)定义业务层Bean @Repository@Repository(“Bean的名称”)定义DAO层Bean @Component定义Bean, 不好归类时使用. –自动装配Bean （选用一种注解就可以）@Autowired (Srping提供的)默认按类型匹配,自动装配(Srping提供的)，可以写在成员属性上,或写在setter方法上 @Autowired(required=true)一定要找到匹配的Bean，否则抛异常。 默认值就是true @Autowired@Qualifier(“bean的名字”)按名称装配Bean,与@Autowired组合使用，解决按类型匹配找到多个Bean问题。 @Resource JSR-250提供的默认按名称装配,当找不到名称匹配的bean再按类型装配.可以写在成员属性上,或写在setter方法上可以通过@Resource(name=”beanName”) 指定被注入的bean的名称, 要是未指定name属性, 默认使用成员属性的变量名,一般不用写name属性.@Resource(name=”beanName”)指定了name属性,按名称注入但没找到bean, 就不会再按类型装配了. @Inject 是JSR-330提供的按类型装配，功能比@Autowired少，没有使用的必要。 –定义Bean的作用域和生命过程@Scope(“prototype”)值有:singleton,prototype,session,request,session,globalSession @PostConstruct相当于init-method,使用在方法上，当Bean初始化时执行。 @PreDestroy相当于destory-method，使用在方法上，当Bean销毁时执行。 –声明式事务@Transactional@Autowired @Resource @Qualifier的区别实用理解：@Autowired @Resource 二选其一，看中哪个就用哪个。 简单理解：@Autowired 根据类型注入，@Resource 默认根据名字注入，其次按照类型搜索@Autowired @Qualifie(“userService”) 两个结合起来可以根据名字和类型注入 复杂理解：比如你有这么一个Bean@Service(“UserService”)public Class UserServiceImpl implements UserService｛｝;现在你想在UserController 里面使用这个UserServiceImplpublic Class UserController ｛@AutoWired //当使用这个注入的时候上面的 UserServiceImpl 只需要这样写 @Service，这样就会自动找到UserService这个类型以及他的子类型。UserServiceImpl 实现了UserService，所以能够找到它。不过这样有一个缺点，就是当UserService实现类有两个以上的时候，这个时候会找哪一个呢，这就造成了冲突，所以要用@AutoWire注入的时候要确保UserService只有一个实现类。@Resource 默认情况下是按照名称进行匹配，如果没有找到相同名称的Bean，则会按照类型进行匹配，有人可能会想了，这下好了，用这个是万能的了，不用管名字了，也不用管类型了，但这里还是有缺点。首先，根据这个注解的匹配效果可以看出，它进行了两次匹配，也就是说，如果你在UserService这个类上面这样写注解，@Service,它会怎么找呢，首先是找相同名字的，如果没有找到，再找相同类型的，而这里的@Service没有写名字，这个时候就进行了两次搜索，显然，速度就下降了许多。也许你还会问，这里的@Service本来就没有名字，肯定是直接进行类型搜索啊。其实不是这样的，UserServiceImpl 上面如果有@Service默认的名字 是这个userServiceImpl，注意看，就是把类名前面的大写变成小写，就是默认的Bean的名字了。 @Resource根据名字搜索是这样写@Resource(“userService”)，如果你写了这个名字叫userService，那么UserServiceImpl上面必须也是这个名字，不然还是会报错。 @Autowired @Qualifie(“userService”) 是直接按照名字进行搜索，也就是说，对于UserServiceImpl 上面@Service注解必须写名字，不写就会报错，而且名字必须是@Autowired @Qualifie(“userService”) 保持一致。如果@Service上面写了名字，而@Autowired @Qualifie() ，一样会报错。 private UserService userService; ｝ 说了这么多，可能你有些说晕了，那么怎么用这三个呢，要实际的工作是根据实际情况来使用的，通常使用AutoWire和@Resource多一些，bean的名字不用写，而UserServiceImpl上面能会这样写 @Service(“userService”)。这里的实际工作情况，到底是什么情况呢？说白了就是整个项目设计时候考虑的情况，如果你的架构设计师考虑的比较精细，要求比较严格，要求项目上线后的访问速度比较好，通常是考虑速度了。这个时候@AutoWire没有@Resource好用，因为@Resource可以根据名字来搜索，是这样写的@Resource(“userService”)。这个@Autowired @Qualifie(“userService”) 也可以用名字啊，为什么不用呢，原因很简单，这个有点长，不喜欢，增加工作量。因为根据名字搜索是最快的，就好像查数据库一样，根据Id查找最快。因为这里的名字与数据库里面的ID是一样的作用。这个时候，就要求你多写几个名字，工作量自然就增加了。而如果你不用注解，用xml文件的时候，对于注入Bean的时候要求写一个Id，xml文件时候的id就相当于这里的名字。 说了那么多没用，你能做的就是简单直接，什么最方便就用什么，你就直接用@Resource得了，如果你喜欢用@AutoWired也行，不用写名字。 通常情况一个Bean的注解写错了，会报下面这些错误，最为常见，No bean named ‘user’ is defined，这个表示没有找到被命名为user的Bean，通俗的说，就是名字为user的类型，以及它的子类型，出现这个错误的原因就是注入时候的类型名字为user，而搜索的时候找不到，也就是说可能那个搜索的类型，并没有命令为user，解决办法就是找到这个类型，去命令为user， 下面这个错误也常见，No qualifying bean of type [com.service.UserService] found for dependency:这个错误的原因就是类型上面没有加@Service这个注入，不仅仅是@Service，如果是其他层也会出现这个错误，这里我是以Service为例子说明，如果是DAO层就是没有加@Repository，Controller层，则是没有加@Controller。还有，如果你还是想再简单点，无论是DAO,Controller，Service三个层，都可以用这个注解，@Component，这个注解通用所有的Bean，这个时候你可能会说了，有通常的为什么用的人少呢，那是因为MVC这个分层的设计原则，用@Repository,@Service，@Controller，这个可以区别MVC原则中的DAO,Service，Controller。便于识别。 博客2：springautowiredqualifierbytypebyname 在使用Spring框架中@Autowired标签时默认情况下使用Java代码@Autowired @Autowired注释进行自动注入时，Spring 容器中匹配的候选 Bean 数目必须有且仅有一个。当找不到一个匹配的 Bean 时，Spring 容器将抛出 BeanCreationException 异常，并指出必须至少拥有一个匹配的 Bean。@Autowired 默认是按照byType进行注入的，如果发现找到多个bean，则，又按照byName方式比对，如果还有多个，则报出异常。例子：@Autowiredprivate ExamUserMapper examUserMapper; - ExamUserMapper是一个接口 spring先找类型为ExamUserMapper的bean 如果存在且唯一，则OK； 如果不唯一，在结果集里，寻找name为examUserMapper的bean。因为bean的name有唯一性，所以，到这里应该能确定是否存在满足要求的bean了 @Autowired也可以手动指定按照byName方式注入，使用@Qualifier标签，例如：@Autowired () @Qualifier ( “baseDao” ) Spring 允许我们通过Java代码@Qualifier@Qualifier注释指定注入 Bean 的名称，这样歧义就消除了，可以通过下面的方法解决异常。 Java代码@Qualifier(“XXX”)@Qualifier(“XXX”)中的 XX是 Bean 的名称，所以 @Autowired 和 @Qualifier 结合使用时，自动注入的策略就从 byType 转变成 byName 了。 @Autowired 可以对成员变量、方法以及构造函数进行注释，而 @Qualifier 的标注对象是成员变量、方法入参、构造函数入参。 Spring不但支持自己定义的@Autowired注解，还支持几个由JSR-250规范定义的注解，它们分别是@Resource、@PostConstruct以及@PreDestroy。 Java代码@Resource@Resource的作用相当于@Autowired，只不过@Autowired按byType自动注入，而@Resource默认按 byName自动注入罢了。@Resource有两个属性是比较重要的，分是name和type，Spring将@Resource注解的name属性解析为bean的名字，而type属性则解析为bean的类型。所以如果使用name属性，则使用byName的自动注入策略，而使用type属性时则使用byType自动注入策略。如果既不指定name也不指定type属性，这时将通过反射机制使用byName自动注入策略。 @Resource装配顺序 1. 如果同时指定了name和type，则从Spring上下文中找到唯一匹配的bean进行装配，找不到则抛出异常 2. 如果指定了name，则从上下文中查找名称（id）匹配的bean进行装配，找不到则抛出异常 3. 如果指定了type，则从上下文中找到类型匹配的唯一bean进行装配，找不到或者找到多个，都会抛出异常 4. 如果既没有指定name，又没有指定type，则自动按照byName方式进行装配；如果没有匹配，则回退为一个原始类型进行匹配，如果匹配则自动装配","link":"/2019/07/16/auto/"},{"title":"bazel","text":"bazel演变一: Carthage将整个工程拆分成多个仓库.每个仓库都是一个Framework.每个小组维护一个仓库.每个小组也只需关心这一个仓库.对其他仓库的依赖都用二进制依赖.大大加快编译速度.代表工具: Carthage.演变二: bazel不把每个库拆分成小仓库了.所有库都在一个Git仓库中.通过bazel拆分成多个package.每个package的编译缓存都是独立的.解决了编译过慢的问题.虽然在同一个大仓库里.但是每组各自维护自己的package.所以不会导致不同组之间的git冲突 bazel :解决了编译过慢问题解决了库之间依赖问题解决了多人协作导致的git冲突 安装安装过程请参考: http://bazel.io/docs/install.html使用工作区（workspace）所有的Bazel构建都是基于一个 工作区（workspace） 概念，它是文件系统中一个保存了全部源代码的目录，同时还将包含一些构建后的输出目录的符号链接（例如：bazel-bin和 bazel-out 等输出目录）。工作区目录可以随意放在哪里，但是工作区的根目录必须包含一个名为 WORKSPACE 的工作区配置文件。工作区配置文件可以是一个空文件，也可以包含引用外部构建输出所需的 依赖关系。在一个工作区内，可以根据需要共享多个项目。为了简单，我们先从只有一个项目的工作区开始介绍。先假设你已经有了一个项目，对应 ~/gitroot/my-project/ 目录。我们先创建一个空的 ~/gitroot/my-project/WORKSPACE 工作区配置文件，用于表示这是Bazel项目对应的根目录。创建自己的Build构建文件使用下面的命令创建一个简单的Java项目：$ # If you’re not already there, move to your workspace directory.$ cd ~/gitroot/my-project$ mkdir -p src/main/java/com/example$ cat &gt; src/main/java/com/example/ProjectRunner.java &lt;&lt;EOFpackage com.example; public class ProjectRunner {public static void main(String args[]) {Greeting.sayHi();} }EOF$ cat &gt; src/main/java/com/example/Greeting.java &lt;&lt;EOFpackage com.example; public class Greeting {public static void sayHi() {System.out.println(“Hi!”);} }EOF Bazel通过工作区中所有名为 BUILD 的文件来解析需要构建的项目信息，因此，我们需要先在 ~/gitroot/my-project 目录创建一个 BUILD 构建文件。下面是BUILD构建文件的内容： ~/gitroot/my-project/BUILD12345java_binary(name = \"my-runner\",srcs = glob([\"**/*.java\"]),main_class = \"com.example.ProjectRunner\",) BUILD文件采用类似Python的语法。虽然不能包含任意的Python语法，但是BUILD文件中的每个构建规则看起来都象是一个Python函数调用，而且你也可以用 “#” 开头来添加单行注释。java_binary 是一个构建规则。其中 name 对应一个构建目标的标识符，可用用它来向Bazel指定构建哪个项目。srcs 对应一个源文件列表，Bazel需要将这些源文件编译为二进制文件。其中 glob([“*/.java”]) 表示递归包含每个子目录中以每个 .java 为后缀名的文件。com.example.ProjectRunner 指定包含main方法的类。 现在可以用下面的命令构建这个Java程序了： 123456789$ cd ~/gitroot/my-project$ bazel build //:my-runnerINFO: Found 1 target...Target //:my-runner up-to-date:bazel-bin/my-runner.jarbazel-bin/my-runnerINFO: Elapsed time: 1.021s, Critical Path: 0.83s$ bazel-bin/my-runnerHi! 恭喜，你已经成功构建了第一个Bazel项目了！添加依赖关系对于小项目创建一个规则是可以的，但是随着项目的变大，则需要分别构建项目的不同的部件，最终再组装成产品。这种构建方式可以避免因为局部细小的修改儿导致重现构建整个应用，同时不同的构建步骤可以很好地并发执行以提高构建效率。我们现在将一个项目拆分为两个部分独立构建，同时设置它们之间的依赖关系。基于上面的例子，我们重写了BUILD构建文件： 1234567891011java_binary(name = \"my-other-runner\",srcs = [\"src/main/java/com/example/ProjectRunner.java\"],main_class = \"com.example.ProjectRunner\",deps = [\":greeter\"],)java_library(name = \"greeter\",srcs = [\"src/main/java/com/example/Greeting.java\"],) 虽然源文件是一样的，但是现在Bazel将采用不同的方式来构建：首先是构建 greeter库，然后是构建 my-other-runner。可以在构建成功后立刻运行 //:my-other-runner： 123456789$ bazel run //:my-other-runnerINFO: Found 1 target...Target //:my-other-runner up-to-date:bazel-bin/my-other-runner.jarbazel-bin/my-other-runnerINFO: Elapsed time: 2.454s, Critical Path: 1.58sINFO: Running command line: bazel-bin/my-other-runnerHi! 现在如果你改动ProjectRunner.java代码并重新构建my-other-runner目标，Greeting.java文件因为没有变化而不会重现编译。使用多个包（Packages）对于更大的项目，我们通常需要将它们拆分到多个目录中。你可以用类似//path/to/directory:target-name的名字引用在其他BUILD文件定义的目标。假设src/main/java/com/example/有一个cmdline/子目录，包含下面的文件： 123456789101112$ mkdir -p src/main/java/com/example/cmdline$ cat &gt; src/main/java/com/example/cmdline/Runner.java &lt;&lt;EOFpackage com.example.cmdline;import com.example.Greeting;public class Runner {public static void main(String args[]) {Greeting.sayHi();}}EOF Runner.java依赖com.example.Greeting，因此我们需要在src/main/java/com/example/cmdline/BUILD构建文件中添加相应的依赖规则： ~/gitroot/my-project/src/main/java/com/example/cmdline/BUILD123456java_binary(name = \"runner\",srcs = [\"Runner.java\"],main_class = \"com.example.cmdline.Runner\",deps = [\"//:greeter\"]) 然而，默认情况下构建目标都是 私有 的。也就是说，我们只能在同一个BUILD文件中被引用。这可以避免将很多实现的细节暴漏给公共的接口，但是也意味着我们需要手工允许runner所依赖的//:greeter目标。就是类似下面这个在构建runner目标时遇到的错误： 123456$ bazel build //src/main/java/com/example/cmdline:runnerERROR: /home/user/gitroot/my-project/src/main/java/com/example/cmdline/BUILD:2:1:Target '//:greeter' is not visible from target '//src/main/java/com/example/cmdline:runner'.Check the visibility declaration of the former target if you think the dependency is legitimate.ERROR: Analysis of target '//src/main/java/com/example/cmdline:runner' failed; build aborted.INFO: Elapsed time: 0.091s 可用通过在BUILD文件增加visibility = level属性来改变目标的可间范围。下面是通过在~/gitroot/my-project/BUILD文件增加可见规则，来改变greeter目标的可见范围： 12345java_library(name = \"greeter\",srcs = [\"src/main/java/com/example/Greeting.java\"],visibility = [\"//src/main/java/com/example/cmdline:__pkg__\"],) 这个规则表示//:greeter目标对于//src/main/java/com/example/cmdline包是可见的。现在我们可以重新构建runner目标程序： 12345678$ bazel run //src/main/java/com/example/cmdline:runnerINFO: Found 1 target...Target //src/main/java/com/example/cmdline:runner up-to-date:bazel-bin/src/main/java/com/example/cmdline/runner.jarbazel-bin/src/main/java/com/example/cmdline/runnerINFO: Elapsed time: 1.576s, Critical Path: 0.81sINFO: Running command line: bazel-bin/src/main/java/com/example/cmdline/runnerHi! 参考文档 中有可见性配置说明。部署如果你查看 bazel-bin/src/main/java/com/example/cmdline/runner.jar 的内容，可以看到里面只包含了Runner.class，并没有保护所依赖的Greeting.class： 1234567$ jar tf bazel-bin/src/main/java/com/example/cmdline/runner.jarMETA-INF/META-INF/MANIFEST.MFcom/com/example/com/example/cmdline/com/example/cmdline/Runner.class 这只能在本机正常工作（因为Bazel的runner脚本已经将greeter jar添加到了classpath），但是如果将runner.jar单独复制到另一台机器上讲不能正常运行。如果想要构建可用于部署发布的自包含所有依赖的目标，可以构建runner_deploy.jar目标（类似_deploy.jar以_deploy为后缀的名字对应可部署目标）。 12345$ bazel build //src/main/java/com/example/cmdline:runner_deploy.jarINFO: Found 1 target...Target //src/main/java/com/example/cmdline:runner_deploy.jar up-to-date:bazel-bin/src/main/java/com/example/cmdline/runner_deploy.jarINFO: Elapsed time: 1.700s, Critical Path: 0.23s win环境下 Bazel 离线安装教程对于bazel谷歌的开发脚手架，发现用windows系统下powershell下载速度特别慢，在刨坑过程中发现了一种捷径，就是离线安装，接下来将简单介绍离线安装过程1、安装chococmd安装指令： -NoProfile -InputFormat None -ExecutionPolicy Bypass -Command \"iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\" && SET \"PATH1234567891011121314151617181920powershell安装指令： ```Set-ExecutionPolicy Bypass -Scope Process -Force; iex ((New-Object System.Net.WebClient).DownloadString(&apos;https://chocolatey.org/install.ps1&apos;))``` 特别说明：一定注意都是在管理员用户权限下打开cmd或者是powershell 这里推荐win10系统下使用powershell，如图所示，右键开始菜单打开管理员用户下的powershell： 2、通过powershell安装bazel 2.1、输入指令： `choco install bazel` &lt;hr/&gt; 离线安装 从官网上下载目前最新的bazel压缩包 先运行choco install bazel指令 如图所示等待选择阶段，这个时候不要打y 或者 n，让命令行停留于此: ![upload successful](/images/pasted-9.png) 在命令行停留之际，修改C:\\ProgramData\\chocolatey\\lib\\bazel\\tools目录下的params.txt文件内容，将网络版本指定地址修改为本地地址 修改之前的： ``` https://github.com/bazelbuild/bazel/releases/download/0.12.0/bazel-0.12.0-windows-x86_64.zip// 此处是文件哈希，禁止修改，原来系统给予是怎样的就是怎样的。86f84e2c870ed14e4d2e599c309614298b9e08a049657e860d218d56873111bc 修改之后的： 【由于我这里直接下载到桌面上，所以指向桌面地址，如果下载到其他位置，请将绝对地址替换，请勿复制粘贴此段】 1234C:/Users/ke_li/Desktop/bazel-0.12.0-windows-x86_64.zip// 此处是文件哈希，禁止修改，仅修改了上面文件目录，下面未修改，注意保留哈希的意思86f84e2c870ed14e4d2e599c309614298b9e08a049657e860d218d56873111bc 接下来输入y，回车进行下一阶段，等待一段时间后，结果： 可调试bazel，在命令行输入bazel 检查安装是否成功，如图所示即为成功安装","link":"/2019/07/19/bazel/"},{"title":"beanutils","text":"BeanUtil工具类的使用BeanUtils的使用 1.commons-beanutils的介绍 commons-beanutils是Apache组织下的一个基础的开源库，它提供了对Java反射和内省的API的包装，依赖内省，其主要目的是利用反射机制对JavaBean的属性进行处理。我们知道一个JavaBean通常包含了大量的属性，很多情况下，对JavaBean的处理导致了大量的get/set代码堆积，增加了代码长度和阅读代码的难度，现在有了BeanUtils，我们对JavaBean属性的处理就方便很多。 2.BeanUtils的使用 BeanUtils是commons-beanutils包下的一个工具类，如果想在我们的项目中使用这个类需要导入以下两个jar包： l commons-beanutils.jar l commons-logging.jar 下面我们就来练习如何使用BeanUtils，案例详情请参考BeanUtils使用案例详解，点击此处下载案例源代码，具体如下： (1)创建一个web应用，Example5，将上面说到的两个jar包拷贝的WEB-INF/lib下； (2)在该应用下的src目录下新建一个Class类，名称为Person，主要代码如例1-1所示： 例1-1 Person.java 1234567891011121314151617181920212223public class Person { private Stringname; private int age; private Stringgender; private boolean bool; 此处省略以上四个属性的get/set方法@Override public String toString() { return \"Person [name=\" + name + \", age=\" + age + \", gender=\" + gender + \"]\"; }} 例1-1中，定义了四个成员变量，并重写了toString()方法。 (3)在src目录下新建一个Class类，名称为Demo，在该类中定义了一个单元测试方法，主要代码如例1-2所示： 例1-2 Demo.java 12345678910111213141516171819202122232425public class Demo { @Test public void fun1() throws Exception{ String className=\"cn.itcast.domain.Person\"; Class clazz=Class.forName(className); Object bean=clazz.newInstance(); BeanUtils.setProperty(bean, \"name\", \"张三\"); BeanUtils.setProperty(bean, \"age\", \"23\"); BeanUtils.setProperty(bean, \"gender\", \"男\"); BeanUtils.setProperty(bean, \"xxx\", \"XXX\"); System.out.println(bean); }} 例1-2中，利用反射获得Person类的对象，然后使用BeanUtils类的静态方法setProperty(Object bean,String name,Object value)为指定bean的指定属性赋值。该方法的第一参数是javaBean对象，第二个参数是javaBean的属性，第三个参数是属性的值。 (4)运行Demo类的单元测试方法fun1()，控制台打印结果如图1-1所示： 图1-1 控制台打印结果 图1-1中，Person信息的打印格式是我们再Person类的toString()方法中设置的。 (5)使用BeanUtils的getProperty(Object bean,String name)方法获取指定bean的指定属性值，如例1-3所示： 1234567891011121314151617181920212223242526272829public class Demo { @Test public void fun1() throws Exception{ String className=\"cn.itcast.domain.Person\"; Class clazz=Class.forName(className); Object bean=clazz.newInstance(); BeanUtils.setProperty(bean, \"name\", \"张三\"); BeanUtils.setProperty(bean, \"age\", \"23\"); BeanUtils.setProperty(bean, \"gender\", \"男\"); BeanUtils.setProperty(bean, \"xxx\", \"XXX\"); System.out.println(bean);String age = BeanUtils.getProperty(bean, \"age\"); System.out.println(age); }} (6)测试fun1方法，控制台打印结果如图1-2所示： 图1-2 控制台打印结果 以上是通过BeanUtils类的setProperty()和getProperty()方法对javaBean属性的设置和获取；开发中可能会有这样的需求：将表单提交过来的请求参数封装在一个javaBean中，这个时候我们再使用BeanUtils的setProperty()和getProperty()方法就会很麻烦；因此BeanUtils又为我们提供了一个静态方法populate(Object bean,Map properties)，其中第二个参数就是封装请求参数的Map，我们可以通过request.getParameterMap()方法获取一个封装了所有请求参数的Map对象。 下面通过一个例子来了解BeanUtils类的populate(Object bean,Map properties)方法，如下所示： (7)在Example5中创建一个javaBean类，User，主要代码如例1-4所示： 例1-4 User.java 1234567891011121314151617public class User { private Stringusername; private String password; 此处省略User类的成员变量的get/set方法@Override public String toString() { return \"User [username=\" + username + \", password=\" + password + \"]\"; }} (8)在Demo类中再定义一个单元测试方法fun2，主要代码如例1-5所示： 例1-5 fun2()方法 1234567891011121314151617@Testpublic void fun2() throws Exception { Map&lt;String,String&gt; map = new HashMap&lt;String,String&gt;(); map.put(\"username\", \"zhangSan\"); map.put(\"password\", \"123\"); User user = new User(); BeanUtils.populate(user, map); System.out.println(user);} 例1-5中，将map里面的数据封装到javaBean中，这里有一个要求：Map中的key值要与JavaBean中的属性名称保持一致，否则封装不进去。 (9)测试fun2方法，控制台打印结果如图1-3所示： 图1-3 控制台打印结果 现在我们再对BeanUtils进行封装，封装成一个工具类，我们之前也封装过类似的一个工具类，该工具类中提供了一个方法用来获取不重复的32位长度的大写字符串，如下所示： (10)在Example5中创建一个工具类，名称为CommonUtils，在该类中定义一个方法，用来将map中的数据封装到javaBean中，主要代码如例1-6所示： 例1-6 CommonUtils.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class CommonUtils { /** * 生成不重复的32位长的大写字符串 * @return */ public static String uuid() { return UUID.randomUUID().toString().replace(\"-\",\"\").toUpperCase(); } /** * 把map转换成指定类型的javaBean对象 * @param map * @param clazz * @return */ public static &lt;T&gt; T toBean(Map map, Class&lt;T&gt; clazz) { try { /* * 1. 创建指定类型的javabean对象 */ T bean = clazz.newInstance(); /* * 2. 把数据封装到javabean中 */ BeanUtils.populate(bean, map); /* * 返回javabean对象 */ return bean; } catch(Exception e) { throw new RuntimeException(e); } }} 例1-6中，CommonUtils定义了一个静态的泛型方法：toBean(Map map,Class clazz)，根据传递的参数来判断将map中的数据封装到哪个javaBean中。当中来利用了反射获得指定类型的javaBean对象，然后再调用BeanUtils类的populate()方法。 (11)在Demo类中再定义一个单元测试方法fun3，主要代码如例1-7所示： 例1-7 fun3()方法 123456789101112131415161718192021@Testpublic void fun3() { Map&lt;String,String&gt; map = new HashMap&lt;String,String&gt;(); map.put(\"username\", \"lisi\"); map.put(\"password\", \"123\"); /* * request.getParameterMap(); */ User user = CommonUtils.toBean(map, User.class); System.out.println(user);} 例1-7中，将map中的数据使用CommonUtils类的toBean()方法封装到user中，然后返回一个user对象。 (12)运行fun3()方法，控制台打印结果如图1-4所示： 图1-4 控制台打印结果 需要注意的是，在使用BeanUtils类的setProperty()、getProperty()和populate()方法时都抛出了异常，我们制作的帮助类需要对异常进行处理，这样在调用这个帮助类的这个方法时就不用再对异常进行处理。另外，在调用BeanUtils的setProperty()方法时，如果设置的属性不存在或者没有给javaBean的某个属性赋值，该方法不会抛出异常。","link":"/2019/09/12/beanutils/"},{"title":"brew安装问题","text":"[TOCM] [TOC] github下载Homebrew for macOS 替换源把下载的install文件里的 BREW_REPO 替换掉BREW_REPO = “https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git”.freeze 然后install./install 问题解决brew 安装出现Checksum mismatch解决方法用brew 安装APP出现Error: Checksum mismatch.说明下载的文件和期望的hashCode对不上，删掉对应的文件就行了删除对应文件，然后重新执行brew安装脚本rm -rf /Users/tongqiao/Library/Caches/Homebrew/portable-ruby-2.6.3.mavericks.bottle.tar.gz 替换国内源为下载源123456789101112131415// 执行下面这句命令，更换为中科院的镜像：git clone git://mirrors.ustc.edu.cn/homebrew-core.git/ /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core --depth=1// 把homebrew-core的镜像地址也设为中科院的国内镜像cd \"$(brew --repo)\" git remote set-url origin https://mirrors.ustc.edu.cn/brew.gitcd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git// 更新brew update","link":"/2019/12/04/brew安装问题/"},{"title":"casb server","text":"CASB 后台管理（管理端）Java项目结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748-src - java - com - enlink - admin - aop - config - druid - constraint - core - dao - feature - pojos - service - timer - util - excel - web - controller - apps - auditmanage - auth - chargingsystem - external - firewall - ipsec - netconfig - rule - s2s - signaturemanage - sms - sysconfig - usermanager - BaseController - filter - ElinkAdminBackApplication -libs - cn - com - org -resources - backup - mapper - sql - application.properties - generatorConfig.xml - logback-spring.xml - mybatis-generator.xml","link":"/2019/07/15/casb-server/"},{"title":"command num of  file  for linux","text":"linux 下查看文件个数及大小查看当前目录下的文件数量： ls -l |grep “^-“|wc -l 或 find ./company -type f | wc -l 查看某文件夹下文件的个数，包括子文件夹里的。 ls -lR|grep “^-“|wc -l 查看某文件夹下文件夹的个数，包括子文件夹里的。 ls -lR|grep “^d”|wc -l 说明： ls -l 长列表输出该目录下文件信息(注意这里的文件，不同于一般的文件，可能是目录、链接、设备文件等) grep “^-“ 这里将长列表输出信息过滤一部分，只保留一般文件，如果只保留目录就是 ^d wc -l 统计输出信息的行数，因为已经过滤得只剩一般文件了，所以统计结果就是一般文件信息的行数，又由于 一行信息对应一个文件，所以也就是文件的个数。 linux查看文件大小： du -sh 查看当前文件夹大小 du -sh * | sort -n 统计当前文件夹(目录)大小，并按文件大小排序 du -sk filename 查看指定文件大小","link":"/2019/07/18/command-num-of-file-for-linux/"},{"title":"crontab another form","text":"@hourly /usr/local/www/awstats/cgi-bin/awstats.sh使用 @hourly 对应的是 0 * * * *, 还有下述可以使用: 复制代码string meaning—— ——-@reboot Run once, at startup.@yearly Run once a year, “0 0 1 1 *”.@annually (same as @yearly)@monthly Run once a month, “0 0 1 * *”.@weekly Run once a week, “0 0 * * 0”.@daily Run once a day, “0 0 * * *”.@midnight (same as @daily)@hourly Run once an hour, “0 * * * *”. 特別是看到 @reboot, 所以写 rc.local 的東西, 也可以使用 @reboot 寫在 crontab 里面。 1.每分钟定时执行一次规则：每1分钟执行： /1 * * * *或者 * * * * 每5分钟执行： */5 * * * * 2.每小时定时执行一次规则：每小时执行： 0 * * * *或者0 */1 * * * 每天上午7点执行：0 7 * * * 每天上午7点10分执行：10 7 * * * 3.每天定时执行一次规则：每天执行 0 0 * * * 4.每周定时执行一次规则：每周执行 0 0 * * 0 5.每月定时执行一次规则：每月执行 0 0 1 * * 6.每年定时执行一次规则：每年执行 0 0 1 1 * 7.其他例子5 * * * * 指定每小时的第5分钟执行一次ls命令30 5 * * * ls 指定每天的 5:30 执行ls命令30 7 8 * * ls 指定每月8号的7：30分执行ls命令30 5 8 6 * ls 指定每年的6月8日5：30执行ls命令30 6 * * 0 ls 指定每星期日的6:30执行ls命令[注：0表示星期天，1表示星期1，以此类推，也可以用英文来表示，sun表示星期天，mon表示星期一等。]30 3 10,20 * * ls 每月10号及20号的3：30执行ls命令[注：“，”用来连接多个不连续的时段]25 8-11 * * * ls 每天8-11点的第25分钟执行ls命令[注：“-”用来连接连续的时段]*/15 * * * * ls 每15分钟执行一次ls命令 [即每个小时的第0 15 30 45 60分钟执行ls命令 ]30 6 */10 * * ls 每个月中，每隔10天6:30执行一次ls命令[即每月的1、11、21、31日是的6：30执行一次ls命令。 ]","link":"/2019/07/20/crontab-another-form/"},{"title":"computer and network","text":"计算机网络基本知识汇总 概述OSI分层（7层）物理层、数据链路层、网络层、运输层、会话层、表示层、应用层TCP/IP分层（4层）网络接口层、网络层、运输层、应用层五层协议（5层）物理层、数据链路层、网络层、运输层、应用层五层结构的概述应用层：通过应用进程间的交互来完成特定网络应用数据：报文协议：HTTP, SMTP(邮件), FTP(文件传送)运输层：向两个主机进程之间的通信提供通用的数据传输服务。数据：TCP:报文段，UDP:用户数据报协议：TCP, UDP网络层：为分组交换网上的不同主机提供通信服务数据：包或IP数据报协议：IP数据链路层：数据：帧物理层：数据：比特ARP地址解析协议:用来获取目标IP地址所对应的MAC地址的各层协议应用层域名系统DNS例：某用户通过主机A浏览西安交大的主页 www.xjtu.edu.cn A向本地域名服务器DNS查询 如果DNS上有www.xjtu.edu.cn的记录，就立即返回IP地址给主机A 如果DNS上没有该域名记录，则DNS向根域名服务器发出查询请求 根域名服务器把负责cn域的顶级域名服务器B的IP地址告诉DNS DNS向B查询获得二级域名服务器C的IP地址，最终迭代查询到www.xjtu.edu.cn的ip直接返回DNSHTTP请求报文 常用的 HTTP 请求方法有GET、POST、HEAD、PUT、DELETE、OPTIONS、TRACE、CONNECT;GET：当客户端要从服务器中读取某个资源时，使用GET 方法。GET 方法要求服务器将URL 定位的资源放在响应报文的部分，回送给客户端，即向服务器请求某个资源。使用GET 方法时，请求参数和对应的值附加在 URL 后面，利用一个问号(“?”)代表URL 的结尾与请求参数的开始，传递参数长度受限制。例如，/index.jsp?id=100&amp;op=bind。POST：当客户端给服务器提供信息较多时可以使用POST 方法，POST 方法向服务器提交数据，比如完成表单数据的提交，将数据提交给服务器处理。GET 一般用于获取/查询资源信息，POST 会附带用户数据，一般用于更新资源信息。POST 方法将请求参数封装在HTTP 请求数据中，以名称/值的形式出现，可以传输大量数据;请求头部：请求头部由关键字/值对组成，每行一对，关键字和值用英文冒号“:”分隔。请求头部通知服务器有关于客户端请求的信息，典型的请求头有：User-Agent：产生请求的浏览器类型;Accept：客户端可识别的响应内容类型列表;星号 “ * ” 用于按范围将类型分组，用 “ / ” 指示可接受全部类型，用“ type/* ”指示可接受 type 类型的所有子类型;Accept-Language：客户端可接受的自然语言;Accept-Encoding：客户端可接受的编码压缩格式;Accept-Charset：可接受的应答的字符集;Host：请求的主机名，允许多个域名同处一个IP 地址，即虚拟主机;connection：连接方式(close 或 keepalive);Cookie：存储于客户端扩展字段，向同一域名的服务端发送属于该域的cookie;GET /search?hl=zh-CN&amp;source=hp&amp;q=domety&amp;aq=f&amp;oq= HTTP/1.1Accept: image/gif, image/x-xbitmap, image/jpeg, image/pjpeg, application/vnd.ms-excel, application/vnd.ms-powerpoint,application/msword, application/x-silverlight, application/x-shockwave-flash, /Referer: http://www.google.cn/Accept-Language: zh-cnAccept-Encoding: gzip, deflateUser-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 2.0.50727; TheWorld)Host: www.google.cnConnection: Keep-AliveCookie: PREF=ID=80a06da87be9ae3c:U=f7167333e2c3b714:NW=1:TM=1261551909:LM=1261551917:S=ybYcq2wpfefs4V9g;NID=31=ojj8d-IygaEtSxLgaJmqSjVhCspkviJrB6omjamNrSm8lZhKy_yMfO2M4QMRKcH1g0iQv9u-2hfBW7bUFwVh7pGaRUb0RnHcJU37y-FxlRugatx63JLv7CWMD6UB_O_r 响应报文 状态码由三位数字组成，第一位数字表示响应的类型，常用的状态码有五大类如下所示：1xx：表示服务器已接收了客户端请求，客户端可继续发送请求;2xx：表示服务器已成功接收到请求并进行处理;3xx：表示服务器要求客户端重定向;4xx：表示客户端的请求有非法内容;5xx：表示服务器未能正常处理客户端的请求而出现意外错误;200 OK：表示客户端请求成功;400 Bad Request：表示客户端请求有语法错误，不能被服务器所理解;401 Unauthonzed：表示请求未经授权，该状态代码必须与 WWW-Authenticate 报头域一起使用;403 Forbidden：表示服务器收到请求，但是拒绝提供服务，通常会在响应正文中给出不提供服务的原因;404 Not Found：请求的资源不存在，例如，输入了错误的URL;500 Internal ServerError：表示服务器发生不可预期的错误，导致无法完成客户端的请求;503 Service Unavailable：表示服务器当前不能够处理客户端的请求，在一段时间之后，服务器可能会恢复正常;响应头部：响应头可能包括： Location：Location响应报头域用于重定向接受者到一个新的位置。例如：客户端所请求的页面已不存在原先的位置，为了让客户端重定向到这个页面新的位置，服务器端可以发回Location响应报头后使用重定向语句，让客户端去访问新的域名所对应的服务器上的资源; Server：Server 响应报头域包含了服务器用来处理请求的软件信息及其版本。它和 User-Agent 请求报头域是相对应的，前者发送服务器端软件的信息，后者发送客户端软件(浏览器)和操作系统的信息。 Vary：指示不可缓存的请求头列表; Connection：连接方式;对于请求来说：close(告诉WEB 服务器或者代理服务器，在完成本次请求的响应后，断开连接，不等待本次连接的后续请求了)。keepalive(告诉WEB服务器或者代理服务器，在完成本次请求的响应后，保持连接，等待本次连接的后续请求);对于响应来说：close(连接已经关闭); keepalive(连接保持着，在等待本次连接的后续请求); Keep-Alive：如果浏览器请求保持连接，则该头部表明希望WEB 服务器保持连接多长时间(秒);例如：Keep-Alive：300;WWW-Authenticate：WWW-Authenticate响应报头域必须被包含在401 (未授权的)响应消息中，这个报头域和前面讲到的Authorization 请求报头域是相关的，当客户端收到 401 响应消息，就要决定是否请求服务器对其进行验证。如果要求服务器对其进行验证，就可以发送一个包含了Authorization 报头域的请求;问题： Http1.1与Http1.0的区别http1.0使用非持久连接（短连接），而http1.1默认是持久连接（长连接），当然也可以配置成非持久连接。Cookie和Session的作用和工作原理FTP文件传送协议运输层使用UDP和TCP协议的各种应用和应用层协议应用应用层协议运输层协议名字转换DNS(域名系统)UDP文件传送TFTP(简单文件传送协议)UDP路由器选择协议RIP(路由信息协议)UDPIP地址配置DHCP(动态主机配置协议)UDP网络管理SNMP(简单网络管理协议)UDP远程服务器NFS(网络文件系统)UDP多播IGMP(网际组管理协议)UDP电子邮件SMTP(简单邮件传送协议)TCP远程终端TELNET(远程终端协议)TCP万维网HTTP(超文本传送协议)TCP文件传送FTP(文件传送协议)TCP 端口TCP和UDP都需要有源端口和目的端口（端口：用16位来表示,即一个主机共有65536个端口.序号小于256的端口称为通用端口,如FTP是21端口,WWW是80端口等.端口用来标识一个服务或应用.一台主机可以同时提供多个服务和建立多个连接.端口(port)就是传输层的应用程序接口.应用层的各个进程是通过相应的端口才能与运输实体进行交互.服务器一般都是通过人们所熟知的端口号来识别的）服务端常用的熟知端口应用程序FTPTELNETSMTPDNSTFTPHTTPSNMPSNMP(trap)熟知端口 登记端口 102449151客户端端口号由客户进程动态选择。数值范围 4915265535UDP特点无连接的（发送数据之前不需要建立连接，因此减少了开销和发送数据之前的时延）尽最大努力交付（不保证可靠支付，因此主机不需要维持复杂的连接状态表）面向报文的（UDP对应用层交下来的报文，添加完首部后就直接交付IP层。如果太长就会分 片）UDP没有拥塞控制UDP支持一对一、一对多、多对一和多对多的交互通信UDP的首部开销小（只有8个字节，TCP有20个字节）UDP报文 源端口：2字节 = 16bit = 0 ~ 65535 目的端口：2字节 长度：2字节 检验和：2字节如果接受方UDP发现收到的报文中的目的端口号不正确（不存在对应端口号的应用进程），就会丢弃报文，并有网际控制报文协议ICMP（ping某个地址就是用的ICMP）发送“端口不可达”差错报文给发送方。UDP用户数据报首部检验和计算时会在UDP用户数据报前增加12个字节的伪首部。 TCP特点面向连接的运输层协议。点对点（一对一）通信。可靠交付。全双工通信（TCP连接的两端都设有发送缓存和接收缓存，用来临时存放双向通信的数据）。面向字节流。TCP与UDP在发送报文时所采用的方式完全不同。TCP具体发送的报文由接收方给出的窗口值和当前网络拥塞的程度来决定一个报文段包含多少字节。而UDP发送的报文长度由应用进程给出。TCP可靠传输工作原理TCP连接的端点叫做套接字(socket)或插口。套接字socket = (IP地址：端口号)停止等待协议 每发送完一个分组就设置一个超时计时器。 注意： 必须暂时保存已发送的分组的副本 分组和确认分组都必须编号 超市计时器设置的重传时间比数据在分组传输的平均时间更长一些确认丢失和确认迟到 如果接收方接收到数据发送确认没有被发送方接收到，那么发送方超时后会重新发送分组，并且接收方收到重复的分组会丢弃并重传确认。如果接收方收到的确认是已经接受过的，那么会无视这个确认。缺点停止等待协议（自动重传ARQ）虽然简单，但是信道利用率低。 信道利用率U = TD / (TD + RTT + TA)连续ARQ协议和滑动窗口协议 TCP报文格式 源端口和目的端口 各占2字节序号 4字节确认号 4字节期望收到对方下一个报文的第一个数据字节的序号数据偏移 4位保留 6字节紧急URG 当URG=1表示紧急指针有效确认ACK推送PSH复位RST 当RST = 1时，释放连接并重新建立连接同步SYN 当SYN = 1 ACK = 0时，表明这是一个连接请求报文段。终止FIN FIN = 1，请求释放连接。窗口检验和紧急指针选项TCP的三次握手 客户端TCP向服务端TCP发送一个特殊的TCP报文段，不包含应用层数据，报文中SYN=1，设置一个初始号client_isn,记录在报文段的序列号seq中。SYN报文段到达服务器后，为该TCP链接分配缓存和变量，并向客户端发送允许链接的报文段。其中，SYN = 1， ACK = client_isn+1，seq = server_isn;客户端收到允许连接的报文后，客户端也给连接分配缓存和变量，客户端向服务端发送一个报文段，其中ACK = server_isn+1，SYN = 0，并且由于连接已经建立所以现在可以携带应用层数据。TCP四次挥手 客户端发送连接释放报文段，报文中FIN = 1, seq = u; 服务端接收到连接释放报文后发出确认报文，其中ACK = 1; seq = v; ack = u + 1; 服务端在发送完数据后，发送连接释放报文FIN = 1, seq = w, ack = u + 1;并停止向客户端发送数据。 客户端收到连接释放报文后，发送确认报文， ACK = 1; seq = u + 1; ack = w + 1;并且进入等待2MSL，防止服务端没有接收到确认报文，重传报文。并且使连接产生的报文都消失。TCP协议的连接是全双工连接，一个TCP连接存在双向的读写通道。简单说来是 “先关读，后关写”，一共需要四个阶段。以客户机发起关闭连接为例： 服务器读通道关闭 客户机写通道关闭 客户机读通道关闭 服务器写通道关闭TCP拥塞控制拥塞控制和流量控制的区别流量控制针对的是点对点之间的（发送方和接收方）之间的速度匹配服务，因为接收方的应用程序读取的速度不一定很迅速，而接收方的缓存是有限的，就需要避免发送的速度过快而导致的问题。拥塞控制是由于网络中的路由和链路传输速度限制，要避免网络的过载和进行的控制。拥塞控制算法拥塞控制算法主要包含了三个部分：慢启动、拥塞避免和快速回复 慢启动慢开始算法的思路就是，不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。一般一开始为1个MSS，之后翻倍这样来增加，呈指数增长。其中1、慢启动过程有一个阈值ssthresh，一旦到达阈值就进入拥塞避免模式。这是第一种离开结束慢启动的方式2、如果收到了一个丢包提示，就将cwnd设为1并且重新开始慢启动过程，这时要把阈值ssthresh设为当前cwnd值的一半。3、如果收到了三次冗余的ACK，就执行一次快速重传并且进入快速恢复状态，这是最后一种结束慢启动的过程。拥塞避免进入拥塞避免说明cwnd值大约是上一次遇到拥塞是的一半，这时候不能翻倍，而是将cwnd的值每次增加一个MSS。结束的过程有两种可能：1、当出现超时时，将cwnd值设为1个MSS，并且将ssthresh阈值设为当前cwnd值的一半。2、当收到三个冗余ACK时，将ssthresh阈值设为当前cwnd值的一半，并且将cwnd值设为当前cwnd值的一半加3，即ssthresh阈值加3，并且进入快速恢复状态。快速恢复快速恢复就是指进入快速恢复前的一系列操作，即将ssthresh阈值设为当前cwnd值的一半，并且将cwnd值设为当前cwnd值的一半加3，即ssthresh阈值加3，之后进入拥塞避免状态，即每次cwnd的值加1个MSS。网络层协议地址解析协议 ARP网际控制报文协议 ICMP网际组管理协议 IGMPIPIP地址分类：A类:1.0.0.0126.255.255.255,默认子网掩码/8,即255.0.0.0 (其中127.0.0.0127.255.255.255为环回地址,用于本地环回测试等用途)；B类:128.0.0.0191.255.255.255,默认子网掩码/16,即255.255.0.0；C类:192.0.0.0223.255.255.255,默认子网掩码/24,即255.255.255.0；D类:224.0.0.0239.255.255.255,一般于用组播E类:240.0.0.0255.255.255.255(其中255.255.255.255为全网广播地址),E类地址一般用于研究用途","link":"/2019/07/17/computer-and-network/"},{"title":"mac转换dmg到iso","text":"hdiutil convert ~/Downloads/OSX109.dmg -format UDTO -o ~/Downloads/OSX109.iso","link":"/2019/11/29/dmg转iso/"},{"title":"custom annotation","text":"自定义注解详细介绍 1 注解的概念1.1 注解的官方定义 首先看看官方对注解的描述： An annotation is a form of metadata, that can be added to Java source code. Classes, methods, variables, parameters and packages may be annotated. Annotations have no direct effect on the operation of the code they annotate. 翻译： 注解是一种能被添加到java代码中的元数据，类、方法、变量、参数和包都可以用注解来修饰。注解对于它所修饰的代码并没有直接的影响。 通过官方描述得出以下结论： 注解是一种元数据形式。即注解是属于java的一种数据类型，和类、接口、数组、枚举类似。注解用来修饰，类、方法、变量、参数、包。注解不会对所修饰的代码产生直接的影响。 1.2 注解的使用范围继续看看官方对它的使用范围的描述： Annotations have a number of uses, among them:Information for the complier - Annotations can be used by the compiler to detect errors or suppress warnings.Compiler-time and deployment-time processing - Software tools can process annotation information to generate code, XML files, and so forth.Runtime processing - Some annotations are available to be examined at runtime. 翻译： 注解又许多用法，其中有：为编译器提供信息 - 注解能被编译器检测到错误或抑制警告。编译时和部署时的处理 - 软件工具能处理注解信息从而生成代码，XML文件等等。运行时的处理 - 有些注解在运行时能被检测到。 2 如何自定义注解基于上一节，已对注解有了一个基本的认识：注解其实就是一种标记，可以在程序代码中的关键节点（类、方法、变量、参数、包）上打上这些标记，然后程序在编译时或运行时可以检测到这些标记从而执行一些特殊操作。因此可以得出自定义注解使用的基本流程： 第一步，定义注解——相当于定义标记；第二步，配置注解——把标记打在需要用到的程序代码中；第三步，解析注解——在编译期或运行时检测到标记，并进行特殊操作。 2.1 基本语法注解类型的声明部分：注解在Java中，与类、接口、枚举类似，因此其声明语法基本一致，只是所使用的关键字有所不同@interface。在底层实现上，所有定义的注解都会自动继承 123java.lang.annotation.Annotation接口。public @interface CherryAnnotation {} 注解类型的实现部分：根据我们在自定义类的经验，在类的实现部分无非就是书写构造、属性或方法。但是，在自定义注解中，其实现部分只能定义一个东西：注解类型元素（annotation type element）。咱们来看看其语法： 12345public @interface CherryAnnotation { public String name(); int age(); int[] array();} 也许你会认为这不就是接口中定义抽象方法的语法嘛？别着急，咱们看看下面这个： 12345public @interface CherryAnnotation { public String name(); int age() default 18; int[] array();} 看到关键字default了吗？还觉得是抽象方法吗？注解里面定义的是：注解类型元素！定义注解类型元素时需要注意如下几点： 访问修饰符必须为public，不写默认为public；该元素的类型只能是基本数据类型、String、Class、枚举类型、注解类型（体现了注解的嵌套效果）以及上述类型的一位数组；该元素的名称一般定义为名词，如果注解中只有一个元素，请把名字起为value（后面使用会带来便利操作）；()不是定义方法参数的地方，也不能在括号中定义任何参数，仅仅只是一个特殊的语法；default代表默认值，值必须和第2点定义的类型一致；如果没有默认值，代表后续使用注解时必须给该类型元素赋值。 可以看出，注解类型元素的语法非常奇怪，即又有属性的特征（可以赋值）,又有方法的特征（打上了一对括号）。但是这么设计是有道理的，我们在后面的章节中可以看到：注解在定义好了以后，使用的时候操作元素类型像在操作属性，解析的时候操作元素类型像在操作方法。2.2 常用的元注解一个最最基本的注解定义就只包括了上面的两部分内容：1、注解的名字；2、注解包含的类型元素。但是，我们在使用JDK自带注解的时候发现，有些注解只能写在方法上面（比如@Override）；有些却可以写在类的上面（比如@Deprecated）。当然除此以外还有很多细节性的定义，那么这些定义该如何做呢？接下来就该元注解出场了！元注解：专门修饰注解的注解。它们都是为了更好的设计自定义注解的细节而专门设计的。我们为大家一个个来做介绍。2.2.1 @Target@Target注解，是专门用来限定某个自定义注解能够被应用在哪些Java元素上面的。它使用一个枚举类型定义如下： 12345678910111213141516171819202122232425public enum ElementType { /** 类，接口（包括注解类型）或枚举的声明 */ TYPE, /** 属性的声明 */ FIELD, /** 方法的声明 */ METHOD, /** 方法形式参数声明 */ PARAMETER, /** 构造方法的声明 */ CONSTRUCTOR, /** 局部变量声明 */ LOCAL_VARIABLE, /** 注解类型声明 */ ANNOTATION_TYPE, /** 包的声明 */ PACKAGE} 1234567//@CherryAnnotation被限定只能使用在类、接口或方法上面@Target(value = {ElementType.TYPE,ElementType.METHOD})public @interface CherryAnnotation { String name(); int age() default 18; int[] array();} 2.2.2 @Retention@Retention注解，翻译为持久力、保持力。即用来修饰自定义注解的生命力。注解的生命周期有三个阶段：1、Java源文件阶段；2、编译到class文件阶段；3、运行期阶段。同样使用了RetentionPolicy枚举类型定义了三个阶段： 1234567891011121314151617181920212223public enum RetentionPolicy { /** * Annotations are to be discarded by the compiler. * （注解将被编译器忽略掉） */ SOURCE, /** * Annotations are to be recorded in the class file by the compiler * but need not be retained by the VM at run time. This is the default * behavior. * （注解将被编译器记录在class文件中，但在运行时不会被虚拟机保留，这是一个默认的行为） */ CLASS, /** * Annotations are to be recorded in the class file by the compiler and * retained by the VM at run time, so they may be read reflectively. * （注解将被编译器记录在class文件中，而且在运行时会被虚拟机保留，因此它们能通过反射被读取到） * @see java.lang.reflect.AnnotatedElement */ RUNTIME} 我们再详解一下： 如果一个注解被定义为RetentionPolicy.SOURCE，则它将被限定在Java源文件中，那么这个注解即不会参与编译也不会在运行期起任何作用，这个注解就和一个注释是一样的效果，只能被阅读Java文件的人看到；如果一个注解被定义为RetentionPolicy.CLASS，则它将被编译到Class文件中，那么编译器可以在编译时根据注解做一些处理动作，但是运行时JVM（Java虚拟机）会忽略它，我们在运行期也不能读取到；如果一个注解被定义为RetentionPolicy.RUNTIME，那么这个注解可以在运行期的加载阶段被加载到Class对象中。那么在程序运行阶段，我们可以通过反射得到这个注解，并通过判断是否有这个注解或这个注解中属性的值，从而执行不同的程序代码段。我们实际开发中的自定义注解几乎都是使用的RetentionPolicy.RUNTIME；在默认的情况下，自定义注解是使用的RetentionPolicy.CLASS。 2.2.3 @Documented@Documented注解，是被用来指定自定义注解是否能随着被定义的java文件生成到JavaDoc文档当中。2.2.4 @Inherited@Inherited注解，是指定某个自定义注解如果写在了父类的声明部分，那么子类的声明部分也能自动拥有该注解。@Inherited注解只对那些@Target被定义为ElementType.TYPE的自定义注解起作用。3 自定义注解的配置使用回顾一下注解的使用流程： 第一步，定义注解——相当于定义标记；第二步，配置注解——把标记打在需要用到的程序代码中；第三步，解析注解——在编译期或运行时检测到标记，并进行特殊操作。 到目前为止我们只是完成了第一步，接下来我们就来学习第二步，配置注解，如何在另一个类当中配置它。3.1 在具体的Java类上使用注解首先，定义一个注解、和一个供注解修饰的简单Java类 12345678910111213141516@Retention(RetentionPolicy.RUNTIME)@Target(value = {ElementType.METHOD})@Documentedpublic @interface CherryAnnotation { String name(); int age() default 18; int[] score();}12345678public class Student{ public void study(int times){ for(int i = 0; i &lt; times; i++){ System.out.println(\"Good Good Study, Day Day Up!\"); } }} 简单分析下： CherryAnnotation的@Target定义为ElementType.METHOD，那么它书写的位置应该在方法定义的上方，即：public void study(int times)之上；由于我们在CherryAnnotation中定义的有注解类型元素，而且有些元素是没有默认值的，这要求我们在使用的时候必须在标记名后面打上()，并且在()内以“元素名=元素值“的形式挨个填上所有没有默认值的注解类型元素（有默认值的也可以填上重新赋值），中间用“,”号分割； 所以最终书写形式如下： 12345678public class Student { @CherryAnnotation(name = \"cherry-peng\",age = 23,score = {99,66,77}) public void study(int times){ for(int i = 0; i &lt; times; i++){ System.out.println(\"Good Good Study, Day Day Up!\"); } }} 3.2 特殊语法特殊语法一：如果注解本身没有注解类型元素，那么在使用注解的时候可以省略()，直接写为：@注解名，它和标准语法@注解名()等效！ 1234567891011@Retention(RetentionPolicy.RUNTIME)@Target(value = {ElementType.TYPE})@Documentedpublic @interface FirstAnnotation {}12345//等效于@FirstAnnotation()@FirstAnnotationpublic class JavaBean{ //省略实现部分} 特殊语法二：如果注解本本身只有一个注解类型元素，而且命名为value，那么在使用注解的时候可以直接使用：@注解名(注解值)，其等效于：@注解名(value = 注解值) 123456@Retention(RetentionPolicy.RUNTIME)@Target(value = {ElementType.TYPE})@Documentedpublic @interface SecondAnnotation { String value();} 12345//等效于@ SecondAnnotation(value = \"this is second annotation\")@SecondAnnotation(\"this is annotation\")public class JavaBean{ //省略实现部分} 特殊用法三：如果注解中的某个注解类型元素是一个数组类型，在使用时又出现只需要填入一个值的情况，那么在使用注解时可以直接写为：@注解名(类型名 = 类型值)，它和标准写法：@注解名(类型名 = {类型值})等效！ 123456@Retention(RetentionPolicy.RUNTIME)@Target(value = {ElementType.TYPE})@Documentedpublic @interface ThirdAnnotation { String[] name();} 12345//等效于@ ThirdAnnotation(name = {\"this is third annotation\"})@ ThirdAnnotation(name = \"this is third annotation\")public class JavaBean{ //省略实现部分} 特殊用法四：如果一个注解的@Target是定义为Element.PACKAGE，那么这个注解是配置在package-info.java中的，而不能直接在某个类的package代码上面配置。4 自定义注解的运行时解析这一章是使用注解的核心，读完此章即可明白，如何在程序运行时检测到注解，并进行一系列特殊操作！4.1 回顾注解的保持力首先回顾一下，之前自定义的注解@CherryAnnotation，并把它配置在了类Student上，代码如下： 12345678@Retention(RetentionPolicy.RUNTIME)@Target(value = {ElementType.METHOD})@Documentedpublic @interface CherryAnnotation { String name(); int age() default 18; int[] score();} 123456789package pojos;public class Student { @CherryAnnotation(name = \"cherry-peng\",age = 23,score = {99,66,77}) public void study(int times){ for(int i = 0; i &lt; times; i++){ System.out.println(\"Good Good Study, Day Day Up!\"); } }} 注解保持力的三个阶段： Java源文件阶段；编译到class文件阶段；运行期阶段。 只有当注解的保持力处于运行阶段，即使用@Retention(RetentionPolicy.RUNTIME)修饰注解时，才能在JVM运行时，检测到注解，并进行一系列特殊操作。4.2 反射操作获取注解因此，明确我们的目标：在运行期探究和使用编译期的内容（编译期配置的注解），要用到Java中的灵魂技术——反射！ 12345678910111213141516171819202122232425public class TestAnnotation { public static void main(String[] args){ try { //获取Student的Class对象 Class stuClass = Class.forName(\"pojos.Student\"); //说明一下，这里形参不能写成Integer.class，应写为int.class Method stuMethod = stuClass.getMethod(\"study\",int.class); if(stuMethod.isAnnotationPresent(CherryAnnotation.class)){ System.out.println(\"Student类上配置了CherryAnnotation注解！\"); //获取该元素上指定类型的注解 CherryAnnotation cherryAnnotation = stuMethod.getAnnotation(CherryAnnotation.class); System.out.println(\"name: \" + cherryAnnotation.name() + \", age: \" + cherryAnnotation.age() + \", score: \" + cherryAnnotation.score()[0]); }else{ System.out.println(\"Student类上没有配置CherryAnnotation注解！\"); } } catch (ClassNotFoundException e) { e.printStackTrace(); } catch (NoSuchMethodException e) { e.printStackTrace(); } }} 解释一下： 如果我们要获得的注解是配置在方法上的，那么我们要从Method对象上获取；如果是配置在属性上，就需要从该属性对应的Field对象上去获取，如果是配置在类型上，需要从Class对象上去获取。总之在谁身上，就从谁身上去获取！isAnnotationPresent(Class&lt;? extends Annotation&gt; annotationClass)方法是专门判断该元素上是否配置有某个指定的注解；getAnnotation(Class annotationClass)方法是获取该元素上指定的注解。之后再调用该注解的注解类型元素方法就可以获得配置时的值数据；反射对象上还有一个方法getAnnotations()，该方法可以获得该对象身上配置的所有的注解。它会返回给我们一个注解数组，需要注意的是该数组的类型是Annotation类型，这个Annotation是一个来自于java.lang.annotation包的接口。","link":"/2019/07/16/custom-annotation/"},{"title":"debug-idea快捷键","text":"快捷键 介绍 F7 在 Debug 模式下，进入下一步，如果当前行断点是一个方法，则进入当前方法体内，如果该方法体还有方法，则不会进入该内嵌的方法中 F8 在 Debug 模式下，进入下一步，如果当前行断点是一个方法，则不进入当前方法体内 F9 在 Debug 模式下，恢复程序运行，但是如果该断点下面代码还有断点则停在下一个断点上 Alt + F8 在 Debug 的状态下，选中对象，弹出可输入计算表达式调试框，查看该输入内容的调试结果 Ctrl + F8 在 Debug 模式下，设置光标当前行为断点，如果当前已经是断点则去掉断点 Shift + F7 在 Debug 模式下，智能步入。断点所在行上有多个方法调用，会弹出进入哪个方法 Shift + F8 在 Debug 模式下，跳出，表现出来的效果跟 F9 一样 Ctrl + Shift + F8 在 Debug 模式下，指定断点进入条件 Alt + Shift + F7 在 Debug 模式下，进入下一步，如果当前行断点是一个方法，则进入当前方法体内，如果方法体还有方法，则会进入该内嵌的方法中，依此循环进入","link":"/2019/09/17/debug-idea快捷键/"},{"title":"docker onos","text":"在Docker中运行ONOS 下载镜像： docker pull onosproject/onos 查看上一步下载的镜像 docker images 创建docker容器实例docker run -t -d --name onos1 onosproject/onos 查看上一步创建的docker实例 修改~/.bashrc文件，获取容器实例的IP 12345docker-ip() {sudo docker inspect --format '{{ .NetworkSettings.IPAddress }}' \"$@\"} . ~/.bashrc 用SSH连接一个容器实例，密码是karaf 1ssh -p 8101 karaf@`docker-ip onos1` 激活Openflow onos&gt; app activate org.onosproject.openflow onos&gt; app activate org.onosproject.fwd 如果上述命令报错，则SSH连接到其它docker实例,直至成功激活Openflow 测试apt install mininet 1mn --topo tree,2 --controller remote,ip=`docker-ip onos3`","link":"/2019/07/24/docker-onos/"},{"title":"docker gitlab","text":"使用docker搭建gitlab初体验+数据备份 一. 背景作为程序员，像GitHub这种好工具是必须得十分了解的，但是有时GitHub并不能满足我们所有的需求，就如作者所在的公司，我们的代码都是商业性的产品，不可能放到GitHub的开放仓库中的，而申请GitHub私人仓库需要钱。这就陷入了尴尬的局面，那有没有一种既能具有GitHub一样的功能，又能保护隐私免费的管理工具呢？答案是肯定的，感谢程序员伟大的开源精神，我们有了GitLab!!!今天笔者在这里就跟大家分享一下自己使用docker搭建GitLab的过程吧，这其中踩了一些坑，希望看过这篇文章的人不用在踩我踩过的坑了！二. 环境介绍服务器信息：CPU : 2DISK : 30GRAM : 4GOS : Linux centos7-0 3.10.0-229.el7.x86_64这里笔者使用的是自己公司的服务器，也可以使用虚拟机进行搭建三. 搭建过程 安装docker因为我们是使用docker搭建的，所以需要先安装docker，docker支持不同的OS，具体的安装信息这里不做详细介绍，可以自己的操作系统，参考官方的安装指南进行安装。 http://www.docker.io 安装GitLab及相关组件GitLab需要用到数据库来存储相关数据，所以需要在安装GitLab的同时安装数据库，这里使用的是postgresql和redis。我在查找相关的镜像，之后发现有很多现成的镜像，这里我使用的sameersbn镜像。但是有一点我认为不是很好的是：这个镜像没有把redis、postgresql集成到gitlab的容器里面，需要先单独pull这两个镜像run一下，然后再pull gitlab的镜像进行安装。使用如下命令分别拉取最新的镜像：docker pull sameersbn/redisdocker pull sameersbn/postgresqldocker pull sameersbn/gitlab这里有第一个坑：因为我们默认都是从docker的官方仓库中拉去镜像，但是由于国内访问国外的网站有墙，而且速度也是十分的慢，所以需要代理。这里推荐Daocloud加速器 https://www.daocloud.io/ 免费使用，但是需要先注册，登录成功后，找到加速器执行相关命令即可。笔者亲测速度明显快很多！使用如下命令运行postgresql镜像： 1234567docker run --name postgresql -d \\ -e 'DB_NAME=gitlabhq_production' \\ -e 'DB_USER=gitlab' \\-e 'DB_PASS=password' \\ -e 'DB_EXTENSION=pg_trgm' \\ -v /home/root/opt/postgresql/data:/var/lib/postgresql \\ sameersbn/postgresql 这里需要解释的是：(1). 以上是一条命令，反斜杠是为了在命令内换行方便阅读，如果不喜欢，也可以写在一行。(2). -e后面跟的都是容器的环境参数，都是在制作镜像的时候指定好的，所以不要去改动。(3). -v后面是添加数据卷，这样在容器退出的时候数据就不会丢失，其中 /home/root/opt/postgresql/data是作者自己创建的文件夹，读者可以自己自定义，后面的部分是容器内的文件路径，需要保持不变。(4). 命令执行成功之后会在控制台显示一串容器的编号，可以使用命令docker ps查看刚刚启动的容器。 使用如下命令运行redis镜像： 123docker run --name redis -d \\ -v /home/root/opt/redis/data:/var/lib/redis \\ sameersbn/redis 这里跟启动postgresql一样。使用如下命令运行GitLab镜像： 123456789101112131415161718192021docker run --name gitlab -d \\--link postgresql:postgresql --link redis:redisio \\-p 10022:22 -p 10080:80 \\-e 'GITLAB_PORT=10080' \\-e 'GITLAB_SSH_PORT=10022' \\-e 'GITLAB_SECRETS_DB_KEY_BASE=long-and-random-alpha-numeric-string'\\-e 'GITLAB_SECRETS_SECRET_KEY_BASE=long-and-random-alpha-numeric-string' \\-e 'GITLAB_SECRETS_OTP_KEY_BASE=long-and-random-alpha-numeric-string'\\-e 'GITLAB_HOST=服务器地址' \\-e 'GITLAB_EMAIL=邮箱地址' \\-e 'SMTP_ENABLED=true' \\-e 'SMTP_DOMAIN=www.sina.com' \\-e 'SMTP_HOST=smtp.sina.com' \\ -e 'SMTP_STARTTLS=false' \\-e 'SMTP_USER=邮箱地址' \\-e 'SMTP_PASS=邮箱密码' \\-e 'SMTP_AUTHENTICATION=login' \\-e 'GITLAB_BACKUP_SCHEDULE=daily' \\-e 'GITLAB_BACKUP_TIME=10:30' \\-v /home/root/opt/gitlab/data:/home/git/data \\sameersbn/gitlab 这里需要解释的是：(1). 网上又很多教程讲关于使用docker安装GitLab，但是讲的不全面，至少我按照他们的方法安装时不能正常运行，这里是第三个坑：一定要加上如下环境参数： 123-e 'GITLAB_SECRETS_DB_KEY_BASE=long-and-random-alpha-numeric-string'\\-e 'GITLAB_SECRETS_SECRET_KEY_BASE=long-and-random-alpha-numeric-string' \\-e 'GITLAB_SECRETS_OTP_KEY_BASE=long-and-random-alpha-numeric-string'\\ 有关于这三个环境参数的含义： 官方的解释 我个人的理解是用来进行加密的key。(2). 上面有关SMTP的环境参数是配置邮箱的，需要填上对应的邮箱信息，我使用的是新浪邮箱，读者可以根据自己的邮箱进行填写。(3). 使用GitLab需要两个端口，一个是web端口，一个是SSH端口用于push代码的所以一下代码进行端口映射和指定： -p 10022:22 -p 10080:80 -e ‘GITLAB_PORT=10080’ -e ‘GITLAB_SSH_PORT=10022’ (4). GitLab有自带的备份，这里可以通过如下进行配置：-e ‘GITLAB_BACKUP_SCHEDULE=daily’ -e ‘GITLAB_BACKUP_TIME=10:30’ 指定的是每天10:30进行备份。说到这里基本上GitLab就搭建好了，这里还有一个小坑就是：运行这些容器的时候可以把代码写进shell脚本中，然后通过脚本进行运行，不然直接在终端打的话很麻烦。一下就是笔者安装完后的截图，直接访问：http://服务器地址:10080 即可，首次访问可能会出现错误页面，刷新几下页面就可以了然后在修改密码默认用户名：root 之后就可以正常使用。 登录界面 group admin area四. 备份我们可以使用GitLab自带的备份功能，在启动容器的时候就进行设置，然后再使用GitLab的 app:rake gitlab:backup:restore命令进行恢复，这里网上的教程都有说明可以参考以下网站：sameersbn的GitHub wiki：https://github.com/sameersbn/docker-gitlab#automated-backups这个是官方的所以比较全面，里面还有关于各种环境参数的介绍。这里作者使用的是如下的备份方法：因为我们在运行postgresql、redis和GitLab的时候都使用了本地的文件夹进行了数据的持久化，而且我们实际需要备份的数据都在本地了，那么其实就可以直接使用rsync命令备份本地的这些卷（刚刚的文件夹）即可，无需再去深入到GitLab内部。如果搭建的GitLab崩溃了，或者服务器崩溃了，直接再使用docker再搭一个，在把刚刚的卷跟对应的postgresql、redis和GitLab内的数据文件夹进行映射即可。这是也不需要修改之前的启动命令，十分的方便而且作者自己测试过，发现能够达到要求，原先的仓库、用户的SSH信息等都在。 修改100.230 /data/gitlab_875/config/gitlab.rb (docker gitlab875 /etc/gitlab/gitlab.rb)增加: 1234567891011121314151617gitlab_rails['smtp_enable'] = true#gitlab_rails['smtp_address'] = \"smtp.domain.cn\"gitlab_rails['smtp_address'] = \"12.34.56.78\"gitlab_rails['smtp_port'] = 25gitlab_rails['smtp_user_name'] = \"gitlabadmin@domain.cn\"gitlab_rails['smtp_password'] = \"yourpassword\"gitlab_rails['smtp_domain'] = \"smtp.domain.cn\"gitlab_rails['smtp_authentication'] = \"login\"gitlab_rails['smtp_enable_starttls_auto'] = truegitlab_rails['smtp_tls'] = falsegitlab_rails['smtp_openssl_verify_mode'] = 'none'gitlab_rails['gitlab_email_enabled'] = truegitlab_rails['gitlab_email_from'] = 'gitlabadmin@domain.cn'gitlab_rails['gitlab_email_display_name'] = 'gitlabadmin'gitlab_rails['gitlab_email_reply_to'] = 'gitlabadmin@domain.cn'gitlab_rails['gitlab_email_subject_suffix'] = ''","link":"/2019/07/17/docker-gitlab/"},{"title":"docker重指定挂载镜像","text":"docker-修改容器的挂载目录三种方式方式一：修改配置文件（需停止docker服务）1、停止docker服务systemctl stop docker.service（关键，修改之前必须停止docker服务）2、vim /var/lib/docker/containers/container-ID/config.v2.json修改配置文件中的目录位置，然后保存退出 “MountPoints”:{“/home”:{“Source”:”/docker”,”Destination”:”/home”,”RW”:true,”Name”:””,”Driver”:””,”Type”:”bind”,”Propagation”:”rprivate”,”Spec”:{“Type”:”bind”,”Source”:”//docker/“,”Target”:”/home”}}} 3、启动docker服务systemctl start docker.service4、启动docker容器docker start &lt;container-name/ID&gt;方式二：提交现有容器为新镜像，然后重新运行它 $ docker ps -a|CONTAINERID|IMAGE|COMMAND|CREATED|STATUS|PORTS|NAMES||:-|:-|:-|:-|:-|:-|:-||5a3422adeead|ubuntu:14.04|”/bin/bash”| About a minute ago|Exited(0)|About a minute ago|agitated_newton|$ docker commit 5a3422adeead newimagename$ docker run -ti -v “$PWD/dir1”:/dir1 -v “$PWD/dir2”:/dir2 newimagename /bin/bash 然后停止旧容器，并使用这个新容器，如果由于某种原因需要新容器使用旧名称，请在删除旧容器后使用docker rename。方式三：export容器为镜像，然后import为新镜像 $docker container export -o ./myimage.docker 容器ID$docker import ./myimage.docker newimagename$docker run -ti -v “$PWD/dir1”:/dir1 -v “$PWD/dir2”:/dir2 newimagename /bin/bash 然后停止旧容器，并使用这个新容器，如果由于某种原因需要新容器使用旧名称，请在删除旧容器后使用docker rename。","link":"/2019/07/22/docker重指定挂载镜像/"},{"title":"elasticsearch x_pack","text":"SHOW FUNCTIONS; name type AVG AGGREGATE COUNT AGGREGATE FIRST AGGREGATE FIRST_VALUE AGGREGATE LAST AGGREGATE LAST_VALUE AGGREGATE MAX AGGREGATE MIN AGGREGATE SUM AGGREGATE KURTOSIS AGGREGATE MAD AGGREGATE PERCENTILE AGGREGATE PERCENTILE_RANK AGGREGATE SKEWNESS AGGREGATE STDDEV_POP AGGREGATE SUM_OF_SQUARES AGGREGATE VAR_POP AGGREGATE HISTOGRAM GROUPING CASE CONDITIONAL COALESCE CONDITIONAL GREATEST CONDITIONAL IFNULL CONDITIONAL IIF CONDITIONAL ISNULL CONDITIONAL LEAST CONDITIONAL NULLIF CONDITIONAL NVL CONDITIONAL CURDATE SCALAR CURRENT_DATE SCALAR CURRENT_TIME SCALAR CURRENT_TIMESTAMP SCALAR CURTIME SCALAR DATEADD SCALAR DATEDIFF SCALAR DATEPART SCALAR DATETRUNC SCALAR DATE_ADD SCALAR DATE_DIFF SCALAR DATE_PART SCALAR DATE_TRUNC SCALAR DAY SCALAR DAYNAME SCALAR DAYOFMONTH SCALAR DAYOFWEEK SCALAR DAYOFYEAR SCALAR DAY_NAME SCALAR DAY_OF_MONTH SCALAR DAY_OF_WEEK SCALAR DAY_OF_YEAR SCALAR DOM SCALAR DOW SCALAR DOY SCALAR HOUR SCALAR HOUR_OF_DAY SCALAR IDOW SCALAR ISODAYOFWEEK SCALAR ISODOW SCALAR ISOWEEK SCALAR ISOWEEKOFYEAR SCALAR ISO_DAY_OF_WEEK SCALAR ISO_WEEK_OF_YEAR SCALAR IW SCALAR IWOY SCALAR MINUTE SCALAR MINUTE_OF_DAY SCALAR MINUTE_OF_HOUR SCALAR MONTH SCALAR MONTHNAME SCALAR MONTH_NAME SCALAR MONTH_OF_YEAR SCALAR NOW SCALAR QUARTER SCALAR SECOND SCALAR SECOND_OF_MINUTE SCALAR TIMESTAMPADD SCALAR TIMESTAMPDIFF SCALAR TIMESTAMP_ADD SCALAR TIMESTAMP_DIFF SCALAR TODAY SCALAR WEEK SCALAR WEEK_OF_YEAR SCALAR YEAR SCALAR ABS SCALAR ACOS SCALAR ASIN SCALAR ATAN SCALAR ATAN2 SCALAR CBRT SCALAR CEIL SCALAR CEILING SCALAR COS SCALAR COSH SCALAR COT SCALAR DEGREES SCALAR E SCALAR EXP SCALAR EXPM1 SCALAR FLOOR SCALAR LOG SCALAR LOG10 SCALAR MOD SCALAR PI SCALAR POWER SCALAR RADIANS SCALAR RAND SCALAR RANDOM SCALAR ROUND SCALAR SIGN SCALAR SIGNUM SCALAR SIN SCALAR SINH SCALAR SQRT SCALAR TAN SCALAR TRUNCATE SCALAR ASCII SCALAR BIT_LENGTH SCALAR CHAR SCALAR CHARACTER_LENGTH SCALAR CHAR_LENGTH SCALAR CONCAT SCALAR INSERT SCALAR LCASE SCALAR LEFT SCALAR LENGTH SCALAR LOCATE SCALAR LTRIM SCALAR OCTET_LENGTH SCALAR POSITION SCALAR REPEAT SCALAR REPLACE SCALAR RIGHT SCALAR RTRIM SCALAR SPACE SCALAR SUBSTRING SCALAR UCASE SCALAR CAST SCALAR CONVERT SCALAR DATABASE SCALAR USER SCALAR ST_ASTEXT SCALAR ST_ASWKT SCALAR ST_DISTANCE SCALAR ST_GEOMETRYTYPE SCALAR ST_GEOMFROMTEXT SCALAR ST_WKTTOSQL SCALAR ST_X SCALAR ST_Y SCALAR ST_Z SCALAR SCORE SCORE","link":"/2019/12/19/elasticsearchSQL/"},{"title":"postman 接口文档","text":"ensbrain/apis/sdp/user/controller12345678910111213141516171819IntroductionWhat does your API do? 接口功能OverviewThings that the developers should know about 开发者需要知道的事AuthenticationWhat is the preferred way of using the API? 怎样使用Error CodesWhat errors and status codes can a user expect? 什么错误和状态码使用者会遇到Rate limitIs there a limit to the number of requests an user can send? 是否有请求数量的限制 ensbrain/apis/usergroup 123456789101112131415161718192021PUT localhost:8282/ensbrain/apis/usergroup/addlocalhost:8282/ensbrain/apis/usergroup/add新增用户组usergroupHeadersContent-Type application/jsonBodyraw (application/json){ \"groupId\": \"3\", \"groupName\": \"sdptest3\", \"state\": \"ENABLED\"}Example Requestlocalhost:8282/ensbrain/apis/usergroup/addcurl --location --request PUT \"localhost:8282/ensbrain/apis/usergroup/add\" \\ --header \"Content-Type: application/json\" \\ --data \"{ \\\"groupId\\\": \\\"3\\\", \\\"groupName\\\": \\\"sdptest3\\\", \\\"state\\\": \\\"ENABLED\\\"}\" 123456789101112131415161718192021POST localhost:8282/ensbrain/apis/usergroup/updatelocalhost:8282/ensbrain/apis/usergroup/update更新用户组usergroupHeadersContent-Type application/jsonBodyraw (application/json){ \"groupId\": \"4\", \"groupName\": \"sdp4\", \"state\": \"ENABLED\"}Example Requestlocalhost:8282/ensbrain/apis/usergroup/updatecurl --location --request POST \"localhost:8282/ensbrain/apis/usergroup/update\" \\ --header \"Content-Type: application/json\" \\ --data \"{ \\\"groupId\\\": \\\"4\\\", \\\"groupName\\\": \\\"sdp4\\\", \\\"state\\\": \\\"ENABLED\\\"}\" 1234567891011121314151617GET localhost:8282/ensbrain/apis/usergroup/1localhost:8282/ensbrain/apis/usergroup/1根据id查找用户组HeadersContent-Type application/jsonBodyraw (application/json){ \"id\" : \"1\"}Example Requestlocalhost:8282/ensbrain/apis/usergroup/1curl --location --request GET \"localhost:8282/ensbrain/apis/usergroup/1\" \\ --header \"Content-Type: application/json\" \\ --data \"{ \\\"id\\\" : \\\"1\\\"}\" 12345678910111213141516171819202122232425POST localhost:8282/ensbrain/apis/usergroup/conditionlocalhost:8282/ensbrain/apis/usergroup/condition按条件查询HeadersContent-Type application/jsonBodyraw (application/json){ \"condition\" : { \"groupName\" : \"sdptest\" }, \"pageIndex\" : 1, \"pageSize\" : 10}Example Requestlocalhost:8282/ensbrain/apis/usergroup/conditioncurl --location --request POST \"localhost:8282/ensbrain/apis/usergroup/condition\" \\ --header \"Content-Type: application/json\" \\ --data \"{ \\\"condition\\\" : { \\\"groupName\\\" : \\\"sdptest\\\" }, \\\"pageIndex\\\" : 1, \\\"pageSize\\\" : 10}\" 123456789101112131415161718192021DELETE localhost:8282/ensbrain/apis/usergroup/deletelocalhost:8282/ensbrain/apis/usergroup/delete删除用户组HeadersContent-Type application/jsonBodyraw (application/json){ \"groupId\": \"4\", \"groupName\": \"sdp4\", \"state\": \"ENABLED\"}Example Requestlocalhost:8282/ensbrain/apis/usergroup/deletecurl --location --request DELETE \"localhost:8282/ensbrain/apis/usergroup/delete\" \\ --header \"Content-Type: application/json\" \\ --data \"{ \\\"groupId\\\": \\\"4\\\", \\\"groupName\\\": \\\"sdp4\\\", \\\"state\\\": \\\"ENABLED\\\"}\" ensbrain/apis/userTerminal 123456789101112131415161718192021222324252627PUT localhost:8282/ensbrain/apis/userTerminal/addlocalhost:8282/ensbrain/apis/userTerminal/add增加userTerminalHeadersContent-Type application/jsonBodyraw (application/json){ \"terminalId\": \"123\", \"name\": \"term-test\", \"type\": \"PC\", \"description\": \"测试终端\", \"icon\": \"e4U=\", \"state\": \"ENABLED\"}Example Requestlocalhost:8282/ensbrain/apis/userTerminal/addcurl --location --request PUT \"localhost:8282/ensbrain/apis/userTerminal/add\" \\ --header \"Content-Type: application/json\" \\ --data \"{ \\\"terminalId\\\": \\\"123\\\", \\\"name\\\": \\\"term-test\\\", \\\"type\\\": \\\"PC\\\", \\\"description\\\": \\\"测试终端\\\", \\\"icon\\\": \\\"e4U=\\\", \\\"state\\\": \\\"ENABLED\\\"}\" 123456789101112131415161718192021222324252627DELETE localhost:8282/ensbrain/apis/userTerminal/deletelocalhost:8282/ensbrain/apis/userTerminal/delete删除userTerminalHeadersContent-Type application/jsonBodyraw (application/json){ \"terminalId\": \"123\", \"name\": \"term-test\", \"type\": \"PC\", \"description\": \"测试终端\", \"icon\": \"e4U=\", \"state\": \"ENABLED\"}Example Requestlocalhost:8282/ensbrain/apis/userTerminal/deletecurl --location --request DELETE \"localhost:8282/ensbrain/apis/userTerminal/delete\" \\ --header \"Content-Type: application/json\" \\ --data \"{ \\\"terminalId\\\": \\\"123\\\", \\\"name\\\": \\\"term-test\\\", \\\"type\\\": \\\"PC\\\", \\\"description\\\": \\\"测试终端\\\", \\\"icon\\\": \\\"e4U=\\\", \\\"state\\\": \\\"ENABLED\\\"}\" 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152POST localhost:8282/ensbrain/apis/userTerminal/conditionlocalhost:8282/ensbrain/apis/userTerminal/condition条件查询userTerminalHeadersContent-Type application/jsonBodyraw (application/json){ \"condition\" : { \"name\" : \"term-test\" }, \"pageIndex\" : 1, \"pageSize\" : 10}Example Requestlocalhost:8282/ensbrain/apis/userTerminal/conditioncurl --location --request POST \"localhost:8282/ensbrain/apis/userTerminal/condition\" \\ --header \"Content-Type: application/json\" \\ --data \"{ \\\"condition\\\" : { \\\"name\\\" : \\\"term-test\\\" }, \\\"pageIndex\\\" : 1, \\\"pageSize\\\" : 10}\"GET localhost:8282/ensbrain/apis/userTerminal/123localhost:8282/ensbrain/apis/userTerminal/123根据id查找userTerminalHeadersContent-Type application/jsonBodyraw (application/json){ \"terminalId\": \"123\", \"name\": \"term-test\", \"type\": \"PC\", \"description\": \"测试终端\", \"icon\" : [123,133], \"state\": \"ENABLED\"}Example Requestlocalhost:8282/ensbrain/apis/userTerminal/123curl --location --request GET \"localhost:8282/ensbrain/apis/userTerminal/123\" \\ --header \"Content-Type: application/json\" \\ --data \"{ \\\"terminalId\\\": \\\"123\\\", \\\"name\\\": \\\"term-test\\\", \\\"type\\\": \\\"PC\\\", \\\"description\\\": \\\"测试终端\\\", \\\"icon\\\" : [123,133], \\\"state\\\": \\\"ENABLED\\\"}\" 123456789101112131415161718192021222324252627POST localhost:8282/ensbrain/apis/userTerminal/updatelocalhost:8282/ensbrain/apis/userTerminal/update更新userTerminalHeadersContent-Type application/jsonBodyraw (application/json){ \"terminalId\": \"123\", \"name\": \"term-test\", \"type\": \"PC\", \"description\": \"测试终端\", \"icon\" : [123,133], \"state\": \"ENABLED\"}Example Requestlocalhost:8282/ensbrain/apis/userTerminal/updatecurl --location --request POST \"localhost:8282/ensbrain/apis/userTerminal/update\" \\ --header \"Content-Type: application/json\" \\ --data \"{ \\\"terminalId\\\": \\\"123\\\", \\\"name\\\": \\\"term-test\\\", \\\"type\\\": \\\"PC\\\", \\\"description\\\": \\\"测试终端\\\", \\\"icon\\\" : [123,133], \\\"state\\\": \\\"ENABLED\\\"}\"","link":"/2019/08/28/ensbrain-api-doc/"},{"title":"es_sql","text":"通过 elasticsearch-sql 使用 SQL 语句聚合查询 Elasticsearch 获取各种 metrics 度量值 Elasticsearch 的 metrics（度量）包含 count、sum、avg、max、min、percentiles (百分位数)、Unique count（基数 || 去重计数）、Median（中位数）、扩展度量（含方差、平方和、标准差、标准差界限）、Percentile ranks（百分位等级） count（数量）：SELECT count(log_date.d) AS Count FROM INDEX-2017-12 123456789101112131415{ \"from\" : 0, \"size\" : 0, \"_source\" : { \"includes\" : [ \"COUNT\" ], \"excludes\" : [ ] }, \"aggregations\" : { \"Count\" : { \"value_count\" : { \"field\" : \"log_date.d\" } } }} sum（和）：SELECT sum(log_date.d) AS SUM FROM INDEX-2017-12 123456789101112131415{ \"from\" : 0, \"size\" : 0, \"_source\" : { \"includes\" : [ \"SUM\" ], \"excludes\" : [ ] }, \"aggregations\" : { \"SUM\" : { \"sum\" : { \"field\" : \"log_date.d\" } } }} avg（平均数）：SELECT avg(log_date.d) AS AVG FROM INDEX-2017-12 123456789101112131415{ \"from\" : 0, \"size\" : 0, \"_source\" : { \"includes\" : [ \"AVG\" ], \"excludes\" : [ ] }, \"aggregations\" : { \"AVG\" : { \"avg\" : { \"field\" : \"log_date.d\" } } }} max（最大值）：SELECT max(log_date.d) AS MAX FROM INDEX-2017-12 123456789101112131415{ \"from\" : 0, \"size\" : 0, \"_source\" : { \"includes\" : [ \"MAX\" ], \"excludes\" : [ ] }, \"aggregations\" : { \"MAX\" : { \"max\" : { \"field\" : \"log_date.d\" } } }} min（最小值）：SELECT min(log_date.d) AS MIN FROM INDEX-2017-12 123456789101112131415{ \"from\" : 0, \"size\" : 0, \"_source\" : { \"includes\" : [ \"MIN\" ], \"excludes\" : [ ] }, \"aggregations\" : { \"MIN\" : { \"min\" : { \"field\" : \"log_date.d\" } } }} percentiles（百分位数）：SELECT percentiles(log_date.d,1.0,15.0,31.0) AS Percentiles FROM INDEX-2017-12 12345678910111213141516{ \"from\" : 0, \"size\" : 0, \"_source\" : { \"includes\" : [ \"percentiles\" ], \"excludes\" : [ ] }, \"aggregations\" : { \"Percentiles\" : { \"percentiles\" : { \"field\" : \"log_date.d\", \"percents\" : [ 1.0, 15.0, 31.0 ] } } }} Unique count（基数 || 去重计数，就是 SQL 中的 distinct ）：SELECT count(distinct(log_date.d)) AS UniqueCount FROM INDEX-2017-12 12345678910111213141516{ \"from\" : 0, \"size\" : 0, \"_source\" : { \"includes\" : [ \"COUNT\" ], \"excludes\" : [ ] }, \"aggregations\" : { \"UniqueCount\" : { \"cardinality\" : { \"field\" : \"log_date.d\", \"precision_threshold\" : 40000 } } }} Median（中位数）：中位数没找到单独的获取方法，不过在 Kibana 中看到获取中位数时请求中的参数，其实就是获取的某个字段50的百分位数，所以可能有：中位数=50的百分位数SELECT percentiles(log_date.d,50.0) AS percentiles FROM INDEX-2017-12 1234567891011121314151617{ \"from\" : 0, \"size\" : 0, \"_source\" : { \"includes\" : [ \"percentiles\" ], \"excludes\" : [ ] }, \"aggregations\" : { \"percentiles\" : { \"percentiles\" : { \"field\" : \"log_date.d\", \"percents\" : [ 50.0 ] } } }}` 方差、平方和、标准差、标准差界限：这几个度量没有单独方法去获取，都是用 EXTENDED_STATS 一个请求全部获取下来，然后从中取自己需要的结果SELECT EXTENDED_STATS(log_date.d) AS EXTENDED_STATS FROM INDEX-2017-12 123456789101112131415{ \"from\" : 0, \"size\" : 0, \"_source\" : { \"includes\" : [ \"EXTENDED_STATS\" ], \"excludes\" : [ ] }, \"aggregations\" : { \"EXTENDED_STATS\" : { \"extended_stats\" : { \"field\" : \"log_date.d\" } } }} EXTENDED_STATS 查询结果包含：方差、平方和、标准差、标准差界限以及最大值、平均数等基础度量，具体如下： 12345678910111213141516\"aggregations\": { \"1\": { \"count\": 15304326, \"min\": 1, \"max\": 31, \"avg\": 15.068216202399244, \"sum\": 230608893, \"sum_of_squares\": 4588588661, \"variance\": 72.7718426201877, \"std_deviation\": 8.530641395591992, \"std_deviation_bounds\": { \"upper\": 32.129498993583226, \"lower\": -1.9930665887847407 } } } Percentile ranks（百分位等级）暂时没找到求百分位等级的 SQL 语句，只能用原生 ES 查询语句获取了；ES原生查询语句如下： 1234567891011121314151617{ \"size\": 0, ...... \"aggs\": { \"1\": { \"percentile_ranks\": { \"field\": \"log_date.d\", \"values\": [ 6, 15, 31 ], \"keyed\": false } } }}","link":"/2019/12/30/es_sql/"},{"title":"dockerfile command","text":"docker reference https://docs.docker.com/v17.09/engine/reference/builder docker build 创建镜像 -f 路径 指定用哪个dockerfile文件 创建镜像 $ docker build -f /path/to/a/Dockerfile -t 标签 给指定镜像打标签 $ docker build -t shykes/myapp 给某个镜像指定多个标签 $ docker build -t shykes/myapp:1.0.2 -t shykes/myapp:latest 如果Dockerfile中有语法错误docker 会报错 docker会根据Dockerfile 逐条执行命令 如果有必要的花docker 会在生成image ID 前 将每一条执行结果都提交到新的镜像中，守护进程自动清理发送给他的文本，注意每一条指令都是独立的无关的，因此一个新的镜像会被创建，所以运行 Run cd/tmp 不回对下条命令产生任何的影响 只要有可能docker都会复用中间镜像的缓存来加速构建，这可以通过输出台Using cache 来表示 构建缓存只用于有本地父关系链的场景，这也表示这些镜像是前面构建的时候已经创建的，或者也可用docker load 命令加载所有的父关连镜像，如果想使用指定的特定镜像缓存来构建可以用 –cache-from ，用这个命令就不需要父关系镜像 格式： 虽然大小写不敏感，但是指令用大写 可以与参数很好的区分开 必须以FROM 指令开头，它指定了基础镜像 #作为注释只可以放在开头，其他地方插入都只作为参数 解析器指令 解析器指令是可选的，并且影响Dockerfile中后续行的处理方式。解析器指令不向构建中添加层，也不会显示为构建步骤。解析器指令被编写为一种特殊类型的注释，形式为 #directive=value。 一个指令只能使用一次。一旦注释、空行或构建器指令被处理，Docker就不再寻找解析器指令。相反，它将格式化为解析器指令的任何东西都视为注释，并且不尝试验证它是否可能是解析器指令。 因此，所有解析器指令都必须位于Dockerfile的最顶端。解析器指令不区分大小写。然而，习惯上它们都是小写的。约定还包括任何解析器指令后面的空行。解析器指令不支持行延续字符。 一下指令是允许的 12345 escape# escape=\\ (backslash)Or# escape=` (backtick) 环境替换环境变量(用ENV语句声明)也可以在某些指令中用作Dockerfile要解释的变量。转义还处理将类似变量的语法按字面意思包含到语句中。 环境变量在Dockerfile中以$变量名 或 ${变量名} 标注。它们被同等对待，大括号语法通常用于处理变量名没有空格的问题，比如${foo} bar。 ${variable_name}语法还支持以下指定的几个标准bash修饰符${Z}表示，如果设置了变量，那么结果将是该值。如果没有设置变量，那么word将是结果。${variable:+word}表示如果设置了变量，那么word将是结果，否则结果是空字符串。在所有情况下，word可以是任何字符串，包括附加的环境变量。可以通过在变量前面添加\\来转义:例如，$foo或${foo}将分别转换为$foo和${foo}文本。 举个例子：（#后表示解析后的结果） 12345FROM busyboxENV foo /barWORKDIR ${foo} # WORKDIR /barADD . $foo # ADD . /barCOPY \\$foo /quux # COPY $foo /quux Dockerfile中以下环境变量是被允许的： 1234567891011ADDCOPYENVEXPOSEFROMLABELSTOPSIGNALUSERVOLUMEWORKDIRONBUILD(1.4之后支持) 环境变量替换将在整个指令中对每个变量使用相同的值 123ENV abc=helloENV abc=bye def=$abcENV ghi=$abc 将导致def的值为hello，而不是bye。但是，ghi将有一个值bye，因为它不是将abc设置为bye的同一条指令的一部分。 .dockerignore file在docker CLI将上下文发送到docker守护进程之前，它在上下文的根目录中查找一个名为.dockerignore的文件。如果该文件存在，CLI将修改上下文，以排除匹配其中模式的文件和目录。这有助于避免不必要地向守护进程发送大型或敏感的文件和目录，并可能使用ADD或COPY将它们添加到镜像中 CLI将.dockerignore文件解释为一个新行分隔的模式列表，类似于Unix shell的文件globs。为了进行匹配，上下文的根被认为是工作目录和根目录。例子： 1234# comment*/temp**/*/temp*temp? This file causes the following build behavior: Rule Behavior # comment Ignored. /temp Exclude files and directories whose names start with temp in any immediate subdirectory of the root. For example, the plain file /somedir/temporary.txt is excluded, as is the directory /somedir/temp. //temp* Exclude files and directories starting with temp from any subdirectory that is two levels below the root. For example, /somedir/subdir/temporary.txt is excluded. temp? Exclude files and directories in the root directory whose names are a one-character extension of temp. For example, /tempa and /tempb are excluded. 匹配是使用Go’s filepath完成的。匹配规则。预处理步骤删除前导和后置空格并消除.和. .使用Go’s filepath.Clean的元素。预处理后为空的行将被忽略。超越Go’s filepath。匹配规则Docker还支持一个特殊的通配符字符串**，它匹配任意数量的目录(包括0),例如 **/*.go将排除在所有目录中以.go结尾的所有文件，包括构建上下文的根目录。以!(感叹号)开头可以用来排除例外。下面是一个使用此机制的.dockerignore文件示例 12*.md!README.md 所有的标记文件被排除在上下文之外，除了README.md。!的位置异常规则影响行为:.dockerignore中匹配特定文件的最后一行决定是否包含或排除该文件。考虑下面的例子 123 *.mdREADME-secret.md!README*.md 所有的README文件都包括在内。中间的行没有效果，因为!README*.md匹配 README-secret。而且在最后一行。您甚至可以使用.dockerignore文件来排除Dockerfile和.dockerignore文件。这些文件仍然被发送到守护进程，因为守护进程需要它们来完成它的工作。但是添加和复制指令不会将它们复制到镜像中。 最后，您可能希望指定要在上下文中包含哪些文件，而不是要排除哪些文件。要实现这一点，请将*指定为第一个模式(把.md它放在前面)，然后是一个或多个模式 !异常模式（!README.md放在后面）。 FROM1234567FROM &lt;image&gt; [AS &lt;name&gt;]OrFROM &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;]OrFROM &lt;image&gt;[@&lt;digest&gt;] [AS &lt;name&gt;] FROM指令初始化一个新的构建阶段，并为后续指令设置基本镜像。因此，一个有效的Dockerfile必须从FROM指令开始。镜像可以是任何有效的镜像，这是特别容易的从公共仓库拉一个镜像开始ARG是Dockerfile中唯一可能在前面的指令。参见了解ARG和FROM如何交互。 FROM可以在一个Dockerfile中出现多次，以创建多个镜像，或者使用一个构建阶段作为另一个构建阶段的依赖项。只需在每条新的FROM指令之前，记录提交最后一个镜像ID。每个FROM指令清除以前的指令创建的任何状态。 可以通过AS名称 添加到FROM指令中，为新的构建阶段指定一个名称。可以在后续FROM和COPY——FROM =&lt;name|index&gt;指令中使用该名称，以引用此阶段构建的镜像。 tag标记或digest摘要值是可选的。如果您省略了它们中的任何一个，构建器将默认使用最新的标记。如果构建器无法找到标记值，则返回一个错误。 理解ARG和FROM之间是如何交互的 FROM指令支持由第一个FROM之前的任何ARG指令声明的变量。 123456ARG CODE_VERSION=latestFROM base:${CODE_VERSION}CMD /code/run-appFROM extras:${CODE_VERSION}CMD /code/run-extras 在FROM之前声明的ARG位于构建阶段之外，因此不能在FROM之后的任何指令中使用它。若要使用第一个FROM之前声明的ARG的默认值，请使用构建阶段中没有值的ARG指令 To use the default value of an ARG declared before the first FROM use an ARG instruction without a value inside of a build stage: 1234ARG VERSION=latestFROM busybox:$VERSIONARG VERSIONRUN echo $VERSION &gt; image_version RUNrun有两中形式： 12RUN &lt;command&gt; (shell form, the command is run in a shell, which by default is /bin/sh -c on Linux or cmd /S /C on Windows)RUN [\"executable\", \"param1\", \"param2\"] (exec form) RUN指令将在当前映像之上的新层中执行任何命令并提交结果。生成的提交映像将用于Dockerfile中的下一步。分层运行指令和生成提交符合Docker的核心概念，其中提交很便宜，可以从镜像历史中的任何位置创建容器，就像源代码控制一样。exec形式可以避免shell字符串阻塞，并使用不包含指定的shell可执行文件的基本镜像运行命令。可以使用shell命令更改shell形式的默认shell。在shell形式中，您可以使用(反斜杠)将单个运行指令延续到下一行 12RUN /bin/bash -c 'source $HOME/.bashrc; \\echo $HOME' 它们加起来等于这一行RUN /bin/bash -c 'source $HOME/.bashrc; echo $HOME'注意:与shell不同，exec不调用命令shell。这意味着不会发生正常的shell处理。例如，RUN [&quot;echo&quot;， &quot;$HOME&quot;]不会对$HOME执行变量替换。如果您想要shell处理，那么要么使用shell表单，要么直接执行shell，RUN [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; ]当使用exec并直接执行shell时(就像shell的情况一样)，执行环境变量展开的是shell，而不是docker。在JSON格式中，有必要转义反斜杠这在以反斜杠作为路径分隔符的窗口中特别相关。否则，由于不是有效的JSON，以下行将被视为shell形式，并以一种意外的方式失败:RUN [&quot;c:\\windows\\system32\\tasklist.exe&quot;]这个例子的正确语法是RUN [&quot;c:\\\\windows\\\\system32\\\\tasklist.exe&quot;]运行指令的缓存不会在下一次构建期间自动失效。像RUN apt-get distd -upgrade -y这样的指令的缓存将在下一个构建过程中重用。运行指令的缓存可以通过使用–no-cache来失效for example docker build --no-cache运行指令的缓存可以通过添加指令失效。详见下文。 CMD有三种格式： 123CMD [\"executable\",\"param1\",\"param2\"] (exec form, this is the preferred form)CMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT)CMD command param1 param2 (shell form) Dockerfile中只能有一条CMD指令。如果您列出一个以上的CMD，那么只有最后一个CMD将生效CMD的主要目的是为执行容器提供默认值。这些默认值可以包括可执行文件，也可以省略可执行文件，在这种情况下，还必须指定一个入口点指令。注意:如果CMD用于为ENTRYPOINT指令提供默认参数，那么CMD和ENTRYPOINT指令都应该使用JSON数组格式指定。注意:exec表单被解析为JSON数组，这意味着必须在单词周围使用双引号(“)，而不是单引号(‘)。与shell不同，exec不调用命令shell。这意味着不会发生正常的shell处理。例如，CMD [&quot;echo&quot;， &quot;$HOME&quot;]不会对$HOME执行变量替换。如果您想要shell处理，那么要么使用shell表单，要么直接执行shell.例如CMD [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; ] 当使用exec并直接执行shell时(就像shell的情况一样)，执行环境变量展开的是shell，而不是docker。当在shell或exec格式中使用时，CMD指令设置要在运行镜像时执行的命令。如果使用CMD的shell形式，那么&lt;命令&gt;将在/bin/sh -c中执行 12FROM ubuntuCMD echo \"This is a test.\" | wc - 如果您想在没有shell的情况下运行&lt;command&gt;，那么必须将该命令表示为JSON数组，并给出可执行文件的完整路径。这种数组形式是CMD的首选格式。任何附加参数都必须单独表示为字符串 12FROM ubuntuCMD [\"/usr/bin/wc\",\"--help\"] 如果您希望您的容器每次运行相同的可执行文件，那么您应该考虑将ENTRYPOINT与CMD结合使用。看到入口点。 如果用户指定要运行docker run的参数，那么它们将覆盖CMD中指定的默认值 注意:不要将RUN与CMD混淆。RUN实际运行命令并提交结果;CMD在构建时不执行任何操作，但是为镜像指定了预期的命令。 LABELLABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...标签指令向镜像添加元数据。标签是键值对。要在标签值中包含空格，可以像在命令行解析中那样使用引号和反斜杠。一些使用示例 12345LABEL \"com.example.vendor\"=\"ACME Incorporated\"LABEL com.example.label-with-value=\"foo\"LABEL version=\"1.0\"LABEL description=\"This text illustrates \\that label-values can span multiple lines.\" 一个镜像可以有多个标签。要指定多个标签，Docker建议在可能的情况下将标签组合成单个标签指令。每条标签指令都会生成一个新层，如果使用多个标签，则会导致图镜像效率低下LABEL multi.label1=&quot;value1&quot; multi.label2=&quot;value2&quot; other=&quot;value3&quot;也可以写成 123LABEL multi.label1=\"value1\" \\ multi.label2=\"value2\" \\ other=\"value3\" 标签是附加的，包括来自镜像的标签。要查看镜像的标签，请使用docker inspect命令。 123456789\"Labels\": { \"com.example.vendor\": \"ACME Incorporated\" \"com.example.label-with-value\": \"foo\", \"version\": \"1.0\", \"description\": \"This text illustrates that label-values can span multiple lines.\", \"multi.label1\": \"value1\", \"multi.label2\": \"value2\", \"other\": \"value3\"}, MAINTAINER (deprecated) 过时MAINTAINER &lt;name&gt;LABEL maintainer=&quot;SvenDowideit@home.org.au&quot;EXPOSEEXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt;...]ENV 12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key&gt;=&lt;value&gt; ... ENV指令将环境变量&lt;key&gt;设置为值&lt;value&gt;。此值将位于所有Dockerfile后代命令的环境中，并且可以在许多命令中内联替换。 ENV指令有两种形式。第一种形式ENV &lt;key&gt; &lt;value&gt;将把单个变量设置为一个值。第一个空格之后的整个字符串将被视为&lt;value&gt;—包括空格和引号等字符。 第二种形式是ENV &lt;key&gt;=&lt;value&gt;…，允许一次设置多个变量。注意，第二种形式在语法中使用等号(=)，而第一种形式没有。与命令行解析类似，可以使用引号和反斜杠在值中包含空格。 12ENV myName=\"John Doe\" myDog=Rex\\ The\\ Dog \\ myCat=fluffy AND 123ENV myName John DoeENV myDog Rex The DogENV myCat fluffy 将在最终镜像中生成相同的净结果，但首选第一种形式，因为它生成一个缓存层。 当容器从生成的映像运行时，使用ENV设置的环境变量将保持不变。您可以使用docker inspect查看这些值，并使用docker run——env &lt;key&gt;=&lt;value&gt;更改它们。 ADDADD有两种形式： 12ADD &lt;src&gt;... &lt;dest&gt;ADD [\"&lt;src&gt;\",... \"&lt;dest&gt;\"] (this form is required for paths containing whitespace) ADD指令从&lt;src&gt;复制新的文件、目录或远程文件url，并将它们添加到路径&lt;dest&gt;的映像文件系统中可以指定多个&lt;src&gt;资源，但如果它们是文件或目录，则必须相对于正在构建的源目录(构建的上下文)。每个&lt;src&gt;可能包含通配符，匹配将使用Go’s filepath完成。匹配规则。例如 12345ADD hom* /mydir/ # adds all files starting with \"hom\"ADD hom?.txt /mydir/ # ? is replaced with any single character, e.g., \"home.txt\"``` `&lt;dest&gt;`是绝对路径，或相对于WORKDIR的路径，源文件将复制到目标容器中。 ADD test relativeDir/ # adds “test” to WORKDIR/relativeDir/ADD test /absoluteDir/ # adds “test” to /absoluteDir/ 12345678当添加包含特殊字符(如`[and]`)的文件或目录时，需要转义那些遵循Golang规则的路径，以防止将它们视为匹配模式。例如，添加一个名为`arr[0].txt`的文件。 `ADD arr[[]0].txt /mydir/ # copy a file named \"arr[0].txt\" to /mydir/`所有新文件和目录都是用UID和GID为0创建的。在`&lt;src&gt;`是远程文件URL的情况下，目标的权限将为600。如果正在检索的远程文件具有HTTP Last-Modified头，则来自该头的时间戳将用于设置目标文件的时间。然而,与添加过程中处理的任何其他文件一样，在确定文件是否更改、缓存是否应该更新时，并不包括mtime。COPYCOPY 有两种形式: COPY … COPY [““,… ““] (this form is required for paths containing whitespace) 12 每个`&lt;src&gt;`可能包含通配符，匹配将使用Go's filepath完成。匹配规则。例如 COPY hom* /mydir/ # adds all files starting with “hom”COPY hom?.txt /mydir/ # ? is replaced with any single character, e.g., “home.txt” 1`&lt;dest&gt;`是绝对路径，或相对于WORKDIR的路径，源文件将复制到目标容器中。 COPY test relativeDir/ # adds “test” to WORKDIR/relativeDir/COPY test /absoluteDir/ # adds “test” to /absoluteDir/ 12ENTRYPOINTENTRYPOINT 有两种形式: ENTRYPOINT [“executable”, “param1”, “param2”] (exec form, preferred)ENTRYPOINT command param1 param2 (shell form) 1234567入口点允许您配置容器作为可执行文件运行。`docker run -i -t --rm -p 80:80 nginx`VOLUME`VOLUME [\"/data\"]`卷指令创建具有指定名称的挂载点，并将其标记为持有来自本机主机或其他容器的外部挂载卷。值可以是个json数组`VOLUME [\"/var/log/\"]`,或具有多个参数的纯字符串，例如`VOLUME /var/log`或者`VOLUME /var/log /var/db`docker run命令使用存在于基本映像中指定位置的任何数据初始化新创建的卷。例如，考虑下面的Dockerfile片段 FROM ubuntuRUN mkdir /myvolRUN echo “hello world” &gt; /myvol/greetingVOLUME /myvol 12USER USER [:] orUSER [:] 1234567用户指令设置用户名(或UID)和可选的用户组(或GID)，以便在运行镜像时使用，以及在Dockerfile中对其后面的任何运行、CMD和ENTRYPOINT指令进行设置。警告:当用户没有主组时，映像(或下一个指令)将与根组一起运行。WORKDIR`WORKDIR /path/to/workdir`The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn’t exist, it will be created even if it’s not used in any subsequent Dockerfile instruction.The WORKDIR instruction can be used multiple times in a Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example: WORKDIR /aWORKDIR bWORKDIR cRUN pwd 123The output of the final pwd command in this Dockerfile would be /a/b/c.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example: ENV DIRPATH /pathWORKDIR $DIRPATH/$DIRNAMERUN pwd 12345678The output of the final pwd command in this Dockerfile would be /path/$DIRNAMEARG`ARG &lt;name&gt;[=&lt;default value&gt;]`The ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg &lt;varname&gt;=&lt;value&gt; flag. If a user specifies a build argument that was not defined in the Dockerfile, the build outputs a warning.[Warning] One or more build-args [foo] were not consumed.A Dockerfile may include one or more ARG instructions. For example, the following is a valid Dockerfile: FROM busyboxARG user1ARG buildno… 1234Warning: It is not recommended to use build-time variables for passing secrets like github keys, user credentials etc. Build-time variable values are visible to any user of the image with the docker history command.Default valuesAn ARG instruction can optionally include a default value: FROM busyboxARG user1=someuserARG buildno=1… 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141If an ARG instruction has a default value and if there is no value passed at build-time, the builder uses the default.ScopeAn ARG variable definition comes into effect from the line on which it is defined in the Dockerfile not from the argument’s use on the command-line or elsewhere. For example, consider this Dockerfile:1 FROM busybox2 USER ${user:-some_user}3 ARG user4 USER $user...A user builds this file by calling:$ docker build --build-arg user=what_user .The USER at line 2 evaluates to some_user as the user variable is defined on the subsequent line 3. The USER at line 4 evaluates to what_user as user is defined and the what_user value was passed on the command line. Prior to its definition by an ARG instruction, any use of a variable results in an empty string.An ARG instruction goes out of scope at the end of the build stage where it was defined. To use an arg in multiple stages, each stage must include the ARG instruction.FROM busyboxARG SETTINGSRUN ./run/setup $SETTINGSFROM busyboxARG SETTINGSRUN ./run/other $SETTINGSUsing ARG variablesYou can use an ARG or an ENV instruction to specify variables that are available to the RUN instruction. Environment variables defined using the ENV instruction always override an ARG instruction of the same name. Consider this Dockerfile with an ENV and ARG instruction.1 FROM ubuntu2 ARG CONT_IMG_VER3 ENV CONT_IMG_VER v1.0.04 RUN echo $CONT_IMG_VERThen, assume this image is built with this command:$ docker build --build-arg CONT_IMG_VER=v2.0.1 .In this case, the RUN instruction uses v1.0.0 instead of the ARG setting passed by the user:v2.0.1 This behavior is similar to a shell script where a locally scoped variable overrides the variables passed as arguments or inherited from environment, from its point of definition.Using the example above but a different ENV specification you can create more useful interactions between ARG and ENV instructions:1 FROM ubuntu2 ARG CONT_IMG_VER3 ENV CONT_IMG_VER ${CONT_IMG_VER:-v1.0.0}4 RUN echo $CONT_IMG_VERUnlike an ARG instruction, ENV values are always persisted in the built image. Consider a docker build without the --build-arg flag:$ docker build .Using this Dockerfile example, CONT_IMG_VER is still persisted in the image but its value would be v1.0.0 as it is the default set in line 3 by the ENV instruction.The variable expansion technique in this example allows you to pass arguments from the command line and persist them in the final image by leveraging the ENV instruction. Variable expansion is only supported for a limited set of Dockerfile instructions.Predefined ARGsDocker has a set of predefined ARG variables that you can use without a corresponding ARG instruction in the Dockerfile.HTTP_PROXYhttp_proxyHTTPS_PROXYhttps_proxyFTP_PROXYftp_proxyNO_PROXYno_proxyTo use these, simply pass them on the command line using the flag:--build-arg &lt;varname&gt;=&lt;value&gt;By default, these pre-defined variables are excluded from the output of docker history. Excluding them reduces the risk of accidentally leaking sensitive authentication information in an HTTP_PROXY variable.For example, consider building the following Dockerfile using --build-arg HTTP_PROXY=http://user:pass@proxy.lon.example.comFROM ubuntuRUN echo \"Hello World\"In this case, the value of the HTTP_PROXY variable is not available in the docker history and is not cached. If you were to change location, and your proxy server changed to http://user:pass@proxy.sfo.example.com, a subsequent build does not result in a cache miss.If you need to override this behaviour then you may do so by adding an ARG statement in the Dockerfile as follows:FROM ubuntuARG HTTP_PROXYRUN echo \"Hello World\"When building this Dockerfile, the HTTP_PROXY is preserved in the docker history, and changing its value invalidates the build cache.Impact on build cachingARG variables are not persisted into the built image as ENV variables are. However, ARG variables do impact the build cache in similar ways. If a Dockerfile defines an ARG variable whose value is different from a previous build, then a “cache miss” occurs upon its first usage, not its definition. In particular, all RUN instructions following an ARG instruction use the ARG variable implicitly (as an environment variable), thus can cause a cache miss. All predefined ARG variables are exempt from caching unless there is a matching ARG statement in the Dockerfile.For example, consider these two Dockerfile:1 FROM ubuntu2 ARG CONT_IMG_VER3 RUN echo $CONT_IMG_VER1 FROM ubuntu2 ARG CONT_IMG_VER3 RUN echo helloIf you specify --build-arg CONT_IMG_VER=&lt;value&gt; on the command line, in both cases, the specification on line 2 does not cause a cache miss; line 3 does cause a cache miss.ARG CONT_IMG_VER causes the RUN line to be identified as the same as running CONT_IMG_VER=&lt;value&gt; echo hello, so if the &lt;value&gt; changes, we get a cache miss.Consider another example under the same command line:1 FROM ubuntu2 ARG CONT_IMG_VER3 ENV CONT_IMG_VER $CONT_IMG_VER4 RUN echo $CONT_IMG_VERIn this example, the cache miss occurs on line 3. The miss happens because the variable’s value in the ENV references the ARG variable and that variable is changed through the command line. In this example, the ENV command causes the image to include the value.If an ENV instruction overrides an ARG instruction of the same name, like this Dockerfile:1 FROM ubuntu2 ARG CONT_IMG_VER3 ENV CONT_IMG_VER hello4 RUN echo $CONT_IMG_VERLine 3 does not cause a cache miss because the value of CONT_IMG_VER is a constant (hello). As a result, the environment variables and values used on the RUN (line 4) doesn’t change between builds.ONBUILD`ONBUILD [INSTRUCTION]`The ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the FROM instruction in the downstream Dockerfile.Any build instruction can be registered as a trigger.This is useful if you are building an image which will be used as a base to build other images, for example an application build environment or a daemon which may be customized with user-specific configuration.For example, if your image is a reusable Python application builder, it will require application source code to be added in a particular directory, and it might require a build script to be called after that. You can’t just call ADD and RUN now, because you don’t yet have access to the application source code, and it will be different for each application build. You could simply provide application developers with a boilerplate Dockerfile to copy-paste into their application, but that is inefficient, error-prone and difficult to update because it mixes with application-specific code.The solution is to use ONBUILD to register advance instructions to run later, during the next build stage.Here’s how it works:When it encounters an ONBUILD instruction, the builder adds a trigger to the metadata of the image being built. The instruction does not otherwise affect the current build.At the end of the build, a list of all triggers is stored in the image manifest, under the key OnBuild. They can be inspected with the docker inspect command.Later the image may be used as a base for a new build, using the FROM instruction. As part of processing the FROM instruction, the downstream builder looks for ONBUILD triggers, and executes them in the same order they were registered. If any of the triggers fail, the FROM instruction is aborted which in turn causes the build to fail. If all triggers succeed, the FROM instruction completes and the build continues as usual.Triggers are cleared from the final image after being executed. In other words they are not inherited by “grand-children” builds.For example you might add something like this:[...]ONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build --dir /app/src[...]Warning: Chaining ONBUILD instructions using ONBUILD ONBUILD isn’t allowed.Warning: The ONBUILD instruction may not trigger FROM or MAINTAINER instructions.STOPSIGNALSTOPSIGNAL signalThe STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit. This signal can be a valid unsigned number that matches a position in the kernel’s syscall table, for instance 9, or a signal name in the format SIGNAME, for instance SIGKILL.HEALTHCHECKThe HEALTHCHECK instruction has two forms: HEALTHCHECK [OPTIONS] CMD command (check container health by running a command inside the container)HEALTHCHECK NONE (disable any healthcheck inherited from the base image) 12345The HEALTHCHECK instruction tells Docker how to test a container to check that it is still working. This can detect cases such as a web server that is stuck in an infinite loop and unable to handle new connections, even though the server process is still running.When a container has a healthcheck specified, it has a health status in addition to its normal status. This status is initially starting. Whenever a health check passes, it becomes healthy (whatever state it was previously in). After a certain number of consecutive failures, it becomes unhealthy.The options that can appear before CMD are: –interval=DURATION (default: 30s)–timeout=DURATION (default: 30s)–start-period=DURATION (default: 0s)–retries=N (default: 3) 12345678910111213The health check will first run interval seconds after the container is started, and then again interval seconds after each previous check completes.If a single run of the check takes longer than timeout seconds then the check is considered to have failed.It takes retries consecutive failures of the health check for the container to be considered unhealthy.start period provides initialization time for containers that need time to bootstrap. Probe failure during that period will not be counted towards the maximum number of retries. However, if a health check succeeds during the start period, the container is considered started and all consecutive failures will be counted towards the maximum number of retries.There can only be one HEALTHCHECK instruction in a Dockerfile. If you list more than one then only the last HEALTHCHECK will take effect.The command after the CMD keyword can be either a shell command (e.g. HEALTHCHECK CMD /bin/check-running) or an exec array (as with other Dockerfile commands; see e.g. ENTRYPOINT for details).The command’s exit status indicates the health status of the container. The possible values are: 0: success - the container is healthy and ready for use1: unhealthy - the container is not working correctly2: reserved - do not use this exit code 12345678910111213141516For example, to check every five minutes or so that a web-server is able to serve the site’s main page within three seconds:`HEALTHCHECK --interval=5m --timeout=3s \\ CMD curl -f http://localhost/ || exit 1`To help debug failing probes, any output text (UTF-8 encoded) that the command writes on stdout or stderr will be stored in the health status and can be queried with docker inspect. Such output should be kept short (only the first 4096 bytes are stored currently).When the health status of a container changes, a health_status event is generated with the new status.The HEALTHCHECK feature was added in Docker 1.12.SHELL`SHELL [\"executable\", \"parameters\"]`The SHELL instruction allows the default shell used for the shell form of commands to be overridden. The default shell on Linux is` [\"/bin/sh\", \"-c\"]`, and on Windows is `[\"cmd\", \"/S\", \"/C\"]`. The SHELL instruction must be written in JSON form in a Dockerfile.The SHELL instruction is particularly useful on Windows where there are two commonly used and quite different native shells: cmd and powershell, as well as alternate shells available including sh.The SHELL instruction can appear multiple times. Each SHELL instruction overrides all previous SHELL instructions, and affects all subsequent instructions. For example: FROM microsoft/windowsservercore Executed as cmd /S /C echo defaultRUN echo default Executed as cmd /S /C powershell -command Write-Host defaultRUN powershell -command Write-Host default Executed as powershell -command Write-Host helloSHELL [“powershell”, “-command”]RUN Write-Host hello Executed as cmd /S /C echo helloSHELL [“cmd”, “/S””, “/C”]RUN echo hello 123The following instructions can be affected by the SHELL instruction when the shell form of them is used in a Dockerfile: RUN, CMD and ENTRYPOINT.The following example is a common pattern found on Windows which can be streamlined by using the SHELL instruction: …RUN powershell -command Execute-MyCmdlet -param1 “c:\\foo.txt”… 123456The command invoked by docker will be:`cmd /S /C powershell -command Execute-MyCmdlet -param1 \"c:\\foo.txt\"`This is inefficient for two reasons. First, there is an un-necessary cmd.exe command processor (aka shell) being invoked. Second, each RUN instruction in the shell form requires an extra powershell -command prefixing the command.To make this more efficient, one of two mechanisms can be employed. One is to use the JSON form of the RUN command such as: …RUN [“powershell”, “-command”, “Execute-MyCmdlet”, “-param1 &quot;c:\\foo.txt&quot;“]… 1While the JSON form is unambiguous and does not use the un-necessary cmd.exe, it does require more verbosity through double-quoting and escaping. The alternate mechanism is to use the SHELL instruction and the shell form, making a more natural syntax for Windows users, especially when combined with the escape parser directive: escape=`FROM microsoft/nanoserverSHELL [“powershell”,”-command”]RUN New-Item -ItemType Directory C:\\ExampleADD Execute-MyCmdlet.ps1 c:\\exampleRUN c:\\example\\Execute-MyCmdlet -sample ‘hello world’ 1Resulting in: PS E:\\docker\\build\\shell&gt; docker build -t shell .Sending build context to Docker daemon 4.096 kBStep 1/5 : FROM microsoft/nanoserver —&gt; 22738ff49c6dStep 2/5 : SHELL powershell -command —&gt; Running in 6fcdb6855ae2 —&gt; 6331462d4300Removing intermediate container 6fcdb6855ae2Step 3/5 : RUN New-Item -ItemType Directory C:\\Example —&gt; Running in d0eef8386e97 Directory: C:\\Mode LastWriteTime Length Name d—– 10/28/2016 11:26 AM Example —&gt; 3f2fbf1395d9Removing intermediate container d0eef8386e97Step 4/5 : ADD Execute-MyCmdlet.ps1 c:\\example —&gt; a955b2621c31Removing intermediate container b825593d39fcStep 5/5 : RUN c:\\example\\Execute-MyCmdlet ‘hello world’ —&gt; Running in be6d8e63fe75hello world —&gt; 8e559e9bf424Removing intermediate container be6d8e63fe75Successfully built 8e559e9bf424PS E:\\docker\\build\\shell&gt; 12345678The SHELL instruction could also be used to modify the way in which a shell operates. For example, using SHELL cmd /S /C /V:ON|OFF on Windows, delayed environment variable expansion semantics could be modified.The SHELL instruction can also be used on Linux should an alternate shell be required such as zsh, csh, tcsh and others.The SHELL feature was added in Docker 1.12.Dockerfile examplesBelow you can see some examples of Dockerfile syntax. If you’re interested in something more realistic, take a look at the list of Dockerization examples. Nginx# VERSION 0.0.1FROM ubuntuLABEL Description=”This image is used to start the foobar executable” Vendor=”ACME Products” Version=”1.0”RUN apt-get update &amp;&amp; apt-get install -y inotify-tools nginx apache2 openssh-server 1234567891011121314151617```# Firefox over VNC## VERSION 0.3FROM ubuntu# Install vnc, xvfb in order to create a 'fake' display and firefoxRUN apt-get update &amp;&amp; apt-get install -y x11vnc xvfb firefoxRUN mkdir ~/.vnc# Setup a passwordRUN x11vnc -storepasswd 1234 ~/.vnc/passwd# Autostart firefox (might not be the best way, but it does the trick)RUN bash -c 'echo \"firefox\" &gt;&gt; /.bashrc'EXPOSE 5900CMD [\"x11vnc\", \"-forever\", \"-usepw\", \"-create\"] 1234567891011121314# Multiple images example## VERSION 0.1FROM ubuntuRUN echo foo &gt; bar# Will output something like ===&gt; 907ad6c2736fFROM ubuntuRUN echo moo &gt; oink# Will output something like ===&gt; 695d7793cbe4# You'll now have two images, 907ad6c2736f with /bar, and 695d7793cbe4 with# /oink.","link":"/2019/07/19/dockerfile-command/"},{"title":"文档标题","text":"HISTOGRAMeditSynopsis: 123HISTOGRAM( numeric_exp, numeric_interval) 123HISTOGRAM( date_exp, date_time_interval) Input: numeric expression (typically a field) numeric interval date/time expression (typically a field) date/time interval Output: non-empty buckets or groups of the given expression divided according to the given interval DescriptionThe histogram function takes all matching values and divides them into buckets with fixed size matching the given interval, using (roughly) the following formula: bucket_key = Math.floor(value / interval) * intervalNOTEThe histogram in SQL does NOT return empty buckets for missing intervals as the traditional histogram and date histogram. Such behavior does not fit conceptually in SQL which treats all missing values as NULL; as such the histogram places all missing values in the NULL group.Histogram can be applied on either numeric fields: SELECT HISTOGRAM(salary, 5000) AS h FROM emp GROUP BY h; 123456789101112 h---------------25000300003500040000450005000055000600006500070000 or date/time fields: SELECT HISTOGRAM(birth_date, INTERVAL 1 YEAR) AS h, COUNT(*) AS c FROM emp GROUP BY h; 1234567891011121314151617 h | c------------------------+---------------null |101952-01-01T00:00:00.000Z|81953-01-01T00:00:00.000Z|111954-01-01T00:00:00.000Z|81955-01-01T00:00:00.000Z|41956-01-01T00:00:00.000Z|51957-01-01T00:00:00.000Z|41958-01-01T00:00:00.000Z|71959-01-01T00:00:00.000Z|91960-01-01T00:00:00.000Z|81961-01-01T00:00:00.000Z|81962-01-01T00:00:00.000Z|61963-01-01T00:00:00.000Z|71964-01-01T00:00:00.000Z|41965-01-01T00:00:00.000Z|1 Expressions inside the histogram are also supported as long as the return type is numeric: SELECT HISTOGRAM(salary % 100, 10) AS h, COUNT(*) AS c FROM emp GROUP BY h; 12345678910111213 h | c---------------+---------------0 |1010 |1520 |1030 |1440 |950 |960 |870 |1380 |390 |9 Do note that histograms (and grouping functions in general) allow custom expressions but cannot have any functions applied to them in the GROUP BY. In other words, the following statement is NOT allowed: SELECT MONTH(HISTOGRAM(birth_date), 2)) AS h, COUNT(*) as c FROM emp GROUP BY h ORDER BY h DESC;as it requires two groupings (one for histogram followed by a second for applying the function on top of the histogram groups). Instead one can rewrite the query to move the expression on the histogram inside of it: SELECT HISTOGRAM(MONTH(birth_date), 2) AS h, COUNT(*) as c FROM emp GROUP BY h ORDER BY h DESC; 1234567891011 h | c---------------+---------------12 |710 |178 |166 |164 |182 |100 |6null |10","link":"/2019/12/31/es_sql聚合/"},{"title":"es 使用SQL查询","text":"可视化项目中使用esagent代理用sql查询使用SQL查询的语句例子： 12345678select avg(response_time) AS `metric_1`, max(response_time) AS `metric_2`, min(response_time) AS `metric_3` from `res*` where @timestamp &gt;= `2019-06-12T10:53:47+08:00` AND @timestamp &lt;= `2019-06-12T11:08:47+08:00` GROUP BY date_histogram(`@timestamp`,1d) AS `bucket_1` LIMIT 5","link":"/2019/06/22/essql/"},{"title":"es安装问题","text":"1 max file descriptors [65535] for elasticsearch process likely too low, increase to at least [65536]vim /etc/security/limits.conf 添加如下内容: elasticsearch - nofile 655362 elasticsearch dead but subsys lockedrm /var/lock/subsys/elasticsearch3 system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own riskcentos6 不支持 bootstrap.system_call_filter 修改elasticsearch.yml设置bootstrap.system_call_filter: false4 安装X-pack证书curl -XPUT -u elastic &apos;http://&lt;host&gt;:&lt;port&gt;/_xpack/license&apos; -H &quot;Content-Type: application/json&quot; -d @license.json5 max number of threads [1024] for user [elasticsearch] is too low, increase to at least [2048]vim /etc/security/limits.conf 添加如下内容: elasticsearch - nproc 20486 error: “net.bridge.bridge-nf-call-ip6tables” is an unknown key先执行 modprobe bridge7 memory locking requested for elasticsearch process but memory is not lockedvim /etc/security/limits.conf 添加如下内容 elasticsearch soft memlock unlimited elasticsearch hard memlock unlimited8 如果怎么也连接不上主节点修改防火墙配置或者关闭防火墙 service iptables stop9 detected index data in default.path.data [/var/lib/elasticsearch] where there should not be any这是因为在配置自定义datapath前启动了es, 确认之前的启动是误启动，删除留下的文件即可 rm -rf /var/lib/elasticsearch/nodes","link":"/2020/03/01/es安装问题/"},{"title":"es模糊匹配","text":"Elasticsearch查询（4）-模糊匹配和正则表达式 模糊查询前缀开始字符j 12345678GET /mydb/_search{ \"query\": { \"prefix\": { \"name\": \"j\" } } } 模糊查询 ?用来匹配1个任意字符，*用来匹配零个或者多个字符 123456789GET /mydb/_search {\"query\": {\"wildcard\": { \"postcode\": \"c??\"} } } 模糊查询 ?用来匹配1个任意字符，*用来匹配零个或者多个字符 1234567GET /mydb/_search{\"query\": { \"wildcard\": {\"name.keyword\": \"ch??g*y*\" } } 正则表达式 匹配c后面2个任意字符 123456789GET /mydb/_search { \"query\": {\"regexp\": {\"name.keyword\": \"c[a-z]{1,2}\"}} }","link":"/2019/10/24/es模糊匹配/"},{"title":"excel","text":"Excel打印区域怎么设置，超出一页如何全部打印，批量设置页边距 电脑技术角 发布时间：18-08-0923:59在 Excel 中，可以设置打印区域，一次既可以设置一个打印区域，也可以同时设置多个打印区域。若打印区域超出一页，也可以把超出部分缩小到一页中全部打印出来。另外，当进行了打印或页面布局相关设置，会在表格右边显示一条虚线作为分页线，这条虚线如果不想显示，也可以把它去掉。打印时，若一页内容较多，可以把页边距调小，并且一次可以把所有工作表的页边距调小；当然，内容少也可以调大。以下是Excel打印区域怎么设置、超出一页如何全部打印、表格的分页虚线怎么去掉和批量设置页边距的具体操作方法，操作中所用版本为 Excel 2016。 一、Excel打印区域怎么设置 1、框选要打印的区域，选择“页面布局”选项卡，单击“打印区域”，在弹出的菜单中选择“设置打印区域”，则框选区域被加上一条灰黑色的线框；依次选择“文件”→ 打印，打印预览中只有框选的部分，操作过程步骤，如图1所示： 提示：用快捷键 Alt + P + R + S 也可以设置打印区域，按键方法为：选中要打印的区域，按住 Alt，按一次 P，按一次 R，按一次 S，按先后顺序按。 2、取消打印区域。再次单击“打印区域”，在弹出的菜单中选择“取消打印区域”，则打印区域被取消，框住打印区域的灰色线框随即消失，操作过程步骤，如图2所示： 提示：用快捷键 Alt + P + R + C 取消打印区域。按键方法与用快捷键设置打印区域一样，按住 Alt，分别按 P、R、C 一次，注意按先后顺序按。 3、添加到打印区。框选一个打印区域后，还可以继续添加打印区域，方法如下： A、框选要添加的打印区，单击“打印区域”，在弹出的菜单中选择“添加到打印区域”（或者按快捷键 Alt + P + R + A，按键方法为：按住 Alt，分别按 P、R、A 一次），如图3所示： B、则框选区域也被一个灰线框框住，表示已经添加到打印区，如图4所示： C、在打印预览中，第一次设置为打印区域的部分显示在第一页，添加到打印区域的部分显示到第二页之后，例如刚才添加到打印区域的部分显示在第2页，如图5所示： 二、Excel打印区域超出一页怎么缩到一页打印 1、假如有一个 Excel 工作表超出一页，如果要打印，超出的部分（即坚虚线右边的部分）将被移到下一页打印，这样表格就会断裂而不完整，如图6所示： 图6下图是打印预览，从图中可以看出，打印预览的第一页没有坚虚线右边的部分，即“管理”列，它被移到了下一页。 2、如果要求把超出一页的部分放到一页打印，就要通过设置打印区实现。设置方法为： A、选择“布面布局”选项卡，单击“打印标题”，打开“页面设置”窗口，选择“页面”选项卡，选择“缩放”下面的“调整为”，值设置为 1，即 1 页宽 1 页高，单击“确定”，则超出页面的部分会被缩小到一页内打印（见下面的打印预览），操作过程步骤，如图7所示： B、看打印预览。依次选择“文件”→ 打印，则坚虚线右边的“管理”列也在打印预览中，说明设置成功，如图8所示： 三、Excel表格的分页虚线怎么去掉 1、右键功能区任意处，在弹出的菜单中选择“自定义功能区”，打开“Excel选项”窗口，选择左边的“高级”，往下拖右边的滑块直到找到“显示分页符”，单击它把其前面的勾去掉，单击“确定”，则表格分页坚虚线和横虚线都去掉，操作过程步骤，如图9所示： 四、Excel打印区域页边距设置 （一）Excel怎么批量设置多个工作表的打印区域页边距 1、右键 Excel 窗口左下角“学生表”，在弹出的菜单中选择“选定全部工作表”；选择“页面布局”选项卡，单击“页边距”，在弹出的菜单中选择“自定义页边距”，打开“页面设置”窗口，选择“页边距”选项卡，把上下页边距设置为 1.6，左右页边距设置为 1.2，单击“确定”，则当打开的 Excel 文档所有工作表的页边距都变为设置值，操作过程步骤，如图10所示： 2、演示的最后切换到“服装表短”工作表，查看它的页边距，上下为 1.6 厘米，左右为 1.2 厘米，正是刚才设置的值。 （二）在“打印预览”调整页边距 1、依次选择“文件”→ 打印，打开“打印预览”窗口，单击窗口右下角的“显示边距”图标，则打印预览页出现调整页边距的灰色线框，把鼠标移到左边距线上，鼠标随即变为带箭头的十字架，按住左键并往左拖，则页边距被缩小，同样方法调整其它页边距，操作过程步骤，如图11所示： 2、调整好后，再次单击“显示边距”图标，则打印预览中就不会再显示页边距调整边框。","link":"/2019/08/09/excel/"},{"title":"es查询代理sql用法","text":"一、图表数据接口参数格式说明 12345678{ \"datasource\": \"`res-*`\", \"metrics\": [], \"buckets\": [], \"timeRange\": { from : \"\", to: \"\", now: \"\" }, \"queryString\": {}, \"filters\": []} 注意事项: 前端传递的所有字段名、数据源名称、别名都需要使用``引起来。 所有需要传别名的位置，前端默认添加别名字段，默认值：函数名 + “_” + 参数字段 二、图表数据接口参数格式说明 1234567891011121. datasource: \"`res-*`\" ==&gt; FROM2. metrics: [{ ==&gt; SELECT func: \"count\", params: [* || `a` || UNIQUE `@timestamp`] as: \"aliasA\" }, { func: \"cardinality\", params: [`a`] alias: \"aliasA\" }, ......] metrics中params允许接收多个字段，但是只有func为空时有效；count、cardinality等函数时默认取第一个值。 1234567891011121314151617183. buckets: [ ==&gt; GROUP BY { func: \"date_histogram\", filed: \"@timestamp\", range: [12d, 0d], alias: \"per12day\", orderBy: \"\", ==&gt; orderBy字段为metrics中的字段别名(alias) limit: 5 }, { func: \"date_histogram\" filed: \"@timestamp\", range: [12d, 0d], alias: \"per12day\" orderBy: \"\", limit: 5 }, ......] 14. timeRange: { from : \"\", to: \"\", now: \"\" } ==&gt; WHERE 1235. queryString: { type: \"sql/dsl\", value: \"\" }当使用dsl时，使用如下内容：\"status:200 AND extension:PHP\", ==&gt; WHERE( query_string(`QueryString`))例如：select * from `res-*` where query_string(`bytes:3133 AND bytes_in:1387`) limit 100 123456789101112131415161718192021226. filters: {alias: \"\",filters: [{ ==&gt; WHERE field: \"@timestamp\", operator: \"\", opValue: [], alias: \"2\"}, { field: \"@timestamp\", operator: \"\", opValue: [], alias: \"2\"},......]}应支持：range（&lt;、&lt;=、&gt;、&gt;=）、term（=）、must_not + term（!=）、terms（in）、wildcard（like/unlike）对应operators：is between/is not between、is/is not、is one of/is not one of、exists/does not exists注意： 1）第6项中，只有当为范围和terms时，opValue中存在多个值； 2）范围值判断，必须有两个值，第一个为from，第二个为to。 3）以上需要别名的时，请前端创建参数时指定，默认为函数名_字段名，若用户指定了标签，则需要结果返回后，页面中替换为用户指定标签。 三、ESAgent模块提供SQL支持的函数及功能SELECT 所有字段 * 指定字段 field1 as f1, field2 as f2, field3 as f3, ‘field4’ –一般不建议使用字符串作为字段名。 脚本字段 select bytes_in, bytes_in + bytes_out &gt; 1000 AS gt1000, bytes from res-* 函数： count(name) - 统计文档或值的个数 参数： * 统计文档个数 标识符（name) name为桶时，返回每个桶内文档个数；name不为桶时，表示为值统计，等同于value_count()函数。 UNIQUE name 唯一值（势）统计，等同于函数cardinality()。 cardinality(name) - 唯一值（势）统计，参数为：字段name value_count(name) - 值统计，参数为：字段name max(name) - 最大值 min(name) - 最小值 sum(name) - 和 avg(name) - 均值 sum_of_squares(name) - 平方和 variance(name) - 方差 stdev(name) - 标准差 stdev_upper(name) - 标准差上限 stdev_lower(name) - 标准差下限 len(name) - 指定字段值的长度 script(scripts) - 该函数将给定的参数直接当做脚本交由ES处理，默认为painless脚本。 参数：scripts - 脚本字段 示例：select script(`doc[&quot;bytes_in&quot;].value + doc[&quot;bytes_out&quot;].value`) as c from `res-*` 注意：1）脚本字段中所有请使用双引号。 2）脚本字段作为script函数的参数时，使用``符号引起来。1234567891011121314151617181920212223242526272829303132333435363738FROM \"res-*\"，'res-*'，`res-*`WHERE``` last(interval) - 该函数用于设置”过去一段时间“的时间范围限制条件，过去是相对于当前时间而言。如果RDL中设置了全局变量”__now__“时，函数last()读取的当前时间为__now__所指定的时间，否则其读取的当前时间为系统当前时间。 参数：interval - 时间间隔，支持天(d)、小时(h)、分钟(m)、秒(s)，不支持所列单位以外的单位。 last_days(days, interval) - 该函数设置类似于“按天环比”的时间范围条件。 参数: days - 整数类型，指定“环比”的天数。 interval - 时间间隔，支持天(d)、小时(h)、分钟(m)、秒(s)，不支持所列单位以外的单位。 示例：SELECT avg(bytes) AS avg FROM 'res-*' WHERE last_days(30, 1m) last_workdays(days, interval) - 该函数提供搞得功能与last_days()类似，不同的是，last_workdays()函数将跳过用户在__workdays__全局变量以外的日期。 参数: days - 整数类型，指定“环比”的天数。 interval - 时间间隔，支持天(d)、小时(h)、分钟(m)、秒(s)，不支持所列单位以外的单位。 last_weeks(weeks, interval) - 该函数提供另一个时间环比模型，按周环比。 参数: days - 整数类型，指定“环比”的周数。 interval - 时间间隔，支持天(d)、小时(h)、分钟(m)、秒(s)，不支持所列单位以外的单位。 ip_range(field, range[, range ....]) - 该函数提供对IP地址进行过滤的功能。 参数：field - 指定IP地址字段名。 range - IP地址范围，支持格式有：\"192.168.0.0/24\"、\"192.168.0.0 TO 192.168.0.255\"、[\"192.168.0.0\", \"192.168.0.255\"] 示例： SELECT avg(bytes) AS byte FROM `res-*` WHERE last(30d) AND ip_range(client_ip, [`192.168.32.0`, `192.168.32.255`]) GROUP BY client_ip LIMIT 10 SELECT avg(bytes) AS byte FROM `res-*` WHERE last(30d) AND ip_range(client_ip, `192.168.32.0/24`) GROUP BY client_ip LIMIT 10 SELECT avg(bytes) AS byte FROM `res-*` WHERE last(30d) AND ip_range(client_ip, `192.168.32.0 TO 192.168.32.255`) GROUP BY client_ip LIMIT 10 ip_ranges(field, range_list[, range_list ....]) - 该函数提供IP地址进行过滤的功能，与ip_range()函数功能类似，但是参数不一致。 参数：field - 指定IP地址字段名。 range_list - IP地址范围列表，该列表每一个元素为一个IP地址范围，IP地址范围支持格式有：\"192.168.0.0/24\"、\"192.168.0.0 TO 192.168.0.255\"、[\"192.168.0.0\", \"192.168.0.255\"] 示例：SELECT avg(bytes) AS byte FROM `res-*` WHERE last(30d) AND ip_ranges(client_ip, [`183.206.20.0/24`, `192.168.32.0/24`]) GROUP BY client_ip LIMIT 10 query_string(text) - 该函数用于支持ES支持的\"迷你\"查询语言（mini-language），将用户提供的参数当做\"迷你\"查询语言交由ES进行查询。 参数：text - 字符串类型，该参数即为用户输出的\"迷你\"查询语言表达式。 示例：SELECT avg(bytes) AS byte FROM `res-*` WHERE last(30d) AND query_string(`client_ip: 192.168.32.181`) GROUP BY client_ip LIMIT 10GROUP BY range(filed, range [, range, …]) - 桶函数，根据某字段的范围创建桶。 参数：field - 字段名 range - 指定数值范围，支持的格式有：”from TO to”, [from , to] 示例：SELECT avg(bytes) AS byte FROM res-* WHERE last(30d) group by range(bytes, 1 TO 100, [100, 1000], [1000, 2000], [2000, 10000]) as range ip_range(field, range[, range ....]) - 该函数提供对IP地址进行过滤的功能。 参数：field - 指定IP地址字段名。 range - IP地址范围，支持格式有：&quot;192.168.0.0/24&quot;、&quot;192.168.0.0 TO 192.168.0.255&quot;、[&quot;192.168.0.0&quot;, &quot;192.168.0.255&quot;] 示例： SELECT avg(bytes) AS byte FROM `res-*` WHERE last(30d) group by ip_range(client_ip, `192.168.0.0/24`, `192.168.32.0 TO 192.168.32.100`, [`192.168.32.100`, `192.168.32.255`]) as range date_range(field, fmt, range[, range ....]) 参数：field - （可选）指定日志范围的字段，默认为：@timestamp。 fmt - （可选）字符串类型，指定返回的时间格式。如果函数在调用时，只指定了一个字符串类型的参数，则认为是fmt。 range - 列表类型, [from, to]。 from与to支持的时间格式如下： 时间表达式格式：&lt;参考时间&gt; +/- &lt;间隔&gt;/&lt;单位&gt; &lt;参考时间&gt; 可以是now或任意以双竖线（||）结尾的时间字符串； &lt;间隔&gt;为有带单位的数字; &lt;单位&gt;可以是：y(年)、M(月)、w(周)、d(天)、h(时)、m(分)、s(秒) 时间格式可以解释为：参考时间加（减）时间间隔，再四舍五入到某个时间单位，示例如下： 1. now+1h ==&gt; 当前时间加上1小时，精确到毫秒。 2. now+1h+1m ==&gt; 当前时间加上1小时，再加上1分钟，精确到毫秒。 3. now+1h/d ==&gt; 当前时间加上1小时，并四舍五入到最近的天。 4. 2018-12-01||+1M/d ==&gt; 2018年12月1日加上1个月，并四舍五入到最近的天。 示例：SELECT avg(bytes) AS byte FROM `res-*` WHERE last(30d) group by date_range(`@timestamp`, `yyyy-MM-dd`, [`2018-12-01||+1M/d`, `now-1m/m`]) as range histogram(field, interval) - 该函数按照指定字段，创建等分（直方）桶，又名直方图、柱状图等。 参数：field - 标识符，字段名； interval - 数值类型，指定等分间隔。 示例：SELECT avg(bytes) AS sum FROM `res-*` WHERE last(5m) GROUP BY histogram(bytes, 1000) AS bytes1 date_histogram(field, interval) - 参数：field - 标识符，字段名； interval - 数值类型，指定等分时间间隔。支持的单位：天（d）、小时（h）、分钟（m）、秒（s），不支持所列单位以外的时间间隔。 示例：SELECT avg(bytes) AS sum FROM `res-*` WHERE last(30d) GROUP BY date_histogram(`@timestamp`, 1d) AS bytes1 filters(filter[, filter, ...]) 参数：filter - SQL 表达式。必须为 &lt;expr&gt; AS &lt;name&gt; 的格式。例如：0 &lt; byte &lt; 10000 AS &apos;lt10000&apos;。 示例：SELECT avg(bytes) AS sum FROM `res-*` WHERE last(30d) GROUP BY filters( ip_range(client_ip, `192.168.32.0 TO 192.168.32.100`) AS server, ip_range(client_ip, `192.168.32.101 TO 192.168.32.200`) AS pc, ip_range(client_ip, `192.168.32.201 TO 192.168.32.255`) AS cellphone) AS machine ORDER BY 1. 按照查询字段进行排序。 示例：select * from `res-*` order by bytes_in asc 2. 按照桶分组进行排序 示例：SELECT sum(bytes_in + bytes_out) AS bytes, sum(response_time) AS packets FROM `res-*` WHERE last(5m) GROUP BY user_group, user_type ORDER BY bytes DESC, packets DESC LIMIT 3, 3 注意： 1）默认按照文档个数进行倒序排列； 2）默认排序方式为降序 3）使用多个桶，只指定一个排序值时，所有桶都按照这个值进行排序；例如：order by a, b desc 4）使用多个桶，且指定多个排序值，返回结果只在桶内有序，全部结果来看是无序的。 5）指定排序值多于桶个数时，将被忽略。 LIMIT 1. 限制查询数量 示例：select * from `res-*` limit 30 2. 限制桶内查询数量 示例：SELECT sum(bytes_in + bytes_out) AS bytes, sum(response_time) AS packets FROM `res-*` WHERE last(5m) GROUP BY user_group, user_type ORDER BY bytes DESC, packets DESC LIMIT 3, 3 注意: 1）需要对每个桶内返回结果数进行限制； 2）SQL中LIMIT指定的限制值，与桶之间按顺序一一对应。","link":"/2019/06/25/esagent-sql/"},{"title":"find ","text":"# find &lt;directory&gt; -type f -name &quot;*.c&quot; | xargs grep &quot;&lt;strings&gt;&quot;&lt;directory&gt;是你要找的文件夹；如果是当前文件夹可以省略-type f 说明，只找文件-name “*.c” 表示只找C语言写的代码，从而避免去查binary；也可以不写，表示找所有文件是你要找的某个字符串 1find /your/path -type f -print | xargs grep MASQUERADEgrep -F MASQUERADE -R /path find -name &quot;.git&quot;","link":"/2019/07/21/find/"},{"title":"generator配置文件","text":"最全的generator配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\"\"http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\"&gt;&lt;!-- 配置生成器 --&gt;&lt;generatorConfiguration&gt;&lt;!-- 可以用于加载配置项或者配置文件，在整个配置文件中就可以使用${propertyKey}的方式来引用配置项 resource：配置资源加载地址，使用resource，MBG从classpath开始找，比如com/myproject/generatorConfig.properties url：配置资源加载地质，使用URL的方式，比如file:///C:/myfolder/generatorConfig.properties. 注意，两个属性只能选址一个; 另外，如果使用了mybatis-generator-maven-plugin，那么在pom.xml中定义的properties都可以直接在generatorConfig.xml中使用&lt;properties resource=\"\" url=\"\" /&gt; --&gt; &lt;!-- 在MBG工作的时候，需要额外加载的依赖包 location属性指明加载jar/zip包的全路径&lt;classPathEntry location=\"/Program Files/IBM/SQLLIB/java/db2java.zip\" /&gt; --&gt; &lt;!-- context:生成一组对象的环境 id:必选，上下文id，用于在生成错误时提示 defaultModelType:指定生成对象的样式 1，conditional：类似hierarchical； 2，flat：所有内容（主键，blob）等全部生成在一个对象中； 3，hierarchical：主键生成一个XXKey对象(key class)，Blob等单独生成一个对象，其他简单属性在一个对象中(record class) targetRuntime: 1，MyBatis3：默认的值，生成基于MyBatis3.x以上版本的内容，包括XXXBySample； 2，MyBatis3Simple：类似MyBatis3，只是不生成XXXBySample； introspectedColumnImpl：类全限定名，用于扩展MBG--&gt;&lt;context id=\"mysql\" defaultModelType=\"hierarchical\" targetRuntime=\"MyBatis3Simple\" &gt; &lt;!-- 自动识别数据库关键字，默认false，如果设置为true，根据SqlReservedWords中定义的关键字列表； 一般保留默认值，遇到数据库关键字（Java关键字），使用columnOverride覆盖 --&gt; &lt;property name=\"autoDelimitKeywords\" value=\"false\"/&gt; &lt;!-- 生成的Java文件的编码 --&gt; &lt;property name=\"javaFileEncoding\" value=\"UTF-8\"/&gt; &lt;!-- 格式化java代码 --&gt; &lt;property name=\"javaFormatter\" value=\"org.mybatis.generator.api.dom.DefaultJavaFormatter\"/&gt; &lt;!-- 格式化XML代码 --&gt; &lt;property name=\"xmlFormatter\" value=\"org.mybatis.generator.api.dom.DefaultXmlFormatter\"/&gt; &lt;!-- beginningDelimiter和endingDelimiter：指明数据库的用于标记数据库对象名的符号，比如ORACLE就是双引号，MYSQL默认是`反引号； --&gt; &lt;property name=\"beginningDelimiter\" value=\"`\"/&gt; &lt;property name=\"endingDelimiter\" value=\"`\"/&gt; &lt;!-- 必须要有的，使用这个配置链接数据库 @TODO:是否可以扩展 --&gt; &lt;jdbcConnection driverClass=\"com.mysql.jdbc.Driver\" connectionURL=\"jdbc:mysql:///pss\" userId=\"root\" password=\"admin\"&gt; &lt;!-- 这里面可以设置property属性，每一个property属性都设置到配置的Driver上 --&gt; &lt;/jdbcConnection&gt; &lt;!-- java类型处理器 用于处理DB中的类型到Java中的类型，默认使用JavaTypeResolverDefaultImpl； 注意一点，默认会先尝试使用Integer，Long，Short等来对应DECIMAL和 NUMERIC数据类型； --&gt; &lt;javaTypeResolver type=\"org.mybatis.generator.internal.types.JavaTypeResolverDefaultImpl\"&gt; &lt;!-- true：使用BigDecimal对应DECIMAL和 NUMERIC数据类型 false：默认, scale&gt;0;length&gt;18：使用BigDecimal; scale=0;length[10,18]：使用Long； scale=0;length[5,9]：使用Integer； scale=0;length&lt;5：使用Short； --&gt; &lt;property name=\"forceBigDecimals\" value=\"false\"/&gt; &lt;/javaTypeResolver&gt; &lt;!-- java模型创建器，是必须要的元素 负责：1，key类（见context的defaultModelType）；2，java类；3，查询类 targetPackage：生成的类要放的包，真实的包受enableSubPackages属性控制； targetProject：目标项目，指定一个存在的目录下，生成的内容会放到指定目录中，如果目录不存在，MBG不会自动建目录 --&gt; &lt;javaModelGenerator targetPackage=\"com._520it.mybatis.domain\" targetProject=\"src/main/java\"&gt; &lt;!-- for MyBatis3/MyBatis3Simple 自动为每一个生成的类创建一个构造方法，构造方法包含了所有的field；而不是使用setter； --&gt; &lt;property name=\"constructorBased\" value=\"false\"/&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;!-- for MyBatis3 / MyBatis3Simple 是否创建一个不可变的类，如果为true， 那么MBG会创建一个没有setter方法的类，取而代之的是类似constructorBased的类 --&gt; &lt;property name=\"immutable\" value=\"false\"/&gt; &lt;!-- 设置一个根对象， 如果设置了这个根对象，那么生成的keyClass或者recordClass会继承这个类；在Table的rootClass属性中可以覆盖该选项 注意：如果在key class或者record class中有root class相同的属性，MBG就不会重新生成这些属性了，包括： 1，属性名相同，类型相同，有相同的getter/setter方法； --&gt; &lt;property name=\"rootClass\" value=\"com._520it.mybatis.domain.BaseDomain\"/&gt; &lt;!-- 设置是否在getter方法中，对String类型字段调用trim()方法 --&gt; &lt;property name=\"trimStrings\" value=\"true\"/&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成SQL map的XML文件生成器， 注意，在Mybatis3之后，我们可以使用mapper.xml文件+Mapper接口（或者不用mapper接口）， 或者只使用Mapper接口+Annotation，所以，如果 javaClientGenerator配置中配置了需要生成XML的话，这个元素就必须配置 targetPackage/targetProject:同javaModelGenerator --&gt; &lt;sqlMapGenerator targetPackage=\"com._520it.mybatis.mapper\" targetProject=\"src/main/resources\"&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 对于mybatis来说，即生成Mapper接口，注意，如果没有配置该元素，那么默认不会生成Mapper接口 targetPackage/targetProject:同javaModelGenerator type：选择怎么生成mapper接口（在MyBatis3/MyBatis3Simple下）： 1，ANNOTATEDMAPPER：会生成使用Mapper接口+Annotation的方式创建（SQL生成在annotation中），不会生成对应的XML； 2，MIXEDMAPPER：使用混合配置，会生成Mapper接口，并适当添加合适的Annotation，但是XML会生成在XML中； 3，XMLMAPPER：会生成Mapper接口，接口完全依赖XML； 注意，如果context是MyBatis3Simple：只支持ANNOTATEDMAPPER和XMLMAPPER --&gt; &lt;javaClientGenerator targetPackage=\"com._520it.mybatis.mapper\" type=\"ANNOTATEDMAPPER\" targetProject=\"src/main/java\"&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;!-- 可以为所有生成的接口添加一个父接口，但是MBG只负责生成，不负责检查 &lt;property name=\"rootInterface\" value=\"\"/&gt; --&gt; &lt;/javaClientGenerator&gt; &lt;!-- 选择一个table来生成相关文件，可以有一个或多个table，必须要有table元素 选择的table会生成一下文件： 1，SQL map文件 2，生成一个主键类； 3，除了BLOB和主键的其他字段的类； 4，包含BLOB的类； 5，一个用户生成动态查询的条件类（selectByExample, deleteByExample），可选； 6，Mapper接口（可选） tableName（必要）：要生成对象的表名； 注意：大小写敏感问题。正常情况下，MBG会自动的去识别数据库标识符的大小写敏感度，在一般情况下，MBG会 根据设置的schema，catalog或tablename去查询数据表，按照下面的流程： 1，如果schema，catalog或tablename中有空格，那么设置的是什么格式，就精确的使用指定的大小写格式去查询； 2，否则，如果数据库的标识符使用大写的，那么MBG自动把表名变成大写再查找； 3，否则，如果数据库的标识符使用小写的，那么MBG自动把表名变成小写再查找； 4，否则，使用指定的大小写格式查询； 另外的，如果在创建表的时候，使用的\"\"把数据库对象规定大小写，就算数据库标识符是使用的大写，在这种情况下也会使用给定的大小写来创建表名； 这个时候，请设置delimitIdentifiers=\"true\"即可保留大小写格式； 可选： 1，schema：数据库的schema； 2，catalog：数据库的catalog； 3，alias：为数据表设置的别名，如果设置了alias，那么生成的所有的SELECT SQL语句中，列名会变成：alias_actualColumnName 4，domainObjectName：生成的domain类的名字，如果不设置，直接使用表名作为domain类的名字；可以设置为somepck.domainName，那么会自动把domainName类再放到somepck包里面； 5，enableInsert（默认true）：指定是否生成insert语句； 6，enableSelectByPrimaryKey（默认true）：指定是否生成按照主键查询对象的语句（就是getById或get）； 7，enableSelectByExample（默认true）：MyBatis3Simple为false，指定是否生成动态查询语句； 8，enableUpdateByPrimaryKey（默认true）：指定是否生成按照主键修改对象的语句（即update)； 9，enableDeleteByPrimaryKey（默认true）：指定是否生成按照主键删除对象的语句（即delete）； 10，enableDeleteByExample（默认true）：MyBatis3Simple为false，指定是否生成动态删除语句； 11，enableCountByExample（默认true）：MyBatis3Simple为false，指定是否生成动态查询总条数语句（用于分页的总条数查询）； 12，enableUpdateByExample（默认true）：MyBatis3Simple为false，指定是否生成动态修改语句（只修改对象中不为空的属性）； 13，modelType：参考context元素的defaultModelType，相当于覆盖； 14，delimitIdentifiers：参考tableName的解释，注意，默认的delimitIdentifiers是双引号，如果类似MYSQL这样的数据库，使用的是`（反引号，那么还需要设置context的beginningDelimiter和endingDelimiter属性） 15，delimitAllColumns：设置是否所有生成的SQL中的列名都使用标识符引起来。默认为false，delimitIdentifiers参考context的属性 注意，table里面很多参数都是对javaModelGenerator，context等元素的默认属性的一个复写； --&gt; &lt;table tableName=\"userinfo\" &gt; &lt;!-- 参考 javaModelGenerator 的 constructorBased属性--&gt; &lt;property name=\"constructorBased\" value=\"false\"/&gt; &lt;!-- 默认为false，如果设置为true，在生成的SQL中，table名字不会加上catalog或schema； --&gt; &lt;property name=\"ignoreQualifiersAtRuntime\" value=\"false\"/&gt; &lt;!-- 参考 javaModelGenerator 的 immutable 属性 --&gt; &lt;property name=\"immutable\" value=\"false\"/&gt; &lt;!-- 指定是否只生成domain类，如果设置为true，只生成domain类，如果还配置了sqlMapGenerator，那么在mapper XML文件中，只生成resultMap元素 --&gt; &lt;property name=\"modelOnly\" value=\"false\"/&gt; &lt;!-- 参考 javaModelGenerator 的 rootClass 属性 &lt;property name=\"rootClass\" value=\"\"/&gt; --&gt; &lt;!-- 参考javaClientGenerator 的 rootInterface 属性 &lt;property name=\"rootInterface\" value=\"\"/&gt; --&gt; &lt;!-- 如果设置了runtimeCatalog，那么在生成的SQL中，使用该指定的catalog，而不是table元素上的catalog &lt;property name=\"runtimeCatalog\" value=\"\"/&gt; --&gt; &lt;!-- 如果设置了runtimeSchema，那么在生成的SQL中，使用该指定的schema，而不是table元素上的schema &lt;property name=\"runtimeSchema\" value=\"\"/&gt; --&gt; &lt;!-- 如果设置了runtimeTableName，那么在生成的SQL中，使用该指定的tablename，而不是table元素上的tablename &lt;property name=\"runtimeTableName\" value=\"\"/&gt; --&gt; &lt;!-- 注意，该属性只针对MyBatis3Simple有用； 如果选择的runtime是MyBatis3Simple，那么会生成一个SelectAll方法，如果指定了selectAllOrderByClause，那么会在该SQL中添加指定的这个order条件； --&gt; &lt;property name=\"selectAllOrderByClause\" value=\"age desc,username asc\"/&gt; &lt;!-- 如果设置为true，生成的model类会直接使用column本身的名字，而不会再使用驼峰命名方法，比如BORN_DATE，生成的属性名字就是BORN_DATE,而不会是bornDate --&gt; &lt;property name=\"useActualColumnNames\" value=\"false\"/&gt; &lt;!-- generatedKey用于生成生成主键的方法， 如果设置了该元素，MBG会在生成的&lt;insert&gt;元素中生成一条正确的&lt;selectKey&gt;元素，该元素可选 column:主键的列名； sqlStatement：要生成的selectKey语句，有以下可选项： Cloudscape:相当于selectKey的SQL为： VALUES IDENTITY_VAL_LOCAL() DB2 :相当于selectKey的SQL为： VALUES IDENTITY_VAL_LOCAL() DB2_MF :相当于selectKey的SQL为：SELECT IDENTITY_VAL_LOCAL() FROM SYSIBM.SYSDUMMY1 Derby :相当于selectKey的SQL为：VALUES IDENTITY_VAL_LOCAL() HSQLDB :相当于selectKey的SQL为：CALL IDENTITY() Informix :相当于selectKey的SQL为：select dbinfo('sqlca.sqlerrd1') from systables where tabid=1 MySql :相当于selectKey的SQL为：SELECT LAST_INSERT_ID() SqlServer :相当于selectKey的SQL为：SELECT SCOPE_IDENTITY() SYBASE :相当于selectKey的SQL为：SELECT @@IDENTITY JDBC :相当于在生成的insert元素上添加useGeneratedKeys=\"true\"和keyProperty属性 &lt;generatedKey column=\"\" sqlStatement=\"\"/&gt; --&gt; &lt;!-- 该元素会在根据表中列名计算对象属性名之前先重命名列名，非常适合用于表中的列都有公用的前缀字符串的时候， 比如列名为：CUST_ID,CUST_NAME,CUST_EMAIL,CUST_ADDRESS等； 那么就可以设置searchString为\"^CUST_\"，并使用空白替换，那么生成的Customer对象中的属性名称就不是 custId,custName等，而是先被替换为ID,NAME,EMAIL,然后变成属性：id，name，email； 注意，MBG是使用java.util.regex.Matcher.replaceAll来替换searchString和replaceString的， 如果使用了columnOverride元素，该属性无效； &lt;columnRenamingRule searchString=\"\" replaceString=\"\"/&gt; --&gt; &lt;!-- 用来修改表中某个列的属性，MBG会使用修改后的列来生成domain的属性； column:要重新设置的列名； 注意，一个table元素中可以有多个columnOverride元素哈~ --&gt; &lt;columnOverride column=\"username\"&gt; &lt;!-- 使用property属性来指定列要生成的属性名称 --&gt; &lt;property name=\"property\" value=\"userName\"/&gt; &lt;!-- javaType用于指定生成的domain的属性类型，使用类型的全限定名 &lt;property name=\"javaType\" value=\"\"/&gt; --&gt; &lt;!-- jdbcType用于指定该列的JDBC类型 &lt;property name=\"jdbcType\" value=\"\"/&gt; --&gt; &lt;!-- typeHandler 用于指定该列使用到的TypeHandler，如果要指定，配置类型处理器的全限定名 注意，mybatis中，不会生成到mybatis-config.xml中的typeHandler 只会生成类似：where id = #{id,jdbcType=BIGINT,typeHandler=com._520it.mybatis.MyTypeHandler}的参数描述 &lt;property name=\"jdbcType\" value=\"\"/&gt; --&gt; &lt;!-- 参考table元素的delimitAllColumns配置，默认为false &lt;property name=\"delimitedColumnName\" value=\"\"/&gt; --&gt; &lt;/columnOverride&gt; &lt;!-- ignoreColumn设置一个MGB忽略的列，如果设置了改列，那么在生成的domain中，生成的SQL中，都不会有该列出现 column:指定要忽略的列的名字； delimitedColumnName：参考table元素的delimitAllColumns配置，默认为false 注意，一个table元素中可以有多个ignoreColumn元素 &lt;ignoreColumn column=\"deptId\" delimitedColumnName=\"\"/&gt; --&gt; &lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt;","link":"/2019/09/06/generator配置文件/"},{"title":"golang操作数据库","text":"一、安装驱动 在Git命令行中输入go get github.com/go-sql-driver/mysql回车之后等待一会，等提示可以再输入命令时，就说明驱动已经装好了。二、导入mysql包 1234import(\"database/sql\"_ \"github.com/go-sql-driver/mysql\" } 三、对数据库的操作 1、数据库的连接db, err := sql.Open(&quot;mysql&quot;, &quot;root:123456@tcp(127.0.0.1:3306)/test?charset=utf8&quot;); 2、数据库查询 123456789type info struct { id int `db:\"id\"` name string `db:\"name\"` age int `db:\"age\"` sex string `db:\"sex\"` salary int `db:\"salary\"` work string `db:\"work\"` inparty string `db:\"inparty\"`} 首先可以先定义一个结构体用来保存读出的每条数据，当然如果只需要其中某些数据，也可以按照自己的需要修改结构体，这里用我自己创建的表来演示。rows,err:=db.Query(&quot;SELECT * FROM message&quot;)这条语句用来将表中所有的条无条件的读出保存到rows中。接下来是数据的逐条输出： 12345for rows.Next(){ var s info err=rows.Scan(&amp;s.id,&amp;s.name,&amp;s.age,&amp;s.sex,&amp;s.salary,&amp;s.work,&amp;s.inparty,) fmt.Println(s)} for rows.next可以一直读到表格的末尾。 下面是完整的查询代码： 1234567891011121314151617181920212223242526272829303132333435363738import ( _\"mysql\" \"database/sql\" \"fmt\") func check(err error){ //因为要多次检查错误，所以干脆自己建立一个函数。 if err!=nil{ fmt.Println(err) } } func main(){ db,err:=sql.Open(\"mysql\",\"root:123456@tcp(127.0.0.1:3306)/employee\") check(err) //query type info struct { id int `db:\"id\"` name string `db:\"name\"` age int `db:\"age\"` sex string `db:\"sex\"` salary int `db:\"salary\"` work string `db:\"work\"` inparty string `db:\"inparty\"` } rows,err:=db.Query(\"SELECT * FROM message\") check(err) for rows.Next(){ var s info err=rows.Scan(&amp;s.id,&amp;s.name,&amp;s.age,&amp;s.sex,&amp;s.salary,&amp;s.work,&amp;s.inparty,) check(err) fmt.Println(s) } rows.Close()} 如果需要按条件查询的话，可以使用mysql语句的where： 12345678rows,err:=db.Query(\"SELECT name,age FROM message where id=2\") check(err) for rows.Next(){ var s info err=rows.Scan(&amp;s.name,&amp;s.age) check(err) fmt.Println(s) } 3、数据库增加条目 12result,err:=db.Exec(\"INSERT INTO message(id,name,age,sex,salary,work,inparty)VALUES (?,?,?,?,?,?,?)\",7,\"李白\",80,\"男\",1000,\"中\",\"是\") check(err) 4、数据库的更新 123results,err:=db.Exec(\"UPDATE message SET salary=? where id=?\",8900,3) check(err) fmt.Println(results.RowsAffected()) //更新的条目数 5、数据库的删除 删除指的是删除表格中某些条目。 ###下面是完整的增删改查代码： 123results,err:=db.Exec(\"DELETE FROM message where id=?\",2) check(err) fmt.Println(results.RowsAffected()) //删除的条目数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package main import ( _\"mysql\" \"database/sql\" \"fmt\") func check(err error){ if err!=nil{ fmt.Println(err) } } func main(){ db,err:=sql.Open(\"mysql\",\"root:123456@tcp(127.0.0.1:3306)/employee\") check(err) //query type info struct { id int `db:\"id\"` name string `db:\"name\"` age int `db:\"age\"` sex string `db:\"sex\"` salary int `db:\"salary\"` work string `db:\"work\"` inparty string `db:\"inparty\"` } //query rows,err:=db.Query(\"SELECT * FROM message\") for rows.Next(){ var s info err=rows.Scan(&amp;s.id,&amp;s.name,&amp;s.age,&amp;s.sex,&amp;s.salary,&amp;s.work,&amp;s.inparty) check(err) fmt.Println(s) } rows.Close() //insert db.Exec(\"INSERT INTO message(id,name,age,sex,salary,work,inparty)VALUES (?,?,?,?,?,?,?)\",7,\"李白\",80,\"男\",1000,\"中\",\"是\") //update results,err:=db.Exec(\"UPDATE message SET salary=? where id=?\",8900,3) check(err) fmt.Println(results.RowsAffected()) //delete results,err:=db.Exec(\"DELETE FROM message where id=?\",2) check(err) fmt.Println(results.RowsAffected()) }","link":"/2019/11/29/golang操作数据库/"},{"title":"Go 处理 JSON 教程","text":"参考Go 处理 JSON 教程 - 如何创建和解析 JSON 数据 本文介绍在 Go 语言中，如何创建和解析 JSON 数据。 一、创建 JSON使用 Go 标准库中的 encoding/json，可以很方便从 struct、map、slice 等数据结构体来创建 JSON 数据。 1、从 struct 创建通过 json.Marshal，可以很方便的将一个 struct 转化为 []byte 类型的 JSON 数据。 12345678910111213141516171819202122232425package mainimport ( \"encoding/json\" \"fmt\")type Person struct { Name string Age int Emails []string}func main() { bingo := Person{ Name: \"Bingo Huang\", Age: 30, Emails: []string{\"go@bingohuang.com\", \"me@bingohuang.com\"}, } json_bytes, err := json.Marshal(bingo) if err != nil { panic(err) } fmt.Printf(\"%s\",json_bytes)} 输出：{&quot;Name&quot;:&quot;Bingo Huang&quot;,&quot;Age&quot;:30,&quot;Emails&quot;:[go@bingohuang.com&quot;, &quot;me@bingohuang.com&quot;]} 注意： 结构体中的字段名，需要大写开头，否则不会被输出 结构体中可以嵌入其他结构体 json.Marshal 函数返回一个 []byte 类型的 JSON 数据和一个 error，别忘了处理该 error 返回的 []byte 类型 JSON 数据，如果你想当成字符串处理，需要做强制转换：string(json_bytes) 2、定义字段名称如果你希望输出的 JSON 字段，不一定要大写字母开头，甚至想自定义 JSON 字段名，可以打上 json tag。 12345type Person struct { Name string `json:\"name\"` Age int `json:\"age\"` Emails []string `json:\"emails\"`} 再次输出：{&quot;name&quot;:&quot;Bingo Huang&quot;,&quot;age&quot;:30,&quot;emails&quot;:[&quot;go@bingohuang.com&quot;,&quot;me@bingohuang.com&quot;]} 3、忽略空字段如果你希望某些字段在数据为空时能能被自动忽略，只需加上 omitempty 标签。 1234567891011121314151617181920package mainimport ( \"encoding/json\" \"fmt\")type Person struct { Name string `json:\"name,omitempty\"` Age int `json:\"age,omitempty\"` Emails []string `json:\"emails,omitempty\"`}func main() { bingo := Person{ Name: \"Bingo Huang\", } json_bytes, _ := json.Marshal(bingo) fmt.Printf(\"%s\", json_bytes)} 输出： {&quot;name&quot;:&quot;Bingo Huang&quot;} 4、跳过某字段如果你不想让某个字段（无论该字段数据是否为空）输出到 JSON 中，可使用 - 来跳过。 123456789101112131415161718192021package mainimport ( \"encoding/json\" \"fmt\")type Person struct { Name string `json:\"name,omitempty\"` Age int `json:\"-\"` Emails []string `json:\"emails,omitempty\"`}func main() { bingo := Person{ Name: \"Bingo Huang\", Age: 30, } json_bytes, _ := json.Marshal(bingo) fmt.Printf(\"%s\", json_bytes)} 输出： {&quot;name&quot;:&quot;Bingo Huang&quot;} 5、从 map 和 slice 创建从 map 和 slice 来创建 JSON，也很容易，直接上示例 123456789101112131415package mainimport ( \"encoding/json\" \"fmt\")func main() { bingo := map[string]interface{}{ \"name\": \"Bingo Huang\", \"age\": 30, } json_bytes, _ := json.Marshal(bingo) fmt.Printf(\"%s\", json_bytes)} 输出： {&quot;age&quot;:30,&quot;name&quot;:&quot;Bingo Huang&quot;} 1234567891011package mainimport ( \"encoding/json\" \"fmt\")func main() { emails := []string{\"go@bingohuang.com\", \"me@bingohuang.com\"} json_bytes, _ := json.Marshal(emails)} 输出：[&quot;go@bingohuang.com&quot;,&quot;me@bingohuang.com&quot;] 二、解析 JSON创建 JSON 是如此方便，那反过来看看 GO 如何解析 JSON。 1、解析到 struct相反，可以通过 json.Unmarshal 来将一个 []byte 类型的 JSON 数据解析到 struct 中。 12345678910111213141516171819202122232425262728package mainimport ( \"encoding/json\" \"fmt\")type Person struct { Name string `json:\"name\"` Age int `json:\"age\"` Emails []string `json:\"emails\"`}func main() { json_bytes := []byte(` { \"Name\":\"Bingo Huang\", \"Age\":30, \"Emails\":[\"go@bingohuang.com\",\"me@bingohuang.com\"] } `) bingo := Person{} err := json.Unmarshal(json_bytes, &amp;bingo) if err != nil { panic(err) } fmt.Println(bingo.Name, bingo.Age, bingo.Emails)} 这里我们将 json_bytes 和 Person 的指针传递给 json.Unmarshal，注意要传递结构体的指针，因为解析器需要给结构体写入数据。 如果 JSON 数据中不包含结构体的某些字段，转换成 struct 时字段会被忽略，反过来如果结构体中某些字段不需要输出 JSON，也会被忽略，如下： 1234567891011121314151617181920212223242526272829package mainimport ( \"encoding/json\" \"fmt\")type Person struct { Name string `json:\"name\"` Age int `json:\"age\"` Emails []string `json:\"emails\"` Address string}func main() { json_bytes := []byte(` { \"Name\":\"Bingo Huang\", \"Age\":30 } `) var bingo Person err := json.Unmarshal(json_bytes, &amp;bingo) if err != nil { panic(err) } fmt.Println(bingo.Emails) fmt.Println(bingo.Address)} 这里 Emails 和 Address 字段都被忽略了，输出都为空。 2、解析到 map 和 slice通过 json.Unmarshal 将 JSON 数据解析到 map 和 slice 也十分方便，这里以 map 为例。 12345678910111213141516171819202122package mainimport ( \"encoding/json\" \"fmt\")func main() { json_bytes := []byte(` { \"Name\":\"Bingo Huang\", \"Age\":30, \"Emails\":[\"go@bingohuang.com\",\"me@bingohuang.com\"] } `) var bingo map[string]interface{} err := json.Unmarshal(json_bytes, &amp;bingo) if err != nil { panic(err) } fmt.Println(bingo[\"Name\"], bingo[\"Age\"], bingo[\"Emails\"], bingo[\"Score\"])} 三、JSON 流处理json 包还提供了另外两个函数 NewEncoder 和 NewDecoder，提供 Encoder 和 Decoder 类型，可支持 io.Reader 和 io.Writer 接口做流处理。 1、创建 JSON 到文件中我们可以将一个 JSON 文件转化到结构体中。 1234567891011121314package mainimport ( \"encoding/json\" \"fmt\" \"os\")func main() { fileReader, _ := os.Open(\"bingo.json\") var bingo map[string]interface{} json.NewDecoder(fileReader).Decode(&amp;bingo) fmt.Println(bingo)} bingo.json 内容如下： 12345{ \"Name\":\"Bingo Huang\", \"Age\":30, \"Emails\":[\"go@bingohuang.com\",\"me@bingohuang.com\"]} map[Name:Bingo Huang Age:30 Emails:[go@bingohuang.com me@bingohuang.com]] 2、从文件中解析 JSON同样，可以反过来，将对象转化为 JSON 格式，并写入文件中。 12345678910111213141516171819202122package mainimport ( \"encoding/json\" \"os\")type Person struct { Name string Age int Emails []string}func main() { bingo := Person{ Name: \"Bingo Huang\", Age: 30, Emails: []string{\"go@bingohuang.com\",\"me@bingohuang.com\"}, } fileWriter, _ := os.Create(\"output.json\") json.NewEncoder(fileWriter).Encode(bingo)} {&quot;Name&quot;:&quot;Bingo Huang&quot;,&quot;Age&quot;:30,&quot;Emails&quot;:[&quot;go@bingohuang.com&quot;,&quot;me@bingohuang.com&quot;]}","link":"/2019/11/07/go与json/"},{"title":"euraka 配置介绍","text":"spring cloud eureka 参数配置 eureka.client.registry-fetch-interval-seconds表示eureka client间隔多久去拉取服务注册信息，默认为30秒，对于api-gateway，如果要迅速获取服务注册状态，可以缩小该值，比如5秒 eureka.instance.lease-expiration-duration-in-secondsleaseExpirationDurationInSeconds，表示eureka server至上一次收到client的心跳之后，等待下一次心跳的超时时间，在这个时间内若没收到下一次心跳，则将移除该instance。 默认为90秒如果该值太大，则很可能将流量转发过去的时候，该instance已经不存活了。如果该值设置太小了，则instance则很可能因为临时的网络抖动而被摘除掉。该值至少应该大于leaseRenewalIntervalInSecondseureka.instance.lease-renewal-interval-in-secondsleaseRenewalIntervalInSeconds，表示eureka client发送心跳给server端的频率。如果在leaseExpirationDurationInSeconds后，server端没有收到client的心跳，则将摘除该instance。除此之外，如果该instance实现了HealthCheckCallback，并决定让自己unavailable的话，则该instance也不会接收到流量。 默认30秒eureka.server.enable-self-preservation是否开启自我保护模式，默认为true。 默认情况下，如果Eureka Server在一定时间内没有接收到某个微服务实例的心跳，Eureka Server将会注销该实例（默认90秒）。但是当网络分区故障发生时，微服务与Eureka Server之间无法正常通信，以上行为可能变得非常危险了——因为微服务本身其实是健康的，此时本不应该注销这个微服务。 Eureka通过“自我保护模式”来解决这个问题——当Eureka Server节点在短时间内丢失过多客户端时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。一旦进入该模式，Eureka Server就会保护服务注册表中的信息，不再删除服务注册表中的数据（也就是不会注销任何微服务）。当网络故障恢复后，该Eureka Server节点会自动退出自我保护模式。 综上，自我保护模式是一种应对网络异常的安全保护措施。它的架构哲学是宁可同时保留所有微服务（健康的微服务和不健康的微服务都会保留），也不盲目注销任何健康的微服务。使用自我保护模式，可以让Eureka集群更加的健壮、稳定。 eureka.server.eviction-interval-timer-in-mseureka server清理无效节点的时间间隔，默认60000毫秒，即60秒 测试环境参考配置eureka server 1234eureka: server: enable-self-preservation: false # 关闭自我保护模式（缺省为打开） eviction-interval-timer-in-ms: 5000 # 续期时间，即扫描失效服务的间隔时间（缺省为60*1000ms） eureka client 1234567eureka: instance: lease-renewal-interval-in-seconds: 5 # 心跳时间，即服务续约间隔时间（缺省为30s） lease-expiration-duration-in-seconds: 10 # 发呆时间，即服务续约到期时间（缺省为90s） client: healthcheck: enabled: true # 开启健康检查（依赖spring-boot-starter-actuator） zuul 123eureka: client: registry-fetch-interval-seconds: 5 # 默认为30秒 client完整参数列表~/.m2/repository/org/springframework/cloud/spring-cloud-netflix-eureka-client/1.2.3.RELEASE/spring-cloud-netflix-eureka-client-1.2.3.RELEASE.jar!/META-INF/spring-configuration-metadata.json 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510{ \"groups\": [ { \"name\": \"eureka.client\", \"type\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.instance\", \"type\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" } ], \"properties\": [ { \"name\": \"eureka.client.allow-redirects\", \"type\": \"java.lang.Boolean\", \"description\": \"Indicates whether server can redirect a client request to a backup server/cluster.\\n If set to false, the server will handle the request directly, If set to true, it\\n may send HTTP redirect to the client, with a new server location.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.client.availability-zones\", \"type\": \"java.util.Map&lt;java.lang.String,java.lang.String&gt;\", \"description\": \"Gets the list of availability zones (used in AWS data centers) for the region in\\n which this instance resides.\\n\\n The changes are effective at runtime at the next registry fetch cycle as specified\\n by registryFetchIntervalSeconds.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.backup-registry-impl\", \"type\": \"java.lang.String\", \"description\": \"Gets the name of the implementation which implements BackupRegistry to fetch the\\n registry information as a fall back option for only the first time when the eureka\\n client starts.\\n\\n This may be needed for applications which needs additional resiliency for registry\\n information without which it cannot operate.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.cache-refresh-executor-exponential-back-off-bound\", \"type\": \"java.lang.Integer\", \"description\": \"Cache refresh executor exponential back off related property. It is a maximum\\n multiplier value for retry delay, in case where a sequence of timeouts occurred.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 10 }, { \"name\": \"eureka.client.cache-refresh-executor-thread-pool-size\", \"type\": \"java.lang.Integer\", \"description\": \"The thread pool size for the cacheRefreshExecutor to initialise with\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 2 }, { \"name\": \"eureka.client.client-data-accept\", \"type\": \"java.lang.String\", \"description\": \"EurekaAccept name for client data accept\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.decoder-name\", \"type\": \"java.lang.String\", \"description\": \"This is a transient config and once the latest codecs are stable, can be removed\\n (as there will only be one)\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.disable-delta\", \"type\": \"java.lang.Boolean\", \"description\": \"Indicates whether the eureka client should disable fetching of delta and should\\n rather resort to getting the full registry information.\\n\\n Note that the delta fetches can reduce the traffic tremendously, because the rate\\n of change with the eureka server is normally much lower than the rate of fetches.\\n\\n The changes are effective at runtime at the next registry fetch cycle as specified\\n by registryFetchIntervalSeconds\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.client.dollar-replacement\", \"type\": \"java.lang.String\", \"description\": \"Get a replacement string for Dollar sign &lt;code&gt;$&lt;\\/code&gt; during\\n serializing/deserializing information in eureka server.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": \"_-\" }, { \"name\": \"eureka.client.enabled\", \"type\": \"java.lang.Boolean\", \"description\": \"Flag to indicate that the Eureka client is enabled.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.client.encoder-name\", \"type\": \"java.lang.String\", \"description\": \"This is a transient config and once the latest codecs are stable, can be removed\\n (as there will only be one)\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.escape-char-replacement\", \"type\": \"java.lang.String\", \"description\": \"Get a replacement string for underscore sign &lt;code&gt;_&lt;\\/code&gt; during\\n serializing/deserializing information in eureka server.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": \"__\" }, { \"name\": \"eureka.client.eureka-connection-idle-timeout-seconds\", \"type\": \"java.lang.Integer\", \"description\": \"Indicates how much time (in seconds) that the HTTP connections to eureka server can\\n stay idle before it can be closed.\\n\\n In the AWS environment, it is recommended that the values is 30 seconds or less,\\n since the firewall cleans up the connection information after a few mins leaving\\n the connection hanging in limbo\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 30 }, { \"name\": \"eureka.client.eureka-server-connect-timeout-seconds\", \"type\": \"java.lang.Integer\", \"description\": \"Indicates how long to wait (in seconds) before a connection to eureka server needs\\n to timeout. Note that the connections in the client are pooled by\\n org.apache.http.client.HttpClient and this setting affects the actual connection\\n creation and also the wait time to get the connection from the pool.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 5 }, { \"name\": \"eureka.client.eureka-server-d-n-s-name\", \"type\": \"java.lang.String\", \"description\": \"Gets the DNS name to be queried to get the list of eureka servers.This information\\n is not required if the contract returns the service urls by implementing\\n serviceUrls.\\n\\n The DNS mechanism is used when useDnsForFetchingServiceUrls is set to true and the\\n eureka client expects the DNS to configured a certain way so that it can fetch\\n changing eureka servers dynamically.\\n\\n The changes are effective at runtime.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.eureka-server-port\", \"type\": \"java.lang.String\", \"description\": \"Gets the port to be used to construct the service url to contact eureka server when\\n the list of eureka servers come from the DNS.This information is not required if\\n the contract returns the service urls eurekaServerServiceUrls(String).\\n\\n The DNS mechanism is used when useDnsForFetchingServiceUrls is set to true and the\\n eureka client expects the DNS to configured a certain way so that it can fetch\\n changing eureka servers dynamically.\\n\\n The changes are effective at runtime.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.eureka-server-read-timeout-seconds\", \"type\": \"java.lang.Integer\", \"description\": \"Indicates how long to wait (in seconds) before a read from eureka server needs to\\n timeout.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 8 }, { \"name\": \"eureka.client.eureka-server-total-connections\", \"type\": \"java.lang.Integer\", \"description\": \"Gets the total number of connections that is allowed from eureka client to all\\n eureka servers.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 200 }, { \"name\": \"eureka.client.eureka-server-total-connections-per-host\", \"type\": \"java.lang.Integer\", \"description\": \"Gets the total number of connections that is allowed from eureka client to a eureka\\n server host.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 50 }, { \"name\": \"eureka.client.eureka-server-u-r-l-context\", \"type\": \"java.lang.String\", \"description\": \"Gets the URL context to be used to construct the service url to contact eureka\\n server when the list of eureka servers come from the DNS. This information is not\\n required if the contract returns the service urls from eurekaServerServiceUrls.\\n\\n The DNS mechanism is used when useDnsForFetchingServiceUrls is set to true and the\\n eureka client expects the DNS to configured a certain way so that it can fetch\\n changing eureka servers dynamically. The changes are effective at runtime.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.eureka-service-url-poll-interval-seconds\", \"type\": \"java.lang.Integer\", \"description\": \"Indicates how often(in seconds) to poll for changes to eureka server information.\\n Eureka servers could be added or removed and this setting controls how soon the\\n eureka clients should know about it.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.client.fetch-registry\", \"type\": \"java.lang.Boolean\", \"description\": \"Indicates whether this client should fetch eureka registry information from eureka\\n server.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.client.fetch-remote-regions-registry\", \"type\": \"java.lang.String\", \"description\": \"Comma separated list of regions for which the eureka registry information will be\\n fetched. It is mandatory to define the availability zones for each of these regions\\n as returned by availabilityZones. Failing to do so, will result in failure of\\n discovery client startup.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.filter-only-up-instances\", \"type\": \"java.lang.Boolean\", \"description\": \"Indicates whether to get the applications after filtering the applications for\\n instances with only InstanceStatus UP states.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.client.g-zip-content\", \"type\": \"java.lang.Boolean\", \"description\": \"Indicates whether the content fetched from eureka server has to be compressed\\n whenever it is supported by the server. The registry information from the eureka\\n server is compressed for optimum network traffic.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.client.heartbeat-executor-exponential-back-off-bound\", \"type\": \"java.lang.Integer\", \"description\": \"Heartbeat executor exponential back off related property. It is a maximum\\n multiplier value for retry delay, in case where a sequence of timeouts occurred.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 10 }, { \"name\": \"eureka.client.heartbeat-executor-thread-pool-size\", \"type\": \"java.lang.Integer\", \"description\": \"The thread pool size for the heartbeatExecutor to initialise with\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 2 }, { \"name\": \"eureka.client.initial-instance-info-replication-interval-seconds\", \"type\": \"java.lang.Integer\", \"description\": \"Indicates how long initially (in seconds) to replicate instance info to the eureka\\n server\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 40 }, { \"name\": \"eureka.client.instance-info-replication-interval-seconds\", \"type\": \"java.lang.Integer\", \"description\": \"Indicates how often(in seconds) to replicate instance changes to be replicated to\\n the eureka server.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 30 }, { \"name\": \"eureka.client.log-delta-diff\", \"type\": \"java.lang.Boolean\", \"description\": \"Indicates whether to log differences between the eureka server and the eureka\\n client in terms of registry information.\\n\\n Eureka client tries to retrieve only delta changes from eureka server to minimize\\n network traffic. After receiving the deltas, eureka client reconciles the\\n information from the server to verify it has not missed out some information.\\n Reconciliation failures could happen when the client has had network issues\\n communicating to server.If the reconciliation fails, eureka client gets the full\\n registry information.\\n\\n While getting the full registry information, the eureka client can log the\\n differences between the client and the server and this setting controls that.\\n\\n The changes are effective at runtime at the next registry fetch cycle as specified\\n by registryFetchIntervalSecondsr\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.client.on-demand-update-status-change\", \"type\": \"java.lang.Boolean\", \"description\": \"If set to true, local status updates via ApplicationInfoManager will trigger\\n on-demand (but rate limited) register/updates to remote eureka servers\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.client.prefer-same-zone-eureka\", \"type\": \"java.lang.Boolean\", \"description\": \"Indicates whether or not this instance should try to use the eureka server in the\\n same zone for latency and/or other reason.\\n\\n Ideally eureka clients are configured to talk to servers in the same zone\\n\\n The changes are effective at runtime at the next registry fetch cycle as specified\\n by registryFetchIntervalSeconds\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.client.property-resolver\", \"type\": \"org.springframework.core.env.PropertyResolver\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.proxy-host\", \"type\": \"java.lang.String\", \"description\": \"Gets the proxy host to eureka server if any.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.proxy-password\", \"type\": \"java.lang.String\", \"description\": \"Gets the proxy password if any.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.proxy-port\", \"type\": \"java.lang.String\", \"description\": \"Gets the proxy port to eureka server if any.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.proxy-user-name\", \"type\": \"java.lang.String\", \"description\": \"Gets the proxy user name if any.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.region\", \"type\": \"java.lang.String\", \"description\": \"Gets the region (used in AWS datacenters) where this instance resides.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": \"us-east-1\" }, { \"name\": \"eureka.client.register-with-eureka\", \"type\": \"java.lang.Boolean\", \"description\": \"Indicates whether or not this instance should register its information with eureka\\n server for discovery by others.\\n\\n In some cases, you do not want your instances to be discovered whereas you just\\n want do discover other instances.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.client.registry-fetch-interval-seconds\", \"type\": \"java.lang.Integer\", \"description\": \"Indicates how often(in seconds) to fetch the registry information from the eureka\\n server.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": 30 }, { \"name\": \"eureka.client.registry-refresh-single-vip-address\", \"type\": \"java.lang.String\", \"description\": \"Indicates whether the client is only interested in the registry information for a\\n single VIP.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.service-url\", \"type\": \"java.util.Map&lt;java.lang.String,java.lang.String&gt;\", \"description\": \"Map of availability zone to list of fully qualified URLs to communicate with eureka\\n server. Each value can be a single URL or a comma separated list of alternative\\n locations.\\n\\n Typically the eureka server URLs carry protocol,host,port,context and version\\n information if any. Example:\\n http://ec2-256-156-243-129.compute-1.amazonaws.com:7001/eureka/\\n\\n The changes are effective at runtime at the next service url refresh cycle as\\n specified by eurekaServiceUrlPollIntervalSeconds.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.transport\", \"type\": \"com.netflix.discovery.shared.transport.EurekaTransportConfig\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\" }, { \"name\": \"eureka.client.use-dns-for-fetching-service-urls\", \"type\": \"java.lang.Boolean\", \"description\": \"Indicates whether the eureka client should use the DNS mechanism to fetch a list of\\n eureka servers to talk to. When the DNS name is updated to have additional servers,\\n that information is used immediately after the eureka client polls for that\\n information as specified in eurekaServiceUrlPollIntervalSeconds.\\n\\n Alternatively, the service urls can be returned serviceUrls, but the users should\\n implement their own mechanism to return the updated list in case of changes.\\n\\n The changes are effective at runtime.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaClientConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.instance.a-s-g-name\", \"type\": \"java.lang.String\", \"description\": \"Gets the AWS autoscaling group name associated with this instance. This information\\n is specifically used in an AWS environment to automatically put an instance out of\\n service after the instance is launched and it has been disabled for traffic..\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.app-group-name\", \"type\": \"java.lang.String\", \"description\": \"Get the name of the application group to be registered with eureka.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.appname\", \"type\": \"java.lang.String\", \"description\": \"Get the name of the application to be registered with eureka.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": \"unknown\" }, { \"name\": \"eureka.instance.data-center-info\", \"type\": \"com.netflix.appinfo.DataCenterInfo\", \"description\": \"Returns the data center this instance is deployed. This information is used to get\\n some AWS specific instance information if the instance is deployed in AWS.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.default-address-resolution-order\", \"type\": \"java.lang.String[]\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": [] }, { \"name\": \"eureka.instance.environment\", \"type\": \"org.springframework.core.env.Environment\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.health-check-url\", \"type\": \"java.lang.String\", \"description\": \"Gets the absolute health check page URL for this instance. The users can provide\\n the healthCheckUrlPath if the health check page resides in the same instance\\n talking to eureka, else in the cases where the instance is a proxy for some other\\n server, users can provide the full URL. If the full URL is provided it takes\\n precedence.\\n\\n &lt;p&gt;\\n It is normally used for making educated decisions based on the health of the\\n instance - for example, it can be used to determine whether to proceed deployments\\n to an entire farm or stop the deployments without causing further damage. The full\\n URL should follow the format http://${eureka.hostname}:7001/ where the value\\n ${eureka.hostname} is replaced at runtime.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.health-check-url-path\", \"type\": \"java.lang.String\", \"description\": \"Gets the relative health check URL path for this instance. The health check page\\n URL is then constructed out of the hostname and the type of communication - secure\\n or unsecure as specified in securePort and nonSecurePort.\\n\\n It is normally used for making educated decisions based on the health of the\\n instance - for example, it can be used to determine whether to proceed deployments\\n to an entire farm or stop the deployments without causing further damage.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": \"/health\" }, { \"name\": \"eureka.instance.home-page-url\", \"type\": \"java.lang.String\", \"description\": \"Gets the absolute home page URL for this instance. The users can provide the\\n homePageUrlPath if the home page resides in the same instance talking to eureka,\\n else in the cases where the instance is a proxy for some other server, users can\\n provide the full URL. If the full URL is provided it takes precedence.\\n\\n It is normally used for informational purposes for other services to use it as a\\n landing page. The full URL should follow the format http://${eureka.hostname}:7001/\\n where the value ${eureka.hostname} is replaced at runtime.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.home-page-url-path\", \"type\": \"java.lang.String\", \"description\": \"Gets the relative home page URL Path for this instance. The home page URL is then\\n constructed out of the hostName and the type of communication - secure or unsecure.\\n\\n It is normally used for informational purposes for other services to use it as a\\n landing page.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": \"/\" }, { \"name\": \"eureka.instance.host-info\", \"type\": \"org.springframework.cloud.commons.util.InetUtils$HostInfo\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.hostname\", \"type\": \"java.lang.String\", \"description\": \"The hostname if it can be determined at configuration time (otherwise it will be\\n guessed from OS primitives).\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.hostname\", \"type\": \"java.lang.String\", \"description\": \"The hostname if it can be determined at configuration time (otherwise it will be\\n guessed from OS primitives).\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.inet-utils\", \"type\": \"org.springframework.cloud.commons.util.InetUtils\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.initial-status\", \"type\": \"com.netflix.appinfo.InstanceInfo$InstanceStatus\", \"description\": \"Initial status to register with rmeote Eureka server.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.instance-enabled-onit\", \"type\": \"java.lang.Boolean\", \"description\": \"Indicates whether the instance should be enabled for taking traffic as soon as it\\n is registered with eureka. Sometimes the application might need to do some\\n pre-processing before it is ready to take traffic.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.instance.instance-id\", \"type\": \"java.lang.String\", \"description\": \"Get the unique Id (within the scope of the appName) of this instance to be\\n registered with eureka.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.ip-address\", \"type\": \"java.lang.String\", \"description\": \"Get the IPAdress of the instance. This information is for academic purposes only as\\n the communication from other instances primarily happen using the information\\n supplied in {@link #getHostName(boolean)}.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.lease-expiration-duration-in-seconds\", \"type\": \"java.lang.Integer\", \"description\": \"Indicates the time in seconds that the eureka server waits since it received the\\n last heartbeat before it can remove this instance from its view and there by\\n disallowing traffic to this instance.\\n\\n Setting this value too long could mean that the traffic could be routed to the\\n instance even though the instance is not alive. Setting this value too small could\\n mean, the instance may be taken out of traffic because of temporary network\\n glitches.This value to be set to atleast higher than the value specified in\\n leaseRenewalIntervalInSeconds.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": 90 }, { \"name\": \"eureka.instance.lease-renewal-interval-in-seconds\", \"type\": \"java.lang.Integer\", \"description\": \"Indicates how often (in seconds) the eureka client needs to send heartbeats to\\n eureka server to indicate that it is still alive. If the heartbeats are not\\n received for the period specified in leaseExpirationDurationInSeconds, eureka\\n server will remove the instance from its view, there by disallowing traffic to this\\n instance.\\n\\n Note that the instance could still not take traffic if it implements\\n HealthCheckCallback and then decides to make itself unavailable.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": 30 }, { \"name\": \"eureka.instance.metadata-map\", \"type\": \"java.util.Map&lt;java.lang.String,java.lang.String&gt;\", \"description\": \"Gets the metadata name/value pairs associated with this instance. This information\\n is sent to eureka server and can be used by other instances.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.namespace\", \"type\": \"java.lang.String\", \"description\": \"Get the namespace used to find properties. Ignored in Spring Cloud.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": \"eureka\" }, { \"name\": \"eureka.instance.non-secure-port\", \"type\": \"java.lang.Integer\", \"description\": \"Get the non-secure port on which the instance should receive traffic.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": 80 }, { \"name\": \"eureka.instance.non-secure-port-enabled\", \"type\": \"java.lang.Boolean\", \"description\": \"Indicates whether the non-secure port should be enabled for traffic or not.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.instance.prefer-ip-address\", \"type\": \"java.lang.Boolean\", \"description\": \"Flag to say that, when guessing a hostname, the IP address of the server should be\\n used in prference to the hostname reported by the OS.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.instance.secure-health-check-url\", \"type\": \"java.lang.String\", \"description\": \"Gets the absolute secure health check page URL for this instance. The users can\\n provide the secureHealthCheckUrl if the health check page resides in the same\\n instance talking to eureka, else in the cases where the instance is a proxy for\\n some other server, users can provide the full URL. If the full URL is provided it\\n takes precedence.\\n\\n &lt;p&gt;\\n It is normally used for making educated decisions based on the health of the\\n instance - for example, it can be used to determine whether to proceed deployments\\n to an entire farm or stop the deployments without causing further damage. The full\\n URL should follow the format http://${eureka.hostname}:7001/ where the value\\n ${eureka.hostname} is replaced at runtime.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.secure-port\", \"type\": \"java.lang.Integer\", \"description\": \"Get the Secure port on which the instance should receive traffic.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": 443 }, { \"name\": \"eureka.instance.secure-port-enabled\", \"type\": \"java.lang.Boolean\", \"description\": \"Indicates whether the secure port should be enabled for traffic or not.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.instance.secure-virtual-host-name\", \"type\": \"java.lang.String\", \"description\": \"Gets the secure virtual host name defined for this instance.\\n\\n This is typically the way other instance would find this instance by using the\\n secure virtual host name.Think of this as similar to the fully qualified domain\\n name, that the users of your services will need to find this instance.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": \"unknown\" }, { \"name\": \"eureka.instance.status-page-url\", \"type\": \"java.lang.String\", \"description\": \"Gets the absolute status page URL path for this instance. The users can provide the\\n statusPageUrlPath if the status page resides in the same instance talking to\\n eureka, else in the cases where the instance is a proxy for some other server,\\n users can provide the full URL. If the full URL is provided it takes precedence.\\n\\n It is normally used for informational purposes for other services to find about the\\n status of this instance. Users can provide a simple HTML indicating what is the\\n current status of the instance.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\" }, { \"name\": \"eureka.instance.status-page-url-path\", \"type\": \"java.lang.String\", \"description\": \"Gets the relative status page URL path for this instance. The status page URL is\\n then constructed out of the hostName and the type of communication - secure or\\n unsecure as specified in securePort and nonSecurePort.\\n\\n It is normally used for informational purposes for other services to find about the\\n status of this instance. Users can provide a simple HTML indicating what is the\\n current status of the instance.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": \"/info\" }, { \"name\": \"eureka.instance.virtual-host-name\", \"type\": \"java.lang.String\", \"description\": \"Gets the virtual host name defined for this instance.\\n\\n This is typically the way other instance would find this instance by using the\\n virtual host name.Think of this as similar to the fully qualified domain name, that\\n the users of your services will need to find this instance.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean\", \"defaultValue\": \"unknown\" } ], \"hints\": []} server完整参数列表~/.m2/repository/org/springframework/cloud/spring-cloud-netflix-eureka-server/1.2.3.RELEASE/spring-cloud-netflix-eureka-server-1.2.3.RELEASE.jar!/META-INF/spring-configuration-metadata.json 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471{ \"groups\": [ { \"name\": \"eureka.dashboard\", \"type\": \"org.springframework.cloud.netflix.eureka.server.EurekaDashboardProperties\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaDashboardProperties\" }, { \"name\": \"eureka.instance.registry\", \"type\": \"org.springframework.cloud.netflix.eureka.server.InstanceRegistryProperties\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.InstanceRegistryProperties\" }, { \"name\": \"eureka.server\", \"type\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\" } ], \"properties\": [ { \"name\": \"eureka.dashboard.enabled\", \"type\": \"java.lang.Boolean\", \"description\": \"Flag to enable the Eureka dashboard. Default true.\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaDashboardProperties\", \"defaultValue\": true }, { \"name\": \"eureka.dashboard.path\", \"type\": \"java.lang.String\", \"description\": \"The path to the Eureka dashboard (relative to the servlet path). Defaults to \\\"/\\\".\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaDashboardProperties\", \"defaultValue\": \"/\" }, { \"name\": \"eureka.instance.registry.default-open-for-traffic-count\", \"type\": \"java.lang.Integer\", \"description\": \"Value used in determining when leases are cancelled, default to 1 for standalone.\\n Should be set to 0 for peer replicated eurekas\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.InstanceRegistryProperties\", \"defaultValue\": 1 }, { \"name\": \"eureka.instance.registry.expected-number-of-renews-per-min\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.InstanceRegistryProperties\", \"defaultValue\": 1 }, { \"name\": \"eureka.server.a-s-g-cache-expiry-timeout-ms\", \"type\": \"java.lang.Long\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.a-s-g-query-timeout-ms\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 300 }, { \"name\": \"eureka.server.a-s-g-update-interval-ms\", \"type\": \"java.lang.Long\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.a-w-s-access-id\", \"type\": \"java.lang.String\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\" }, { \"name\": \"eureka.server.a-w-s-secret-key\", \"type\": \"java.lang.String\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\" }, { \"name\": \"eureka.server.batch-replication\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.server.binding-strategy\", \"type\": \"com.netflix.eureka.aws.AwsBindingStrategy\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\" }, { \"name\": \"eureka.server.delta-retention-timer-interval-in-ms\", \"type\": \"java.lang.Long\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.disable-delta\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.server.disable-delta-for-remote-regions\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.server.disable-transparent-fallback-to-other-region\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.server.e-i-p-bind-rebind-retries\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 3 }, { \"name\": \"eureka.server.e-i-p-binding-retry-interval-ms\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.e-i-p-binding-retry-interval-ms-when-unbound\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.enable-replicated-request-compression\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.server.enable-self-preservation\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.server.eviction-interval-timer-in-ms\", \"type\": \"java.lang.Long\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.g-zip-content-from-remote-region\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.server.json-codec-name\", \"type\": \"java.lang.String\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\" }, { \"name\": \"eureka.server.list-auto-scaling-groups-role-name\", \"type\": \"java.lang.String\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": \"ListAutoScalingGroups\" }, { \"name\": \"eureka.server.log-identity-headers\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.server.max-elements-in-peer-replication-pool\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 10000 }, { \"name\": \"eureka.server.max-elements-in-status-replication-pool\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 10000 }, { \"name\": \"eureka.server.max-idle-thread-age-in-minutes-for-peer-replication\", \"type\": \"java.lang.Long\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 15 }, { \"name\": \"eureka.server.max-idle-thread-in-minutes-age-for-status-replication\", \"type\": \"java.lang.Long\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 10 }, { \"name\": \"eureka.server.max-threads-for-peer-replication\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 20 }, { \"name\": \"eureka.server.max-threads-for-status-replication\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 1 }, { \"name\": \"eureka.server.max-time-for-replication\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 30000 }, { \"name\": \"eureka.server.min-threads-for-peer-replication\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 5 }, { \"name\": \"eureka.server.min-threads-for-status-replication\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 1 }, { \"name\": \"eureka.server.number-of-replication-retries\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 5 }, { \"name\": \"eureka.server.peer-eureka-nodes-update-interval-ms\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.peer-eureka-status-refresh-time-interval-ms\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.peer-node-connect-timeout-ms\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 200 }, { \"name\": \"eureka.server.peer-node-connection-idle-timeout-seconds\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 30 }, { \"name\": \"eureka.server.peer-node-read-timeout-ms\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 200 }, { \"name\": \"eureka.server.peer-node-total-connections\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 1000 }, { \"name\": \"eureka.server.peer-node-total-connections-per-host\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 500 }, { \"name\": \"eureka.server.prime-aws-replica-connections\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.server.property-resolver\", \"type\": \"org.springframework.core.env.PropertyResolver\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\" }, { \"name\": \"eureka.server.rate-limiter-burst-size\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 10 }, { \"name\": \"eureka.server.rate-limiter-enabled\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.server.rate-limiter-full-fetch-average-rate\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 100 }, { \"name\": \"eureka.server.rate-limiter-privileged-clients\", \"type\": \"java.util.Set&lt;java.lang.String&gt;\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\" }, { \"name\": \"eureka.server.rate-limiter-registry-fetch-average-rate\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 500 }, { \"name\": \"eureka.server.rate-limiter-throttle-standard-clients\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": false }, { \"name\": \"eureka.server.registry-sync-retries\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.registry-sync-retry-wait-ms\", \"type\": \"java.lang.Long\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.remote-region-app-whitelist\", \"type\": \"java.util.Map&lt;java.lang.String,java.util.Set&lt;java.lang.String&gt;&gt;\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\" }, { \"name\": \"eureka.server.remote-region-connect-timeout-ms\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 1000 }, { \"name\": \"eureka.server.remote-region-connection-idle-timeout-seconds\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 30 }, { \"name\": \"eureka.server.remote-region-fetch-thread-pool-size\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 20 }, { \"name\": \"eureka.server.remote-region-read-timeout-ms\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 1000 }, { \"name\": \"eureka.server.remote-region-registry-fetch-interval\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 30 }, { \"name\": \"eureka.server.remote-region-total-connections\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 1000 }, { \"name\": \"eureka.server.remote-region-total-connections-per-host\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 500 }, { \"name\": \"eureka.server.remote-region-trust-store\", \"type\": \"java.lang.String\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": \"\" }, { \"name\": \"eureka.server.remote-region-trust-store-password\", \"type\": \"java.lang.String\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": \"changeit\" }, { \"name\": \"eureka.server.remote-region-urls\", \"type\": \"java.lang.String[]\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\" }, { \"name\": \"eureka.server.remote-region-urls-with-name\", \"type\": \"java.util.Map&lt;java.lang.String,java.lang.String&gt;\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\" }, { \"name\": \"eureka.server.renewal-percent-threshold\", \"type\": \"java.lang.Double\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0.85 }, { \"name\": \"eureka.server.renewal-threshold-update-interval-ms\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.response-cache-auto-expiration-in-seconds\", \"type\": \"java.lang.Long\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 180 }, { \"name\": \"eureka.server.response-cache-update-interval-ms\", \"type\": \"java.lang.Long\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.retention-time-in-m-s-in-delta-queue\", \"type\": \"java.lang.Long\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.route53-bind-rebind-retries\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 3 }, { \"name\": \"eureka.server.route53-binding-retry-interval-ms\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.route53-domain-t-t-l\", \"type\": \"java.lang.Long\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 30 }, { \"name\": \"eureka.server.sync-when-timestamp-differs\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.server.use-read-only-response-cache\", \"type\": \"java.lang.Boolean\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": true }, { \"name\": \"eureka.server.wait-time-in-ms-when-sync-empty\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\", \"defaultValue\": 0 }, { \"name\": \"eureka.server.xml-codec-name\", \"type\": \"java.lang.String\", \"sourceType\": \"org.springframework.cloud.netflix.eureka.server.EurekaServerConfigBean\" } ], \"hints\": []}","link":"/2019/10/13/euraka-配置介绍/"},{"title":"HandlerInterceptor","text":"Spring Boot使用处理器拦截器HandlerInterceptor 首先我们说说什么是处理器拦截器，SpringWebMVC的处理器拦截器，类似于Servlet开发中的过滤器Filter，用于处理器进行预处理和后处理。 使用场景 1、日志记录：记录请求信息的日志，以便进行信息监控、信息统计、计算PV（Page View）等. 2、权限检查：如登录检测，进入处理器检测检测是否登录，如果没有直接返回到登录页面； 3、性能监控：有时候系统在某段时间莫名其妙的慢，可以通过拦截器在进入处理器之前记录开始时间，在处理完后记录结束时间，从而得到该请求的处理时间（如果有反向代理，如apache可以自动记录）； 4、通用行为：读取cookie得到用户信息并将用户对象放入请求，从而方便后续流程使用，还有如提取Locale、Theme信息等，只要是多个处理器都需要的即可使用拦截器实现。 5、OpenSessionInView：如Hibernate，在进入处理器打开Session，在完成后关闭Session。…………本质也是AOP（面向切面编程），也就是说符合横切关注点的所有功能都可以放入拦截器实现。 实现方式 1、引入spring-boot-starter-web在pom.xml 中引入spring-boot-starter-web包。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 2、建立拦截器preHandle ：在DispatcherServlet之前执行。postHandle：在controller执行之后的DispatcherServlet之后执行。afterCompletion：在页面渲染完成返回给客户端之前执行。 1234567891011121314151617181920212223242526272829303132333435363738394041public class PiceaInterceptor implements HandlerInterceptor { /** *在DispatcherServlet之前执行 * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(\"我是在DispatcherServlet之前执行的方法\"); return true; } /** * 在controller执行之后的DispatcherServlet之后执行 * @param request * @param response * @param handler * @param modelAndView * @throws Exception */ @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { System.out.println(\"在controller执行之后的DispatcherServlet之后执行\"); } /** * 在页面渲染完成返回给客户端之前执行 * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { System.out.println(\"在页面渲染完成返回给客户端之前执行\"); }} 3、建立WebAppConfigurer类这个类的主要作用是注册上我们配置的拦截器。 12345678910111213@Configurationpublic class PiceaWebAppConfigurer implements WebMvcConfigurer { @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(authenticationInterceptor())//增加过滤的方法类 .addPathPatterns(\"/**\");//定义过滤的范围 } @Bean public PiceaInterceptor authenticationInterceptor() { return new PiceaInterceptor(); }} 4、建立Contoller类这个类比较简单，不做特别说明 12345678@RestControllerpublic class PiceaContoller { @RequestMapping(\"/query\") public void asyncTask() throws Exception { System.out.println(\"我是控制类里面的方法，我正在思考...............\"); }} 5、测试结果在浏览中输入：http://localhost:2001/query这个时候控制台的输入为如下图片。 Spring-boot-fi-interceptor.png","link":"/2019/07/15/handleintercepertor/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/06/24/hello-world/"},{"title":"hexo show imgs","text":"Hexo快速发布博文及插入图片 md文件放在hexo网站所在位置下的source_posts目录，这个目录下存储了很多个md文件，每个文件对应着一篇博客。在博客站点文件夹输入：hexo g，生成静态页面，再输入：hexo server，到localhost:4000预览博客效果，最后输入：hexo d，部署；第四步可简单输入命令：hexo d -g解决利用csdn发布博客图片不显示的问题1、 将主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true；2、在hexo目录下执行npm install hexo-asset-image –save，这是下载安装一个可以上传本地图片的插件；3、稍等片刻，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹4、 在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片：![这里输入图片描述](/xxxx/图片名.jpg) 注意： 此处xxxx代表的是新建博文md文件的名字，也是同名文件夹的名字 如下图：具体引入路径如下：/Hexo快速发布博文及插入图片/图片显示问题.png","link":"/2019/07/16/hexo-show-imgs/"},{"title":"idea springboot autowired hide red underline","text":"SpringBoot：Dao层mapper注入报红问题 在mapper接口上加上 @Component注解例如: import com.example.sptingboot_mybatis.model.User; import org.apache.ibatis.annotations.Mapper; import org.springframework.stereotype.Component; import java.util.List; @Mapper @Component(&quot;IUser&quot;) public interface IUser { List&lt;User&gt; findAll(); }就可以解决问题 原文链接：https://blog.csdn.net/Sir_He/article/details/81879854","link":"/2019/07/16/idea-springboot-autowired-show-redunderline/"},{"title":"破解idea 2019.2","text":"idea.vmoptions文件加上 -javaagent:jetbrains-agent.jar 激活码 1MERYY2SM09-eyJsaWNlbnNlSWQiOiJNRVJZWTJTTTA5IiwibGljZW5zZWVOYW1lIjoiRnFHdW1wIiwiYXNzaWduZWVOYW1lIjoiIiwiYXNzaWduZWVFbWFpbCI6IiIsImxpY2Vuc2VSZXN0cmljdGlvbiI6IiIsImNoZWNrQ29uY3VycmVudFVzZSI6ZmFsc2UsInByb2R1Y3RzIjpbeyJjb2RlIjoiSUkiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiQUMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiRFBOIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlBTIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkdPIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkRNIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkNMIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlJTMCIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSQyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSRCIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJQQyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSTSIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJXUyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJEQiIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJEQyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSU1UiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In1dLCJoYXNoIjoiMTI3OTY4NzcvMCIsImdyYWNlUGVyaW9kRGF5cyI6NywiYXV0b1Byb2xvbmdhdGVkIjpmYWxzZSwiaXNBdXRvUHJvbG9uZ2F0ZWQiOmZhbHNlfQ==-i6ySGpwlyxeDfJjDUa3I4a6G1iWhhvXKP4eP9sw3wGDGKoAnz+VNTwxTHe4T0ioaibUU2ts++TewhtmV1/MXkdo0E2yiAo4rQZAp38KKGFfF9qXpOj5+3ylubo2FXmSmmm++bomgbT4RHEHQg+Oi++dJrhlzoTDg3ZMNNvDP9lC6LvwCrKxmiRbL34wOa+NjnUO8JLX3SCl0+PZA/TNp4Jl/F1G4yE6njPSUUjsNbNGiMI/T0JyJ4nYEdmPS34BRk/TzgxuNSWfARt3eBTj807f8RyLtfAFlO04RoVsbRfgX6QAjgS3URiWf/u80t5fip5GHmrgpqB5F2liMLrfnfg==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ndaik1GD0nyTdqkZgURQZGW+RGxCdBITPXIwpjhhaD0SXGa4XSZBEBoiPdY6XV6pOfUJeyfi9dXsY4MmT0D+sKoST3rSw96xaf9FXPvOjn4prMTdj3Ji3CyQrGWeQU2nzYqFrp1QYNLAbaViHRKuJrYHI6GCvqCbJe0LQ8qqUiVMA9wG/PQwScpNmTF9Kp2Iej+Z5OUxF33zzm+vg/nYV31HLF7fJUAplI/1nM+ZG8K+AXWgYKChtknl3sW9PCQa3a3imPL9GVToUNxc0wcuTil8mqveWcSQCHYxsIaUajWLpFzoO2AhK4mfYBSStAqEjoXRTuj17mo8Q6M2SHOcwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQBonMu8oa3vmNAa4RQP8gPGlX3SQaA3WCRUAj6Zrlk8AesKV1YSkh5D2l+yUk6njysgzfr1bIR5xF8eup5xXc4/G7NtVYRSMvrd6rfQcHOyK5UFJLm+8utmyMIDrZOzLQuTsT8NxFpbCVCfV5wNRu4rChrCuArYVGaKbmp9ymkw1PU6+HoO5i2wU3ikTmRv8IRjrlSStyNzXpnPTwt7bja19ousk56r40SmlmC04GdDHErr0ei2UbjUua5kw71Qn9g02tL9fERI2sSRjQrvPbn9INwRWl5+k05mlKekbtbu2ev2woJFZK4WEXAd/GaAdeZZdumv8T2idDFL7cAirJwcrbfpawPeXr52oKTPnXfi0l5+g9Gnt/wfiXCrPElX6ycTR6iL3GC2VR4jTz6YatT4Ntz59/THOT7NJQhr6AyLkhhJCdkzE2cob/KouVp4ivV7Q3Fc6HX7eepHAAF/DpxwgOrg9smX6coXLgfp0b1RU2u/tUNID04rpNxTMueTtrT8WSskqvaJd3RH8r7cnRj6Y2hltkja82HlpDURDxDTRvv+krbwMr26SB/40BjpMUrDRCeKuiBahC0DCoU/4+ze1l94wVUhdkCfL0GpJrMSCDEK+XEurU18Hb7WT+ThXbkdl6VpFdHsRvqAnhR2g4b+Qzgidmuky5NUZVfEaZqV/g==","link":"/2019/10/24/idea2019.2破解/"},{"title":"ide配置自定义模板","text":"设置类注释模板1.选择File–&gt;Settings–&gt;Editor–&gt;File and Code Templates–&gt;Includes–&gt;File Header. 2.在右边空白处，编写自己的模板即可，注意Scheme是模板的生效范围，可选变量在description有介绍，附图中本人使用的模板（${USER}为计算机用户名，可以自行修改）。 12345/** * @Auther: ${USER} * @Date: ${DATE} ${HOUR}:${MINUTE} * @Description: */ 3.设置完成后，创建类时自动生成注释，效果如下。 设置方法注释模板Idea没有可以直接设置方法注释模板的地方，可以借用Live Templates基本实现，步骤如下。1.选择File–&gt;Settings–&gt;Editor–&gt;Live Templates，先选择右侧绿色加号新建一个自己的模板组，如图命名为myGroup。 2.选中已建好的组，选择右侧绿色加号新建模板，如下图。 3.填好Abbreviation（快捷输入），Description（描述）和模板内容（图中模板如下） 123456789/** * * 功能描述: * * @param: $param$ * @return: $return$ * @auther: $user$ * @date: $date$ $time$ */ 4.点击Define，勾选Java5.点击Edit variables编辑变量，设置如下，点击Ok–&gt;Apply完成设置。 6.输入“/**”，然后按Tab键即可生成注释，如下图。注意此方式有个缺点，需要在方法内部生成，否则@param为null。","link":"/2019/10/12/ide配置自定义模板/"},{"title":"hexo域名配置","text":"Hexo站点之域名配置 摘要 因为Hexo个人博客是托管在github之上，每次访问都要使用githubname.github.io这么一个长串的域名来访问，会显得非常繁琐。这个时候我们可以购买一个域名，设置DNS跳转，以达到通过域名即可访问我们的个人博客。通过查阅文档发现，github pages是支持域名绑定的。购买域名 国内国外有很多的域名供应商，选择一个好的机构购买域名，会为自己的站点配置节约很多时间，也不会因为域名的出错，导致影响百度对我们个人博客的收录。近几年来，国内做的比较好的域名供应商有阿里的万网。我就是在阿里的万网购买的域名。通过查找，找到自己喜欢的域名，后来为了解决成本，我选了.top结尾的域名，一年只需要4块钱，很便宜，建议如果只是作为自己的博客使用建议不要购买.com的域名。(注意：购买.top域名之后，大概6个小时之内就会生效，5之内必须对域名进行认证，超过5天没有认证域名将会被锁定。)域名解析 登录进入万网的域名控制台，点击”域名和网站”中的”云DNS” 点击对应域名的”解析” 点击添加解析，记录类型选A或CNAME，A记录的记录值就是ip地址，github(官方文档)提供了两个IP地址，192.30.252.153和192.30.252.154，这两个IP地址为github的服务器地址，两个都要填上，解析记录设置两个www和@，线路就默认就行了，CNAME记录值填你的github博客网址。如我的是whitescholars.github.io。 这些全部设置完成后，此时你并不能要申请的域名访问你的博客。接着你需要做的是在hexo根目录的source文件夹里创建CNAME文件，不带任何后缀，里面添加你的域名信息，如：penglei.com。实践证明如果此时你填写的是www.penglei.top那么以后你只能用www.penglei.top访问，而如果你填写的是penglei.top。那么用www.penglei.top和penglei.top访问都是可以的。重新清理hexo,并发布即可用新的域名访问。 搭建完成访问出现404 可能的原因是： 绑定了个人域名，但是域名解析错误。域名解析正确但你的域名是通过国内注册商注册的，你的域名因没有实名制而无法访问。你认为配置没有问题，那么可能只是你的浏览器在捣鬼，可尝试清除浏览器缓存再访问或者换个浏览器访问。也有可能是你的路由器缓存导致的错觉，所以也可以尝试换个局域网访问你的网站。最有可能的原因是你下载的hexo有问题，导致所有的东西都上传到了github,而导致index页面在主域名的下一级目录。你可以尝试查看上传的内容，找到index页面，在域名后面添加下一级目录。若能访问index页面（此时样式可能是乱的），则证明是hexo安装有问题，笔者当时遇到的就是这个问题。可卸载重新安装。注：1，2默认你的CNAME文件配置没有问题，如果没有绑定个人域名，则不需要CNAME文件。","link":"/2019/08/31/hexo域名配置/"},{"title":"java使用listener","text":"定义用于监听Web应用的内部事件的实现类。可以监听用户session的开始与结束，用户请求的到达等等，当事件发生时，会回调监听器的内部方法使用Listener步骤通过实现具体接口创建实现类（可实现多个监听器接口）配置实现类成为监听器，有两种配置方式：直接用@WebListener注解修饰实现类通过web.xml方式配置，代码如下： 123&lt;listener&gt; &lt;listener-class&gt;com.zrgk.listener.MyListener&lt;/lisener-class&gt;&lt;/listener&gt; 常用Web事件监听器接口 ServletContextListener 1.该接口用于监听Web应用的启动与关闭 2.该接口的两个方法： 1.contextInitialized(ServletContextEvent event); // 启动web应用时调用 2.contextDestroyed(ServletContextEvent event); // 关闭web应用时调用 3.如何获得application对象：ServletContext application = event.getServletContext(); 4.示例 1234567891011121314151617181920@WebListenerpublic class MyServetContextListener implements ServletContextListener{ //web应用关闭时调用该方法 @Override public void contextDestroyed(ServletContextEvent event) { ServletContext application = event.getServletContext(); String userName = application.getInitParameter(\"userName\"); System.out.println(\"关闭web应用的用户名字为：\"+userName); } //web应用启动时调用该方法 @Override public void contextInitialized(ServletContextEvent event) { ServletContext application = event.getServletContext(); String userName = application.getInitParameter(\"userName\"); System.out.println(\"启动web应用的用户名字为：\"+userName); }} ServletContextAttributeListener 1.该接口用于监听ServletContext范围（application）内属性的改变。 2.该接口的两个方法： 1.attributeAdded(ServletContextAttributeEvent event); // 当把一个属性存进application时触发 2.attributeRemoved(ServletContextAttributeEvent event); // 当把一个属性从application删除时触发 3.attributeReplaced(ServletContextAttributeEvent event); // 当替换application内的某个属性值时触发 3.如何获得application对象：ServletContext application = event.getServletContext(); 4.示例 12345678910111213141516171819202122232425@WebListenerpublic class MyServletContextAttributeListener implements ServletContextAttributeListener{ //向application范围内添加一个属性时触发 @Override public void attributeAdded(ServletContextAttributeEvent event) { String name = event.getName();//向application范围添加的属性名 Object val = event.getValue(); //向application添加的属性对应的属性值 System.out.println(\"向application范围内添加了属性名为：\"+name+\"，属性值为：\"+val+\"的属性\"); } //删除属性时触发 @Override public void attributeRemoved(ServletContextAttributeEvent event) { // ... } //替换属性值时触发 @Override public void attributeReplaced(ServletContextAttributeEvent event) { // ... }} ServletRequestListener与ServletRequestAttributeListener 1.ServletRequestListener用于监听用户请求，而ServletRequestAttributeListener用于监听request范围内属性的变化。 2.ServletRequestListener两个需要实现的方法 1.requestInitialized(ServletRequestEvent event); //用户请求到达、被初始化时触发 2.requestDestroyed(ServletRequestEvent event); // 用户请求结束、被销毁时触发 3.ServletRequestAttributeListener两个需要实现的方法 1.attributeAdded(ServletRequestAttributeEvent event); // 向request范围内添加属性时触发 2.attributeRemoved(ServletRequestAttributeEvent event); // 从request范围内删除某个属性时触发 3.attributeReplaced(ServletRequestAttributeEvent event); // 替换request范围内某个属性值时触发 4.获取reqeust对象HttpServletRequest req = (HttpServletRequest)event.getServletRequest(); 5.代码片 123456789101112131415161718192021222324252627282930313233343536373839@WebListenerpublic class MyRequestListener implements ServletRequestListener,ServletRequestAttributeListener{ //用户请求结束、被销毁时触发 @Override public void requestDestroyed(ServletRequestEvent event) { HttpServletRequest req = (HttpServletRequest) event.getServletRequest(); String ip = req.getRemoteAddr(); System.out.println(\"IP为:\"+ip+\"的用户发送到\"+req.getRequestURI()+\"的请求结束\"); } //用户请求到达、被初始化时触发 @Override public void requestInitialized(ServletRequestEvent event) { HttpServletRequest req = (HttpServletRequest) event.getServletRequest(); String ip = req.getRemoteAddr(); System.out.println(\"IP为:\"+ip+\"的用户发送到\"+req.getRequestURI()+\"的请求被初始化\"); } //向request范围内添加属性时触发 @Override public void attributeAdded(ServletRequestAttributeEvent event) { String name = event.getName(); Object val = event.getValue(); System.out.println(\"向request范围内添加了名为：\"+name+\"，值为：\"+val+\"的属性\"); } //删除request范围内某个属性时触发 @Override public void attributeRemoved(ServletRequestAttributeEvent event) { //... } //替换request范围内某个属性值时触发 @Override public void attributeReplaced(ServletRequestAttributeEvent event) { // ... }} HttpSessionListener与HttpSessionAttributeListener 1.HttpSessionListener监听用户session的开始与结束，而HttpSessionAttributeListener监听HttpSession范围（session）内的属性的改变。 2.HttpSessionListener要实现的方法： 1.sessionCreated(HttpSessionEvent event); // 用户与服务器的会话开始、创建时触发 2.sessionDestroyed(HttpSessionEvent event); // 用户与服务器的会话结束时触发 3.HttpSessionAttributeListener要实现的方法： 1.attributeAdded(HttpSessionBindingEvent event) ; // 向session范围内添加属性时触发 2.attributeRemoved(HttpSessionBindingEvent event); // 删除session范围内某个属性时触发 3.attributeReplaced(HttpSessionBindingEvent event); // 替换session范围内某个属性值时触发 3.如何得到session对象HttpSession session = event.getSession(); 4.代码片 1234567891011121314151617181920212223242526272829303132333435@WebListenerpublic class MySessionListener implements HttpSessionListener,HttpSessionAttributeListener { //建立session会话时触发 @Override public void sessionCreated(HttpSessionEvent event) { HttpSession session = event.getSession(); String sessionId = session.getId(); System.out.println(\"建立了会话，会话ID为：\"+sessionId); } @Override public void sessionDestroyed(HttpSessionEvent event) { // ... } //向session范围内添加属性时触发 @Override public void attributeAdded(HttpSessionBindingEvent event) { String name = event.getName(); Object val = event.getValue(); System.out.println(\"向session范围内添加了名为：\"+name+\",值为：\"+val+\"的属性\"); } @Override public void attributeRemoved(HttpSessionBindingEvent event) { // ... } @Override public void attributeReplaced(HttpSessionBindingEvent event) { // ... }}","link":"/2019/09/28/java使用listener/"},{"title":"java 动态代理","text":"retrofit是一个解耦性非常高的网络请求框架，最近在研究的时候发现了动态代理这个非常强大且实用的技术，这篇文章将作为retrofit的前置知识，让大家认识：动态代理有哪些应用场景,什么是动态代理，怎样使用，它的局限性在什么地方？ 动态代理的应用场景 AOP—面向切面编程，程序解耦 简言之当你想要对一些类的内部的一些方法，在执行前和执行后做一些共同的的操作，而在方法中执行个性化操作的时候–用动态代理。在业务量庞大的时候能够降低代码量，增强可维护性。 想要自定义第三放类库中的某些方法 我引用了一个第三方类库，但他的一些方法不满足我的需求，我想自己重写一下那几个方法，或在方法前后加一些特殊的操作–用动态代理。但需要注意的是，这些方法有局限性，我会在稍后说明。 什么是动态代理 以上的图太过于抽象，我们从生活中的例子开始切入。 假如你是一个大房东（被代理人），你有很多套房子想要出租，而你觉得找租客太麻烦，不愿意自己弄，因而你找一个人来代理你（代理人），帮打理这些东西，而这个人（代理人也就是中介）在帮你出租房屋的时候对你收取一些相应的中介费（对房屋出租的一些额外操作）。对于租客而言，中介就是房东，代理你做一些事情。 以上，就是一个代理的例子，而他为什么叫动态代理，“动态”两个字体现在什么地方？ 我们可以这样想，如果你的每一套房子你都请一个代理人帮你打理，每当你想再出租一套房子的时候你得再请一个，这样你会请很多的代理人，花费高额的中介成本，这可以看作常说的“静态代理”。 但假如我们把所有的房子都交给一个中介来代理，让他在多套房子之间动态的切换身份，帮你应付每一个租客。这就是一个“动态代理”的过程。动态代理的一大特点就是编译阶段没有代理类在运行时才生成代理类。 我们用一段代码来看一下 房屋出租的操作 12345678910111213/***定义一个接口**/public interface RentHouse {void rent();//房屋出租void charge(String str);//出租费用收取} 房东 123456789101112131415public class HouseOwner implements RentHouse {public void rent() {System.out.println(\"I want to rent my house\");}public void charge(String str) {System.out.println(\"You get : \" + str + \" RMB HouseCharge.\");}} 中介 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class DynamicProxy implements InvocationHandler {// 这个就是我们要代理的真实对象，即房东private Object subject;// 构造方法，给我们要代理的真实对象赋初值public DynamicProxy(Object subject) {this.subject = subject;}@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable {// 在代理真实对象前我们可以添加一些自己的操作，中介收取中介费System.out.println(\"before \"+method.getName()+\" house\");System.out.println(\"Method:\" + method.getName());// 如果方法是 charge 则中介收取100元中介费if (method.getName().equals(\"charge\")) {method.invoke(subject, args);System.out.println(\"I will get 100 RMB ProxyCharge.\");} else {// 当代理对象调用真实对象的方法时，其会自动的跳转到代理对象关联的handler对象的invoke方法来进行调用method.invoke(subject, args);}// 在代理真实对象后我们也可以添加一些自己的操作System.out.println(\"after \"+method.getName()+\" house\");return null;}} 客人 123456789101112131415161718192021222324252627282930313233343536373839public class Client {public static void main(String[] args){// 我们要代理的真实对象--房东HouseOwner houseOwner = new HouseOwner();// 我们要代理哪个真实对象，就将该对象传进去，最后是通过该真实对象来调用其方法的InvocationHandler handler = new DynamicProxy(houseOwner);/** 通过Proxy的newProxyInstance方法来创建我们的代理对象，我们来看看其三个参数* 第一个参数 handler.getClass().getClassLoader() ，我们这里使用handler这个类的ClassLoader对象来加载我们的代理对象* 第二个参数realSubject.getClass().getInterfaces()，我们这里为代理对象提供的接口是真实对象所实行的接口，表示我要代理的是该真实对象，这样我就能调用这组接口中的方法了* 第三个参数handler， 我们这里将这个代理对象关联到了上方的 InvocationHandler 这个对象上*/RentHouse rentHouse = (RentHouse) Proxy.newProxyInstance(handler.getClass().getClassLoader(), houseOwner.getClass().getInterfaces(), handler);//一个动态代理类，中介System.out.println(rentHouse.getClass().getName());rentHouse.rent();rentHouse.charge(\"10000\");}} 我们来看一下输出 123456789101112131415161718192021com.sun.proxy.$Proxy0before rent houseMethod:rentI want to rent my houseafter rent housebefore charge houseMethod:chargeYou get : 10000 RMB HouseCharge.I will get 100 RMB ProxyCharge.after charge houseProcess finished with exit code 0 输出里有 before rent house以及after rent house，说明我们可以在方法的前后增加操作。再看输出 I will get 100 RMB ProxyCharge. 中介收取了100块的中介费，说明我们不仅可以增加操作，甚至可以替换该方法或者直接让该方法不执行。 刚开始看代码你可能会有很多疑惑，我们通过以下的内容来看看动态代理应该怎么用。 动态代理该如何使用 在java的动态代理机制中，有两个重要的类和接口，一个是InvocationHandler(Interface)、另一个则是Proxy(Class)，这一个类和接口是实现我们动态代理所必须用到的。 每一个动态代理类都必须要实现InvocationHandler这个接口（代码中的中介），并且每个代理类的实例都关联到了一个handler，当我们通过代理对象调用一个方法的时候，这个方法的调用就会被转发为由InvocationHandler这个接口的invoke（对方法的增强就写在这里面） 方法来进行调用。 Object invoke(Object proxy, Method method, Object[] args) throws Throwable 我们看到这个方法一共接受三个参数，那么这三个参数分别代表什么呢？ Object invoke(Object proxy, Method method, Object[] args) throws Throwable //proxy: 指代我们所代理的那个真实对象 //method: 指代的是我们所要调用真实对象的某个方法的Method对象 //args: 指代的是调用真实对象某个方法时接受的参数 接下来我们来看看Proxy这个类 public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException Proxy这个类的作用就是用来动态创建一个代理对象的类，它提供了许多的方法，但是我们用的最多的就是 newProxyInstance 这个方法： public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException 这个方法的作用就是得到一个动态的代理对象，其接收三个参数，我们来看看这三个参数所代表的含义 public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException //loader: 一个ClassLoader对象，定义了由哪个ClassLoader对象来对生成的代理对象进行加载 //interfaces: 一个Interface对象的数组，表示的是我将要给我需要代理的对象提供一组什么接口，如果我提供了一组接口给它，那么这个代理对象就宣称实现了该接口(多态)，这样我就能调用这组接口中的方法了 //h: 一个InvocationHandler对象，表示的是当我这个动态代理对象在调用方法的时候，会关联到哪一个InvocationHandler对象上 这样一来，结合上面给出的代码，我们就可以明白动态代理的使用方法了 动态代理的局限性 从动态代理的使用方法中我们看到其实可以被增强的方法都是实现了借口的（不实现借口的public方法也可以通过继承被代理类来使用），代码中的HouseOwner继承了RentHouse 。而对于private方法JDK的动态代理无能为力！ 以上的动态代理是JDK的，对于java工程还有大名鼎鼎的CGLib，但遗憾的是CGLib并不能在android中使用，android虚拟机相对与jvm还是有区别的。 结束语 动态代理的使用场景远不止这些，内部原理会在以后的文章中介绍，但应用类反射临时生成代理类这一机制决定它对性能会有一定的影响。本文作为retrofit原理的前置文章并没有太过详尽，如有疏漏和错误，欢迎指正，如果觉得不错的朋友也请帮我点个关注，你的喜欢是我最大的动力~！","link":"/2019/07/26/java-动态代理/"},{"title":"java反射调用mapper中的接口","text":"Ijava中的反射需要一个实例，但是接口无法提供这样的实例，但是JDK提供了一个叫做动态代理的东西，这个代理恰恰只能代理接口。所以我们想要反射接口需要使用这个动态代理来做。 在java的动态代理机制中，有两个重要的东西，一个是 InvocationHandler(接口)、另一个则是 Proxy(类)，这是我们动态代理必须用到的两个东西。 首先创建一个接口（以studentMapper为例，其中提供了一个根据ID获取student对象的方法）： 123456public interface StudentMapper{ /** * 根据id查对象 */ Student selectById(@Param(\"id\") Integer id); } 现在如果我们需要反射使用该接口根据学生ID获取学生对象是无法直接反射调取的，所以我们需要一个动态代理类，下面创建一个MyInvocationHandler，需要实现上面说的InvocationHandler接口： 12345678910111213public class MyInvocationHandler implements InvocationHandler { private Object target; public MyInvocationHandler(Object target) { this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return method.invoke(target,args); }} 其中该类设置了一个target属性，该属性即为需要代理的接口对象，也就是studentMapper，还提供了一个需要一个参数的构造函数。 自此，我们的代理工作基本做完，现在需要调用一下这个代理类 123456789101112131415try{ Class interfaceImpl = Class.forName(\"StudentMapper\");//这里要写全类名 Object instance = Proxy.newProxyInstance( interfaceImpl.getClassLoader(), new Class[]{interfaceImpl}, new MyInvocationHandler(sqlSession.getMapper(interfaceImpl)) ); Method method = instance.getClass().getMethod(\"selectById\", Integer.class); method.invoke(instance,2); }catch(Exception e){ e.printStackTrace();}} 这里需要注意，newProxyInstance()方法中最后一个参数，即为我们创建的动态代理的类（因为我这里调用的接口为mybatis中mapper中的接口，所以需要从sqlSession中getMapper）. 最后得到该Mapper的接口之后反射调用selectById方法即可。","link":"/2019/09/18/java反射调用mapper中的接口/"},{"title":"Java 文件注释","text":"Java 语言支持三种注释形式： 注释 描述 /text/ 编译器忽略 / 到 / 的所有东西 //text 编译器忽略从 // 到一行末尾的所有东西｜ /** documentation */ 这是文档注释并且通常而言它被叫做 doc comment。JDK javadoc 工具当准备自动准备生成文件时使用 doc comment 这个指导是关于解释 Javadoc 的。我们将看到我们怎样能利用 Javadoc 来为我们的 Java 代码生成有用的文件。 什么是 Javadoc？Javadoc 是 JDK 附带的一个工具，它被用来生成从需要预定义格式的文档的 Java 源代码至 HTML 格式的 Java 代码文件。 以下是一个简单的例子，其中红色部分代表 Java 注释： 1234567891011121314/*** The HelloWorld program implements an application that* simply displays \"Hello World!\" to the standard output.** @author Zara Ali* @version 1.0* @since 2014-03-31 */public class HelloWorld { public static void main(String[] args) { // Prints Hello, World! on standard output. System.out.println(\"Hello World!\"); }} 你可以将需要的 HTML 标签包括在描述部分内，比如，下面的例子利用 &lt;h1&gt;…&lt;/h1&gt; 来定义头部和 &lt;p&gt; 被用来创建段落间隔： 12345678910111213141516171819/*** &lt;h1&gt;Hello, World!&lt;/h1&gt;* The HelloWorld program implements an application that* simply displays \"Hello World!\" to the standard output.* &lt;p&gt;* Giving proper comments in your program makes it more* user friendly and it is assumed as a high quality code.* ** @author Zara Ali* @version 1.0* @since 2014-03-31 */public class HelloWorld { public static void main(String[] args) { // Prints Hello, World! on standard output. System.out.println(\"Hello World!\"); }} Javadoc 标签Javadoc 标签是 Javadoc 认可的关键字，它定义了下面信息的类型。 Javadoc 工具认可下面的标签： 标签 描述 语法 @author 添加类的作者 @author name-text {@code} 不把文本转换成 HTML 标记和嵌套的 Java 标签而用代码字体展示它 {@code text} {@docRoot} 表示从任何生成页面到生成文档的根目录的相对路径 {@docRoot} @deprecated 添加一个注释暗示 API 应该不再被使用 @deprecated @exception 用类名和描述文本给生成的文档添加一个副标题 @exception class-name description {@inheritDoc} 从最近的可继承的类或可实现的接口继承注释 Inherits a comment from the immediate surperclass. {@link} 用指向特定的包，类或者一个引用类的成员名的文档的可见文本标签插入在线链接 {@link package.class#member label} {@linkplain} 和{@link}相同，除了链接的标签用纯文本标示而不是代码字体 {@linkplain package.class#member label} @param 给“参数”区域添加一个有特定参数名且后跟着特定描述的参数 @param parameter-name description @return 添加一个有描述文本的“Returns”区域 @return description @see 添加带有链接或者指向引用的文本入口的标题“See Also” @see reference @serial 在默认的序列化字段的文本注释中使用 @serial @serialData 记录由 writeObject( ) 或 writeExternal( )方法所写的数据 @serialData data-description @serialField 记录一个 ObjectStreamField 成分 @serialField field-name field-type field-description @since 给生成的文档添加一个带有特定 since 文本的”Since”标题 @since release @throws @throw 和 @exception 标签是同义词 @throws class-name description {@value} 当{@value}被用在一个静态字段的文本注释中，它展示了那个常量的值 {@value package.class#field} @version 当 -version 选项被使用时用特定的 version w文本给生成的文本添加一个“Version”副标题 @version version-text","link":"/2019/10/24/java注释/"},{"title":"java获取IP","text":"2.NetworkInterface.getNetworkInterfaces();可以获取本地所有的ip地址，如docker服务相关的ip等。//all ip 12345678910Enumeration&lt;NetworkInterface&gt; n = NetworkInterface.getNetworkInterfaces();for (; n.hasMoreElements(); ) { NetworkInterface e = n.nextElement(); System.out.println(\"Interface: \" + e.getName()); Enumeration&lt;InetAddress&gt; a = e.getInetAddresses(); for (; a.hasMoreElements(); ) { InetAddress addr = a.nextElement(); System.out.println(\" \" + addr.getHostAddress()); }} 过滤回环网卡、点对点网卡、非活动网卡、虚拟网卡并要求网卡名字是eth或ens开头；再过滤回环地址，并要求是内网地址（非外网） 123456789101112131415161718192021222324252627282930313233343536373839public static List&lt;Inet4Address&gt; getLocalIp4AddressFromNetworkInterface() throws SocketException { List&lt;Inet4Address&gt; addresses = new ArrayList&lt;&gt;(1); Enumeration e = NetworkInterface.getNetworkInterfaces(); if (e == null) { return addresses; } while (e.hasMoreElements()) { NetworkInterface n = (NetworkInterface) e.nextElement(); if (!isValidInterface(n)) { continue; } Enumeration ee = n.getInetAddresses(); while (ee.hasMoreElements()) { InetAddress i = (InetAddress) ee.nextElement(); if (isValidAddress(i)) { addresses.add((Inet4Address) i); } } } return addresses;}/** * 过滤回环网卡、点对点网卡、非活动网卡、虚拟网卡并要求网卡名字是eth或ens开头 * * @param ni 网卡 * @return 如果满足要求则true，否则false */private static boolean isValidInterface(NetworkInterface ni) throws SocketException { return !ni.isLoopback() &amp;&amp; !ni.isPointToPoint() &amp;&amp; ni.isUp() &amp;&amp; !ni.isVirtual() &amp;&amp; (ni.getName().startsWith(\"eth\") || ni.getName().startsWith(\"ens\"));}/** * 判断是否是IPv4，并且内网地址并过滤回环地址. */private static boolean isValidAddress(InetAddress address) { return address instanceof Inet4Address &amp;&amp; address.isSiteLocalAddress() &amp;&amp; !address.isLoopbackAddress();} 3.通过建立UDP连接，让系统通过路由表自己选择一个主要的ip地址的方式。来获取ip地址 1234try(final DatagramSocket socket = new DatagramSocket()){ socket.connect(InetAddress.getByName(\"8.8.8.8\"), 10002); ip = socket.getLocalAddress().getHostAddress();} 另一种写法 1234567891011private static Optional&lt;Inet4Address&gt; getIpBySocket() throws SocketException { try (final DatagramSocket socket = new DatagramSocket()) { socket.connect(InetAddress.getByName(\"8.8.8.8\"), 10002); if (socket.getLocalAddress() instanceof Inet4Address) { return Optional.of((Inet4Address) socket.getLocalAddress()); } } catch (UnknownHostException e) { throw new RuntimeException(e); } return Optional.empty();}","link":"/2020/02/28/java获取IP/"},{"title":"java获取post请求参数","text":"123456789101112131415161718192021222324252627282930313233343536373839// 获取post请求参数（getParameter）@RequestMapping(value=\"saveUser\", method=RequestMethod.POST)public String saveUser(HttpServletRequest request){String username= request.getParameter(\"username\");Integer age = Integer.parseInt(request.getParameter(\"age\"));String phone = request.getParameter(\"phone\");String city = request.getParameter(\"city\");String[] interests = request.getParameterValues(\"interests\");Integer sex = Integer.parseInt(request.getParameter(\"sex\"));System.out.println(\"username : \" + username);System.out.println(\"age : \" + age);System.out.println(\"phone : \" + phone);System.out.println(\"city : \" + city);System.out.println(\"interests : \");for(String interest: interests){System.out.println(\" interest : \" + interest);}System.out.println(\"sex : \" + sex);return \"login/welcome\";}","link":"/2019/11/15/java获取post请求参数/"},{"title":"java获取本机ip","text":"获取localhost不一定对 12345678910111213141516171819private String localHost() { String ip = \"\"; try { Enumeration&lt;NetworkInterface&gt; networkInterfaces = NetworkInterface.getNetworkInterfaces(); while (networkInterfaces.hasMoreElements()) { NetworkInterface ni = networkInterfaces.nextElement(); Enumeration&lt;InetAddress&gt; nias = ni.getInetAddresses(); while (nias.hasMoreElements()) { InetAddress ia = nias.nextElement(); if (!ia.isLinkLocalAddress() &amp;&amp; !ia.isLoopbackAddress() &amp;&amp; ia instanceof Inet4Address) { ip =ia.toString().split(\"/\")[1]; } } } } catch (SocketException e) { LOGGER.error(\"控制器ip地址获取UnknownHostException异常：\", e); } return ip; }","link":"/2019/10/24/java获取本机ip/"},{"title":"jetty servlet 开发","text":"Jetty开发指导：Jetty Websocket APIJetty WebSocket API使用Jetty提供了功能更强的WebSocket API，使用一个公共的核心API供WebSockets的服务端和客户端使用。他是一个基于WebSocket消息的事件驱动的API。 WebSocket事件每个WebSocket都能接收多种事件： On Connect Event 表示WebSocket升级成功，WebSocket现在打开。你将收到一个org.eclipse.jetty.websocket.api.Session对象，对应这个Open事件的session。为通常的WebSocket，应该紧紧抓住这个Session，并使用它与Remote Endpoint进行交流。如果为无状态（Stateless）WebSockets，这个Session将被传递到它出现的每一个事件，允许你使用一个WebSocket的1个实例为多个Remote Endpoint提供服务。 On Close Event 表示WebSocket已经关闭。每个Close事件将有一个状态码（Status Code）（和一个可选的Closure Reason Message）。一个通常的WebSocket终止将经历一个关闭握手，Local Endpoint和Remote Endpoint都会发送一个Close帧表示连接被关闭。本地WebSocket可以通过发送一个Close帧到Remote Endpoint表示希望关闭，但是Remote Endpoint能继续发送信息直到它送一个Close帧为止。这被称之为半开（Half-Open）连接，注意一旦Local Endpoint发送了Close帧后，它将不能再发送任何WebSocket信息。在一个异常的终止中，例如一个连接断开或者超时，底层连接将不经历Close Handshake就被终止，这也将导致一个On Close Event（和可能伴随一个On Error Event）。 On Error Event 如果一个错误出现，在实现期间，WebSocket将通过这个事件被通知。 On Message Event 表示一个完整的信息被收到，准备被你的WebSocket处理。这能是一个（UTF8）TEXT信息或者一个原始的BINARY信息。 WebSocket SessionSession对象能被用于： 获取WebSocket的状态连接状态（打开或者关闭） 1234if(session.isOpen()) { // send message} 连接是安全的吗。 123if(session.isSecure()) { // connection is using 'wss://'} 在升级请求和响应中的是什么。 12345UpgradeRequest req = session.getUpgradeRequest();String channelName = req.getParameterMap().get(\"channelName\"); UpgradeRespons resp = session.getUpgradeResponse();String subprotocol = resp.getAcceptedSubProtocol(); 本地和远端地址是什么。 InetSocketAddress remoteAddr = session.getRemoteAddress(); 配置策略获取和设置空闲超时时间。 session.setIdleTimeout(2000); // 2 second timeout 获取和设置最大信息长度。 session.setMaximumMessageSize(64*1024); // accept messages up to 64k, fail if larger 发送信息到Remote Endpoint Session的最重要的特征是获取org.eclipse.jetty.websocket.api.RemoteEndpoint。 使用RemoteEndpoint，你能选择发送TEXT或者BINARY Websocket信息，或者WebSocket PING和PONG控制帧。 12345678910111213实例1 发送二进制信息（阻塞）RemoteEndpoint remote = session.getRemote(); // Blocking Send of a BINARY message to remote endpointByteBuffer buf = ByteBuffer.wrap(new byte[] { 0x11, 0x22, 0x33, 0x44 });try{ remote.sendBytes(buf);}catch (IOException e){ e.printStackTrace(System.err);} 怎么使用RemoteEndpoint送一个简单的二进制信息。这将阻塞直到信息被发送完成，如果不能发送信息可能将抛出一个IOException。 12345678910111213实例2 发送文本信息（阻塞）RemoteEndpoint remote = session.getRemote(); // Blocking Send of a TEXT message to remote endpointtry{ remote.sendString(\"Hello World\");}catch (IOException e){ e.printStackTrace(System.err);} 怎么使用RemoteEndpoint发送文本信息。这将阻塞直到信息发送，如果不能发送信息可能将抛出一个IOException。 发送部分信息如果你有一个大的信息需要被发送，并且想分多次发送，每次一部分，你能使用RemoteEndpoint发送部分信息的方法。仅需要确保你最后发送一个完成发送的信息（isLast == true） 1234567891011121314151617实例3 发送部分二进制信息（阻塞）RemoteEndpoint remote = session.getRemote(); // Blocking Send of a BINARY message to remote endpoint// Part 1ByteBuffer buf1 = ByteBuffer.wrap(new byte[] { 0x11, 0x22 });// Part 2 (last part)ByteBuffer buf2 = ByteBuffer.wrap(new byte[] { 0x33, 0x44 });try{ remote.sendPartialBytes(buf1,false); remote.sendPartialBytes(buf2,true); // isLast is true}catch (IOException e){ e.printStackTrace(System.err);} 怎么分两次发送一个二进制信息，使用在RemoteEndpoint中的部分信息支持方法。这将阻塞直到每次信息发送完成，如果不能发送信息可能抛出一个IOException。 12345678910111213141516实例4 发送部分文本信息（阻塞）RemoteEndpoint remote = session.getRemote(); // Blocking Send of a TEXT message to remote endpointString part1 = \"Hello\";String part2 = \" World\";try{ remote.sendPartialString(part1,false); remote.sendPartialString(part2,true); // last part}catch (IOException e){ e.printStackTrace(System.err);} 怎么通过两次发送一个文本信息，使用在RemoteEndpoint中的部分信息支持方法。这将阻塞直到每次信息发送完成，如果不能发送信息可能抛出一个IOException。 发送Ping/Pong控制帧你也能使用RemoteEndpoint发送Ping和Pong控制帧。 1234567891011121314实例5 发送Ping控制帧（阻塞）RemoteEndpoint remote = session.getRemote(); // Blocking Send of a PING to remote endpointString data = \"You There?\";ByteBuffer payload = ByteBuffer.wrap(data.getBytes());try{ remote.sendPing(payload);}catch (IOException e){ e.printStackTrace(System.err);} 怎么发送一个Ping控制帧，附带一个负载“You There?”（作为一个字节数组负载到达Remote Endpoint）。这将阻塞直到信息发送完成，如果不能发送Ping帧，可能抛出一个IOException。 123456789101112131415实例6 送Pong控制帧（阻塞）RemoteEndpoint remote = session.getRemote(); // Blocking Send of a PONG to remote endpointString data = \"Yup, I'm here\";ByteBuffer payload = ByteBuffer.wrap(data.getBytes());try{ remote.sendPong(payload);}catch (IOException e){ e.printStackTrace(System.err);} 怎么发送一个Pong控制帧，附带一个”Yup I’m here”负载（作为一个字节数组负载到达Remote Endpoint）。这将阻塞直到信息被发送，如果不能发送Pong帧，可能抛出一个IOException。为了正确的使用Pong帧，你应该返回你在Ping帧中收到的同样的字节数组数据。 异步发送信息也存在来年改革异步发送信息的方法可用：1）RemoteEndpoint.sendBytesByFuture（字节信息）2）RemoteEndpoint.sendStringByFuture（字符串信息）两个方法都返回一个Future，使用标准java.util.concurrent.Future行为，能被用于测试信息发送的成功和失败。 123456实例7 送二进制信息（异步）RemoteEndpoint remote = session.getRemote(); // Async Send of a BINARY message to remote endpointByteBuffer buf = ByteBuffer.wrap(new byte[] { 0x11, 0x22, 0x33, 0x44 });remote.sendBytesByFuture(buf); 怎么使用RemoteEndpoint发送一个简单的二进制信息。这个信息将被放入发送队列，你将不知道发送成功或者失败。 1234567891011121314151617实例8 发送二进制信息（异步，等待直到成功）RemoteEndpoint remote = session.getRemote(); // Async Send of a BINARY message to remote endpointByteBuffer buf = ByteBuffer.wrap(new byte[] { 0x11, 0x22, 0x33, 0x44 });try{ Future&lt;Void&gt; fut = remote.sendBytesByFuture(buf); // wait for completion (forever) fut.get();}catch (ExecutionException | InterruptedException e){ // Send failed e.printStackTrace();} 怎么使用RemoteEndpoint发送一个简单的二进制信息，追踪Future以确定发送成功还是失败。 12345678910111213141516171819202122232425262728实例9 送二进制信息（异步，发送超时）RemoteEndpoint remote = session.getRemote(); // Async Send of a BINARY message to remote endpointByteBuffer buf = ByteBuffer.wrap(new byte[] { 0x11, 0x22, 0x33, 0x44 });Future&lt;Void&gt; fut = null;try{ fut = remote.sendBytesByFuture(buf); // wait for completion (timeout) fut.get(2,TimeUnit.SECONDS);}catch (ExecutionException | InterruptedException e){ // Send failed e.printStackTrace();}catch (TimeoutException e){ // timeout e.printStackTrace(); if (fut != null) { // cancel the message fut.cancel(true); }} 怎么使用RemoteEndpoint发送一个简单的二进制信息，追踪Future并等待一个有限的时间，如果时间超限则取消该信息。 123456789101112131415161718192021222324实例10 发送文本信息（异步）RemoteEndpoint remote = session.getRemote(); // Async Send of a TEXT message to remote endpointremote.sendStringByFuture(\"Hello World\"); 怎么使用RemoteEndpoint发送一个简单的文本信息。这个信息将被放到输出队列中，但是你将不知道发送成功还是失败。 实例11 发送文本信息（异步，等待直到成功）RemoteEndpoint remote = session.getRemote(); // Async Send of a TEXT message to remote endpointtry{ Future&lt;Void&gt; fut = remote.sendStringByFuture(\"Hello World\"); // wait for completion (forever) fut.get();}catch (ExecutionException | InterruptedException e){ // Send failed e.printStackTrace();} 怎么使用RemoteEndpoint发送一个简单的二进制信息，追踪Future以直到发送成功还是失败。 123456789101112131415161718192021222324252627实例12 发送文本信息（异步，发送超时）RemoteEndpoint remote = session.getRemote(); // Async Send of a TEXT message to remote endpointFuture&lt;Void&gt; fut = null;try{ fut = remote.sendStringByFuture(\"Hello World\"); // wait for completion (timeout) fut.get(2,TimeUnit.SECONDS);}catch (ExecutionException | InterruptedException e){ // Send failed e.printStackTrace();}catch (TimeoutException e){ // timeout e.printStackTrace(); if (fut != null) { // cancel the message fut.cancel(true); }} 怎么使用RemoteEndpoint发送一个简单的二进制信息，追踪Future并等待有限的时间，如果超时则取消。使用WebSocket注释WebSocket的最基本的形式是一个被Jetty WebSocket API提供的用注释标记的POJO。 123456789101112131415161718192021实例13 AnnotatedEchoSocket.javapackage examples.echo; import org.eclipse.jetty.websocket.api.Session;import org.eclipse.jetty.websocket.api.annotations.OnWebSocketMessage;import org.eclipse.jetty.websocket.api.annotations.WebSocket; /** * Example EchoSocket using Annotations. */@WebSocket(maxTextMessageSize = 64 * 1024)public class AnnotatedEchoSocket { @OnWebSocketMessage public void onText(Session session, String message) { if (session.isOpen()) { System.out.printf(\"Echoing back message [%s]%n\", message); session.getRemote().sendString(message, null); } }} 上面的例子是一个简单的WebSocket回送端点，将回送所有它收到的文本信息。这个实现使用了一个无状态的方法，因此对每个出现的事件Session都会被传递到Message处理方法中。这将允许你在同多个端口交互时可以重用AnnotatedEchoSocket的单实例。你可用的注释如下： @WebSocket 一个必须的类级别的注释。标记这个POJO作为一个WebSocket。类必须不是abstract，且是public。 @OnWebSocketConnect 一个可选的方法级别的注释。标记一个在类中的方法作为On Connect事件的接收者。方法必须是public，且不是abstract，返回void，并且有且仅有一个Session参数。 @OnWebSocketClose一个可选的方法级的注释。标记一个在类中的方法作为On Close事件的接收者。方法标签必须是public，不是abstract，并且返回void。方法的参数包括：1）Session（可选）2）int closeCode（必须）3）String closeReason（必须） @OnWebSocketMessage一个可选的方法级注释。标记在类中的2个方法作为接收On Message事件的接收者。方法标签必须是public，不是abstract，并且返回void。为文本信息的方法参数包括：1）Session（可选）2）String text（必须）为二进制信息的方法参数包括：1）Session（可选）2）byte buf[]（必须）3）int offset（必须）4）int length（必须） @OnWebSocketError一个可选的方法级注释。标记一个类中的方法作为Error事件的接收者。方法标签必须是public，不是abstract，并且返回void。方法参数包括：1）Session（可选）2）Throwable cause（必须） @OnWebSocketFrame一个可选的方法级注释。标记一个类中的方法作为Frame事件的接收者。方法标签必须是public，不是abstract，并且返回void。方法参数包括：1）Session（可选）2）Frame（必须）收到的Frame将在这个方法上被通知，然后被Jetty处理，可能导致另一个事件，例如On Close，或者On Message。对Frame的改变将不被Jetty看到。 用WebSocketListener一个WebSocket的基本形式是使用org.eclipse.jetty.websocket.api.WebSocketListener处理收到的事件。 1234567891011121314151617181920212223242526272829303132333435363738394041实例14 ListenerEchoSocket.javapackage examples.echo; import org.eclipse.jetty.websocket.api.Session;import org.eclipse.jetty.websocket.api.WebSocketListener; /** * Example EchoSocket using Listener. */public class ListenerEchoSocket implements WebSocketListener { private Session outbound; @Override public void onWebSocketBinary(byte[] payload, int offset, int len) { } @Override public void onWebSocketClose(int statusCode, String reason) { this.outbound = null; } @Override public void onWebSocketConnect(Session session) { this.outbound = session; } @Override public void onWebSocketError(Throwable cause) { cause.printStackTrace(System.err); } @Override public void onWebSocketText(String message) { if ((outbound != null) &amp;&amp; (outbound.isOpen())) { System.out.printf(\"Echoing back message [%s]%n\", message); outbound.getRemote().sendString(message, null); } }} 如果listener做了太多的工作，你能使用WebSocketAdapter代替。使用WebSocketAdapterWebSocketListener的适配器。 1234567891011121314151617181920212223实例15 AdapterEchoSocket.javapackage examples.echo; import java.io.IOException;import org.eclipse.jetty.websocket.api.WebSocketAdapter; /** * Example EchoSocket using Adapter. */public class AdapterEchoSocket extends WebSocketAdapter { @Override public void onWebSocketText(String message) { if (isConnected()) { try { System.out.printf(\"Echoing back message [%s]%n\", message); getRemote().sendString(message); } catch (IOException e) { e.printStackTrace(System.err); } } }} 这个类比WebSocketListener跟为便利，并提供了有用的方法检查Session的状态。 Jetty WebSocket Server APIJetty通过WebSocketServlet和servlet桥接的使用，提供了将WebSocket端点到Servlet路径的对应。内在地，Jetty管理HTTP升级到WebSocket，并且从一个HTTP连接移植到一个WebSocket连接。这只有当运行在Jetty容器内部时才工作。 Jetty WebSocketServlet为了通过WebSocketServlet对应你的WebSocket到一个指定的路径，你将需要扩展org.eclipse.jetty.websocket.servlet.WebSocketServlet并指定什么WebSocket对象应该被创建。 123456789101112131415161718实例16 MyEchoServlet.javapackage examples; import javax.servlet.annotation.WebServlet;import org.eclipse.jetty.websocket.servlet.WebSocketServlet;import org.eclipse.jetty.websocket.servlet.WebSocketServletFactory; @SuppressWarnings(\"serial\")@WebServlet(name = \"MyEcho WebSocket Servlet\", urlPatterns = { \"/echo\" })public class MyEchoServlet extends WebSocketServlet { @Override public void configure(WebSocketServletFactory factory) { factory.getPolicy().setIdleTimeout(10000); factory.register(MyEchoSocket.class); }} 这个例子将创建一个Sevlet，通过@WebServlet注解匹配到Servlet路径”/echo”（或者你能在你的web应用的WEB-INF/web.xml中手动的配置），当收到HTTP升级请求时将创建MyEchoSocket实例。WebSocketServlet.configure(WebSocketServletFactory factory)是为你的WebSocket指定配置的地方。在这个例子中，我们指定一个10s的空闲超时，并注册MyEchoSocket，即当收到请求时我们想创建的WebSocket类，使用默认的WebSocketCreator创建。 使用WebSocketCreator所有WebSocket都是通过你注册到WebSocketServletFactory的WebSocketCreator创建的。默认，WebSocketServletFactory是一个简单的WebSocketCreator，能创建一个单例的WebSocket对象。 使用WebSocketCreator.register(Class&lt;?&gt; websocket)告诉WebSocketServletFactory应该实例化哪个类（确保它有一个默认的构造器）。如果你有更复杂的创建场景，你可以提供你自己的WebSocketCreator，基于在UpgradeRequest对象中出现的信息创建的WebSocket。 123456789101112131415161718192021222324252627282930313233实例17 MyAdvancedEchoCreator.javapackage examples; import org.eclipse.jetty.websocket.servlet.ServletUpgradeRequest;import org.eclipse.jetty.websocket.servlet.ServletUpgradeResponse;import org.eclipse.jetty.websocket.servlet.WebSocketCreator; public class MyAdvancedEchoCreator implements WebSocketCreator { private MyBinaryEchoSocket binaryEcho; private MyEchoSocket textEcho; public MyAdvancedEchoCreator() { this.binaryEcho = new MyBinaryEchoSocket(); this.textEcho = new MyEchoSocket(); } @Override public Object createWebSocket(ServletUpgradeRequest req, ServletUpgradeResponse resp) { for (String subprotocol : req.getSubProtocols()) { if (\"binary\".equals(subprotocol)) { resp.setAcceptedSubProtocol(subprotocol); return binaryEcho; } if (\"text\".equals(subprotocol)) { resp.setAcceptedSubProtocol(subprotocol); return textEcho; } } return null; }} 这儿我们展示了一个WebSocketCreator，将利用来自请求的WebSocket子协议信息决定什么类型的WebSocket应该被创建。 1234567891011121314151617实例18 MyAdvancedEchoServlet.javapackage examples; import javax.servlet.annotation.WebServlet;import org.eclipse.jetty.websocket.servlet.WebSocketServlet;import org.eclipse.jetty.websocket.servlet.WebSocketServletFactory; @SuppressWarnings(\"serial\")@WebServlet(name = \"MyAdvanced Echo WebSocket Servlet\", urlPatterns = { \"/advecho\" })public class MyAdvancedEchoServlet extends WebSocketServlet { @Override public void configure(WebSocketServletFactory factory) { factory.getPolicy().setIdleTimeout(10000); factory.setCreator(new MyAdvancedEchoCreator()); }} 当你想要一个定制的WebSocketCreator时，使用WebSocketServletFactory.setCreator(WebSocketCreator creator)，然后WebSocketServletFactory将为所有在这个servlet上收到的Upgrade请求用你的创造器。一个WebSocketCreator还可以用于：1）控制WebSocket子协议的选择；2）履行任何你认为重要的WebSocket源；3）从输入的请求获取HTTP头；4）获取Servlet HttpSession对象（如果它存在）；5）指定一个响应状态码和原因；如果你不想接收这个请求，简单的从WebSocketCreator.createWebSocket(UpgradeRequest req, UpgradeResponse resp)返回null。 Jetty WebSocket Client APIJetty也提供了一个Jetty WebSocket Client库，为了更容易的与WebSocket服务端交互。为了在你自己的Java项目上使用Jetty WebSocket Client，你将需要下面的maven配置： 12345&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty.websocket&lt;/groupId&gt; &lt;artifactId&gt;websocket-client&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt;&lt;/dependency&gt; WebSocketClient为了使用WebSocketClient，你将需要连接一个WebSocket对象实例到一个指定的目标WebSocket URI。 123456789101112131415161718192021222324252627282930313233343536373839实例19 SimpleEchoClient.javapackage examples; import java.net.URI;import java.util.concurrent.TimeUnit;import org.eclipse.jetty.websocket.client.ClientUpgradeRequest;import org.eclipse.jetty.websocket.client.WebSocketClient; /** * Example of a simple Echo Client. */public class SimpleEchoClient { public static void main(String[] args) { String destUri = \"ws://echo.websocket.org\"; if (args.length &gt; 0) { destUri = args[0]; } WebSocketClient client = new WebSocketClient(); SimpleEchoSocket socket = new SimpleEchoSocket(); try { client.start(); URI echoUri = new URI(destUri); ClientUpgradeRequest request = new ClientUpgradeRequest(); client.connect(socket, echoUri, request); System.out.printf(\"Connecting to : %s%n\", echoUri); socket.awaitClose(5, TimeUnit.SECONDS); } catch (Throwable t) { t.printStackTrace(); } finally { try { client.stop(); } catch (Exception e) { e.printStackTrace(); } } }} 上面的例子连接到一个远端WebSocket服务端，并且连接后使用一个SimpleEchoSocket履行在websocket上的处理逻辑，等待socket关闭。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061实例20 SimpleEchoSocket.javapackage examples; import java.util.concurrent.CountDownLatch;import java.util.concurrent.Future;import java.util.concurrent.TimeUnit;import org.eclipse.jetty.websocket.api.Session;import org.eclipse.jetty.websocket.api.StatusCode;import org.eclipse.jetty.websocket.api.annotations.OnWebSocketClose;import org.eclipse.jetty.websocket.api.annotations.OnWebSocketConnect;import org.eclipse.jetty.websocket.api.annotations.OnWebSocketMessage;import org.eclipse.jetty.websocket.api.annotations.WebSocket; /** * Basic Echo Client Socket */@WebSocket(maxTextMessageSize = 64 * 1024)public class SimpleEchoSocket { private final CountDownLatch closeLatch; @SuppressWarnings(\"unused\") private Session session; public SimpleEchoSocket() { this.closeLatch = new CountDownLatch(1); } public boolean awaitClose(int duration, TimeUnit unit) throws InterruptedException { return this.closeLatch.await(duration, unit); } @OnWebSocketClose public void onClose(int statusCode, String reason) { System.out.printf(\"Connection closed: %d - %s%n\", statusCode, reason); this.session = null; this.closeLatch.countDown(); } @OnWebSocketConnect public void onConnect(Session session) { System.out.printf(\"Got connect: %s%n\", session); this.session = session; try { Future&lt;Void&gt; fut; fut = session.getRemote().sendStringByFuture(\"Hello\"); fut.get(2, TimeUnit.SECONDS); fut = session.getRemote().sendStringByFuture(\"Thanks for the conversation.\"); fut.get(2, TimeUnit.SECONDS); session.close(StatusCode.NORMAL, \"I'm done\"); } catch (Throwable t) { t.printStackTrace(); } } @OnWebSocketMessage public void onMessage(String msg) { System.out.printf(\"Got msg: %s%n\", msg); }} 当SimpleEchoSocket连接成功后，它发送2个文本信息，然后关闭socket。onMessage(String msg)收到来自远端WebSocket的响应，并输出他们到控制台。","link":"/2019/08/08/jetty-servlet-开发/"},{"title":"自定义js弹框","text":"123456789101112131415161718192021222324252627//显示框信息function showMsg(val,time){ if(!document.getElementById('parent_pop_up')){ var parent_pop_up = document.createElement('div'); parent_pop_up.id = \"parent_pop_up\"; parent_pop_up.style.cssText = \"position: fixed; z-index: 9999; bottom: 5rem; width: 100%;\"; var poo_up = document.createElement('div'); poo_up.id = 'poo_up'; poo_up.style.cssText = 'height: 1rem; margin:0 auto; text-align: center;'; var span = document.createElement('span'); span.style.cssText = 'background-color: rgba(0,0,0,0.6); padding: 0.2rem 0.35rem; letter-spacing: 1px; border-radius: 5px; color: #FFFFFF; font-size: 0.34rem; text-align: center;'; span.innerHTML = val; poo_up.appendChild(span); parent_pop_up.appendChild(poo_up); document.body.appendChild(parent_pop_up); if(time == null || time == ''){ time = 2000; } setTimeout(function(){hideMsg();},time); }};//隐藏显示框function hideMsg(){ var pop = document.getElementById('parent_pop_up'); pop.style.display = 'none'; document.body.removeChild(pop);};","link":"/2019/11/06/js弹框自动消失/"},{"title":"js读取文件内容","text":"流程只用两步： 用file类型的input载入文件；用HTML5的FileReader方法读取文件内容。如下是一个js读取文件内容到文本框的方法。 1 演示点击查看演示地址。 2 HTML代码页面有两个元素，file类型的input，和显示内容的textarea文本框。 12&lt;input type=\"file\" name=\"upload\" id=\"upload\" accept=\"text/plain\"/&gt;&lt;textarea name=\"content\" id=\"content\"&gt;&lt;/textarea&gt; 3 JS代码Javasctipt代码有两段，读取文件内容的getFileContent()函数，和响应文件上传的事件： 123456789101112131415161718192021222324252627282930313233343536&lt;script type=\"text/javascript\"&gt; window.onload = function() { /** * 上传函数 * @param fileInput DOM对象 * @param callback 回调函数 */ var getFileContent = function (fileInput, callback) { if (fileInput.files &amp;&amp; fileInput.files.length &gt; 0 &amp;&amp; fileInput.files[0].size &gt; 0) { //下面这一句相当于JQuery的：var file =$(\"#upload\").prop('files')[0]; var file = fileInput.files[0]; if (window.FileReader) { var reader = new FileReader(); reader.onloadend = function (evt) { if (evt.target.readyState == FileReader.DONE) { callback(evt.target.result); } }; // 包含中文内容用gbk编码 reader.readAsText(file, 'gbk'); } } }; /** * upload内容变化时载入内容 */ document.getElementById('upload').onchange = function () { var content = document.getElementById('content'); getFileContent(this, function (str) { content.value = str; }); }; };&lt;/script&gt; 3 代码说明FileReader() 对象提供了一些方法，可以将本地文件读取到内存中。 方法名 参数 描述 abort none 中断读取 readAsBinaryString file 将文件读取为二进制码 readAsDataURL file 将文件读取为 DataURL readAsText file, [encoding] 将文件读取为文本，编码方式默认为“UTF-8”，支持可改用“GBK” 后面三个方法第一个参数传入File对象或者Blob对象。 readAsText：该方法有两个参数，其中第二个参数是文本的编码方式，默认值为 UTF-8，要支持中文要改为GBK。将文件以文本方式读取，读取的结果即是这个文本文件中的内容。 readAsBinaryString：该方法将文件读取为二进制字符串，通常我们将它传送到后端，后端可以通过这段字符串存储文件。 readAsDataURL：该方法将文件读取为一段以 data: 开头的字符串，这段字符串的实质就是 Data URL，这是一种将小文件直接嵌入文档的方案。这里的小文件通常是指图像与 html 等格式的文件。 如果要下载页面内容到本地文件，请看这一篇：Javascript实现文件形式下载页面内容。","link":"/2019/11/08/js读取文件内容/"},{"title":"karaf cmd develop","text":"Karaf命令行辅助开发 在命令行中，要实现如上常用的help,通过ArgumentParsers，则可以容易实现,参考代码 package com.zte.sunquan.demo.param; import java.io.File; import java.util.ArrayList; import net.sourceforge.argparse4j.ArgumentParsers; import net.sourceforge.argparse4j.annotation.Arg; import net.sourceforge.argparse4j.inf.ArgumentParser; import net.sourceforge.argparse4j.inf.ArgumentParserException; /** * Created by sunquan on 2018/1/10. */ public class ParamTest { public static class Params { @Arg(dest = &quot;type&quot;) public String type; @Arg(dest = &quot;num1&quot;) public String num; @Arg(dest = &quot;auth&quot;) public ArrayList&lt;String&gt; auth; @Arg(dest = &quot;schemas-dir&quot;) public File schemasDir; public String getType() { return type; } public String getNum() { return num; } public ArrayList&lt;String&gt; getAuth() { return auth; } public File getSchemasDir() { return schemasDir; } } public static void main(String[] args) throws ArgumentParserException { //指定命令gcs final ArgumentParser parser = ArgumentParsers.newArgumentParser(&quot;gcs&quot;) .defaultHelp(true) .description(&quot;alculate checksum of given String.&quot;);//描述命令功能 parser.addArgument(&quot;-t&quot;, &quot;--type&quot;)//参数选项,第二个参数默认对应@Arg中dest .type(String.class)//默认值 .choices(&quot;SHA-256&quot;, &quot;SHA-512&quot;, &quot;SHA1&quot;)//可选项 .setDefault(&quot;SHA-256&quot;)//默认值 .help(&quot;Specify hash function to use&quot;);//选项描述 parser.addArgument(&quot;-c&quot;, &quot;--content&quot;) .type(String.class) .help(&quot;content which need calculate&quot;) .dest(&quot;num1&quot;);//指定对应的@Arg 定义属性 parser.addArgument(&quot;-a&quot;, &quot;--auth&quot;) .nargs(2) .help(&quot;Username and password for HTTP basic authentication in order username password.&quot;) .dest(&quot;auth&quot;); parser.addArgument(&quot;-f&quot;, &quot;--schemas-dir&quot;) .type(File.class) .help(&quot;Directory containing yang schemas to describe simulated devices. Some schemas e.g. netconf monitoring and inet types are included by default&quot;) .dest(&quot;schemas-dir&quot;); parser.printHelp();//打印help信息 Params params = new Params();//命令包装对象 try { //命令行参数--&gt;自动转包装对象 parser.parseArgs(new String[]{&quot;-t&quot;, &quot;SHA-256&quot;, &quot;--content&quot;, &quot;sunquan&quot;, &quot;-a&quot;, &quot;sunquan&quot;, &quot;password&quot;, &quot;-f&quot;, &quot;C:/Users/Administrator/.m2/settings.xml&quot;}, params);//字符串转命令包装对象 } catch (final ArgumentParserException e) { parser.handleError(e); } System.out.println(&quot;type:\\t&quot; + params.getType()); System.out.println(&quot;num:\\t&quot; + params.getNum()); System.out.println(&quot;userName:\\t&quot; + params.getAuth().get(0)); System.out.println(&quot;password:\\t&quot; + params.getAuth().get(1)); if (params.getSchemasDir().exists()) System.out.println(&quot;file:\\t&quot; + params.getSchemasDir().getName()); } }再介绍org.apache.karaf.shell.table.ShellTable public static void main(String[] args) { ShellTable table = new ShellTable(); table.size(40);//设置显示列宽（从最后一列开始计数），不设置 table.column(&quot;name&quot;).alignLeft();//设置1列标题靠左 table.column(&quot;age&quot;).alignCenter();//设置1列标题居中 Col gender = new Col(&quot;gender&quot;); gender.alignRight(); gender.maxSize(10);//设置该列长度，多余字符会被截断，不设置 table.column(gender); // table.column(&quot;gender&quot;).alignRight();//设置1列标题靠右 table.separator(&quot; % &quot;);//设置分隔符 table.addRow().addContent(&quot;sunquan111111111111111111&quot;, &quot;291111111111&quot;, &quot;boy456789&quot;); table.addRow().addContent(&quot;sunquan&quot;, &quot;29&quot;, &quot;boy&quot;); table.addRow().addContent(&quot;sunquan&quot;, &quot;29&quot;, &quot;boy&quot;); // table.noHeaders();//不显示头，不设置 table.print(System.out); table = new ShellTable(); table.column(&quot;name&quot;).alignLeft();//设置1列标题靠左 table.emptyTableText(&quot;null&quot;);//表示一个空表，用null替代 table.print(System.out, true); }","link":"/2019/07/25/karaf-cmd-develop/"},{"title":"karaf命令行开发","text":"ONOS编程系列(二)命令行命令与服务开发 此文章承接ONOS编程系列（一） Application Tutorial ，如果尚未看过上一篇，请先看完上一篇，再回过头来看此篇。本文章的目的在于让读者明白： 如何将新建的application扩展为新的服务，以便其他服务或者应用可以调用它 如何将该application的功能扩展为Karaf命令行界面下的一个新命令我们假设你已经安装并且能初步掌握Mininet，因为后面的测试工作需要用到它。项目架构 一、扩展为服务1.1 定义服务接口首先，在onos-api包下定义一个新的服务的接口，该包目录是${ONOS_ROOT}/core/api/src/main/java/org/onosproject/net/。在此目录下，创建一个新文件夹apps/，作为新接口的位置。接口文件放在此处的意义在于只有这样cli的包才能访问到它，而cli包正是实现命令行命令的包。 1.2 导入服务接口接下来，我们会在IntentReactiveForwarding文件中实现该服务接口。 在本教程中我们用不到该应用生成的服务，不过如果要调用该服务，只需要这样既可： 1.3 实现该服务接口 要实现该服务接口，我们向类IntentReactiveForwarding增加了新的Map，endPoints成员变量。Map成员在process()方法中用来存储HostService提供的终端信息。 现在，一个引用了ForwardingMapService的模块就可以通过调用getEndPoints()方法获取能够双向通信的终端列表了，这些终端上都安装了本模块的intents。 接下来，创建一个新的Karaf CLI命令来使用这个新的服务。该命令的动能是列出map的内容，并且可选地提供一个过滤参数，来过滤主机源的地址。 二、创建karaf的一个新命令Karaf CLI命令定义在项目目录${ONOS_ROOT}/cli/之下。有两种类型的命令，分别在不同的目录下： ${ONOS_ROOT}/cli/src/main/java/org/onosproject/cli 系统配置与监视相关的命令所在目录 ${ONOS_ROOT}/cli/src/main/java/org/onosproject/cli/net 网络配置与监视相关的命令所在目录 我们要建立的命令要显示网络相关的信息，所以我们要在第二个目录下增加我们的command类。 2.1 新建一个command类 在第二个目录下，创建一个名为ForwardingMapCommand的类文件。该类是AbstractShellCommand的子类，在类中要使用命令相关的一些注解： @Command 该注解用来设置命令的名字，作用范围以及功能描述 @Argument 该注解用来指定命令的参数 2.3 注册这个command，使其能在karaf CLI下使用接下来，我们需要编辑shell-config.xml文件，该文件位于${ONOS_ROOT}/cli/src/main/resources/OSGI-INF/blueprint/，其作用是告诉karaf有新的命令加入了。编辑格式是在里面填上我们的命令的相关信息： 三、验证3.1 重编译，重启动ONOS 一切修改完毕之后，进入onos根目录重新编译，编译成功之后运行onos： 进入欢迎界面之后，可以键入“fwdmap –help”查看我们新建命令的描述： 3.2 启动Mininet，构建测试网络 新开一个命令行界面，在此界面下开启mininet（默认是本地启动，所以ip是127.0.0.1）：sudomn –topo=tree,2,2 –controller=remote,ip=127.0.0.1 –mac此时，在ONOS界面下可以看到四个终端，三个交换机：3.3 测试命令行键入fwdmap，可以看到没有什么结果返回，因为这个时候网络中的主机之间还没有进行通信呢。 切换到mininet控制界面，键入pingall，执行一次主机间的通信。 然后再切换回onos命令行，再次键入fwdmap命令： 问题一 在onos命令行下键入fwdmap，如果出现以下结果：就要回头看看IntentReactiveForwarding类前面有没有加@Service注解了。 如果注解没问题，那么就有可能是当前并没有安装ifwd模块。用命令“feature:list -i | grepifwd”查看当前是否安装了ifwd模块，如果返回结果为空，则要手动安装一下：feature:installonos-app-ifwd 。问题二 在用mininet命令行下pingall以后，再次在onos下键入fwdma，如果还没有任何输出，可能就是源代码哪里又出了问题。 我下载到源代码以后，切换版本到了onos-1.1，该版本下，本来就是有一个ifwd的项目的，不过比教程里的东西要少一些。在手动敲入代码的时候，我大致略过了已有的代码，但是注意，教程里的代码并非完全在原有代码基础上进行的增加，在IntentReactiveForwarding文件中，函数setUpConnectivity中，红框框住的部分是原有的部分，蓝框框住的部分是教程中的部分，可以看到，两者还是有一点区别的： 解决这个问题以后，再次走一遍流程，应该就能看到结果了吧。","link":"/2019/07/27/karaf命令行开发/"},{"title":"keepalived","text":"高性能集群软件Keepalived的介绍以及安装与配置 Keepalived介绍： Keepalived是Linux下一个轻量级的高可用解决方案；起初是为LVS设计的，专门用来监控集群系统中各个服务节点的状态。它根据TCP/IP参考模型的第三、第四和第五层交换机机制检测每个服务节点的状态，如果某个服务节点出现异常，或工作出现故障，Keepalived将检测到，并将出现故障的服务节点从集群系统中剔除，而在故障节点恢复正常后，Keepalived又可以自动将此服务节点重新加入服务器集群中，这些工作全部自动完成，不需要人工干涉，需要人工完成的只是修复出现故障的服务节点。 Keepalived后来又加入了VRRP的功能，VRRP（Virtual Router Redundancy Protocol,虚拟路由器冗余协议）出现的目的是解决静态路由出现的单点故障问题，通过VRRP可以实现网络不间断稳定运行。因此，Keepalived一方面具有服务器状态检测和故障隔离功能，另一方面也具有HA cluster 功能。 VRRP协议与工作原理VRRP，它是一种主备模式的协议，通过VRRP可以在网络发生故障时透明地进行设备切换而不影响主机间的数据通信；这其中涉及两个概念：物理路由器和虚拟路由器。 VRRP可以将两台或多台物理路由器设备虚拟成一个虚拟路由器，这个虚拟路由器通过虚拟IP（一个或多个)对外提供服务，而在虚拟路由器内部是多个物理路由器协同工作，同一时间只有一台物理路由器对外提供服务，这台物理路由器被称为主路由器（处于MASTER角色）。一般情况MASTER由选举算法产生，它拥有对外服务的虚拟IP，提供各种网络功能，如ARP请求、ICMP、数据转发等。而其他物理路由器不拥有对外的虚拟Ip，也不提供对外网络功能，仅仅接收MASTER的VRRP状态通告信息，这些路由器被统称为备份路由器（处于BACKUP角色）。当主路由器失效时，处于BACKUP角色的备份路由器将重新进行选举，产生一个新的主路由器进入MASTER角色继续提供对外服务。 每个虚拟路由器独有一个唯一标识，称为VRID，一个VRID与一组IP地址构成了一个虚拟路由器。在VRRP协议中，所有的报文都是通过IP多播形式发送的，而在一个虚拟路由器中，只有处于MASTER角色的路由器会一直发生VRRP数据包，处于BACKUP角色的路由器只接收MASTER发送过来的报文信息，用来监控MASTER运行状态，因此，不会发生BACKUP抢占的现象，除非它的优先级更高。而当MASTER出现故障，多台BACKUP就会进行选举，优先级最高的BACKUP成为新的MASTER，这种选举并进行角色切换的过程非常快，因而保证了服务的持续可用性。 Keepalived工作原理keepalive运行机制如下： 在网络层，运行着4个重要的协议：互联网协议IP、互联网控制报文协议ICMP、地址转换协议ARP以及反向地址转换协议RARP。Keepalived在网络层采用的最常见的工作方式是通过ICMP协议向服务器集群中的那个节点发送一个ICMP数据包（类似于ping实现的功能），如果某个节点没有返回响应数据包，那么认为此节点发生了故障，Keepalived将报告次节点失效，并从服务器集群中剔除故障节点。 在传输层，提供了两个主要的协议：传输控制协议TCP和用户数据协议UDP。传输控制协议TCP可以提供可靠的数据传输服务、Ip地址和端口代表TCP的一个连接端。要获得TCP服务，需要在发送机的一个端口上和接收机的一个端口上建立连接，而Keepalived在传输层就是利用TCP协议的端口连接和扫描技术来判断集群点是否正常的。比如，对于常见的WEB服务默认的80端口、SSH服务默认的22端口等，Keepalived一旦在传输层探测到这些端口没有响应数据返回，就认为这些端口发生异常，然后强制将此端口对应得节点从服务器集群组中移除。 在应用层，可运行FTP、TELNET、SMTIP、DNS等各种不同类型的高层协议，Keepalived的运行方式也更加全面化和复杂化，用户可以通过自定义Keepalived的工作方式；例如：用户可以通过编写程序来运行keepalived。而keepalived将根据用户的设定检测各种程序或服务是否运行正常，如果Keepalived的检测结果与用户设定不一致时，Keepalived将把对应的服务从服务器中移除。 Keepalived的组件：核心组件： VRRP Stack:实现HA集群中失败切换（Failover)功能。Keepalived通过VRRP功能能再结合LVS负载均衡软件即可部署一个高性能的负载均衡集群系统。ipvs wrapper:可以将设置好的IP VS规则发送到内核空间并提交给IP VS模块，最终实现 IP VS模块的负载均衡功能。 checkers:这是Keepalived 最基础的功能，也是最主要的功能，可实现对服务器运行状态检测和故障隔离。HA Cluster的配置前提：(1) 各节点时间必须同步； ntp, chrony(2) 确保iptables及selinux不会成为阻碍；(3) 各节点之间可通过主机名互相通信（对KA并非必须）；建议使用/etc/hosts文件实现；(4) 确保各节点的用于集群服务的接口支持MULTICAST通信； D类：224-239；Keepalived安装与配置：Centos6.4以后版本可以直接yum安装： yum install keepalived 程序环境： 主配置文件：/etc/keepalived/keepalived.conf 主程序文件：/usr/sbin/keepalived Unit File:keepalived.service Unit File的环境配置文件：/etc/sysconfig/keepalived根据配置文件所实现的功能，将Keepalived配置分三类：Global ConfigurationVRRPD 配置、LVS配置 全局配置以”global_defs”作为标识，在“global_defs”区域内的都是全局配置选项： 123456789global_defs {notification_email { acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc} notification_email_from Alexandre.Cassen@firewall.locsmtp_server 192.168.200.1 smtp_connect_timeout 30router_id LVS_DEVELvrrp_mcast_group4 224.110.129.18} 主配置文件详解 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394global_defs { notification_email { #发送报警邮件收件地址 acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc #指明报警邮件的发送地址 smtp_server 192.168.200.1 #邮件服务器地址 smtp_connect_timeout 30 #smtp的超时时间 router_id LVS_DEVEL #物理服务器的主机名 vrrp_mcast_group4 #定义一个组播地址 static_ipaddress { 192.168.1.1/24 dev eth0 scope global } static_routes { 192.168.2.0/24 via 192.168.1.100 dev eth0 }} vrrp_sync_group VG_1 { #定义一个故障组，组内有一个虚拟路由出现故障另一个也会一起跟着转移，适用于LVS的NAT模型。 group { VI1 # name of vrrp_instance (below) VI2 # One for each moveable IP } }vrrp_instance VI_1 { #定义一个虚拟路由 state MASTER|BACKUP #当前节点在此虚拟路由器上的初始状态；只能有一个是MASTER，余下的都应该为BACKUP； interface eth0 #绑定为当前虚拟路由器使用的物理接口； virtual_router_id 51 #当前虚拟路由器的惟一标识，范围是0-255； priority 100 #当前主机在此虚拟路径器中的优先级；范围1-254； advert_int 1 #通告发送间隔，包含主机优先级、心跳等。 authentication { #认证配置 auth_type PASS #认证类型，PASS表示简单字符串认证 auth_pass 1111 #密码,PASS密码最长为8位 virtual_ipaddress { 192.168.200.16 #虚拟路由IP地址，以辅助地址方式设置 192.168.200.18/24 dev eth2 label eth2:1 #以别名的方式设置 }track_interface { eth0 eth1} #配置要监控的网络接口，一旦接口出现故障，则转为FAULT状态；nopreempt #定义工作模式为非抢占模式；preempt_delay 300 #抢占式模式下，节点上线后触发新选举操作的延迟时长； virtual_routes { #配置路由信息，可选项 # src &lt;IPADDR&gt; [to] &lt;IPADDR&gt;/&lt;MASK&gt; via|gw &lt;IPADDR&gt; [or &lt;IPADDR&gt;] dev &lt;STRING&gt; scope &lt;SCOPE&gt; tab src 192.168.100.1 to 192.168.109.0/24 via 192.168.200.254 dev eth1 192.168.112.0/24 via 192.168.100.254 192.168.113.0/24 via 192.168.200.254 or 192.168.100.254 dev eth1 blackhole 192.168.114.0/24 } notify_master &lt;STRING&gt;|&lt;QUOTED-STRING&gt; #当前节点成为主节点时触发的脚本。 notify_backup &lt;STRING&gt;|&lt;QUOTED-STRING&gt; #当前节点转为备节点时触发的脚本。 notify_fault &lt;STRING&gt;|&lt;QUOTED-STRING&gt; #当前节点转为“失败”状态时触发的脚本。 notify &lt;STRING&gt;|&lt;QUOTED-STRING&gt; #通用格式的通知触发机制，一个脚本可完成以上三种状态的转换时的通知。 smtp_alert #如果加入这个选项，将调用前面设置的邮件设置，并自动根据状态发送信息 }virtual_server 192.168.200.100 443 { #LVS配置段 ，设置LVS的VIP地址和端口 delay_loop #服务轮询的时间间隔；检测RS服务器的状态。 lb_algo rr #调度算法，可选rr|wrr|lc|wlc|lblc|sh|dh。 lb_kind NAT #集群类型。 nat_mask 255.255.255.0 #子网掩码，可选项。 persistence_timeout 50 #是否启用持久连接，连接保存时长 protocol TCP #协议，只支持TCP sorry_server &lt;IPADDR&gt; &lt;PORT&gt; #备用服务器地址，可选项。 real_server 192.168.201.100 443 { #配置RS服务器的地址和端口 weight 1 #权重 SSL_GET { #检测RS服务器的状态，发送请求报文 url { path / #请求的URL digest ff20ad2481f97b1754ef3e12ecd3a9cc #对请求的页面进行hash运算，然后和这个hash码进行比对，如果hash码一样就表示状态正常 status_code &lt;INT&gt; #判断上述检测机制为健康状态的响应码,和digest二选一即可。 } #这个hash码可以使用genhash命令请求这个页面生成 connect_timeout 3 #连接超时时间 nb_get_retry 3 #超时重试次数 delay_before_retry 3 #每次超时过后多久再进行连接 connect_ip &lt;IP ADDRESS&gt; #向当前RS的哪个IP地址发起健康状态检测请求 connect_port &lt;PORT&gt; #向当前RS的哪个PORT发起健康状态检测请求 bindto &lt;IP ADDRESS&gt; #发出健康状态检测请求时使用的源地址； bind_port &lt;PORT&gt; #发出健康状态检测请求时使用的源端口； } }} 健康状态检测机制 HTTP_GETSSL_GETTCP_CHECKSMTP_CHECKMISS_CHECK #调用自定义脚本进行检测 1234567TCP_CHECK { connect_ip &lt;IP ADDRESS&gt; #向当前RS的哪个IP地址发起健康状态检测请求; connect_port &lt;PORT&gt; #向当前RS的哪个PORT发起健康状态检测请求; bindto &lt;IP ADDRESS&gt; #发出健康状态检测请求时使用的源地址； bind_port &lt;PORT&gt; #发出健康状态检测请求时使用的源端口； connect_timeout &lt;INTEGER&gt; #连接请求的超时时长；} 注：在配置服务前需要注意几台主机的防火墙策略，和SELinux配置。ip a l eth0 注意：主设备必需先于从设备启动。Keepalived的执行配置的脚本文件时，需要一个默认的用户keepalived_script，使用如下命令进行添加： useradd keepalived_script 防火墙配置防火墙配置文件是/etc/sysconfig/iptables需要在防火墙配置文件中加入以下两条规则： 123-A DEFAULT-INPUT -p vrrp -j ACCEPT #允许vrrp组播协议访问；-A DEFAULT-INPUT -p tcp -m multiport --dports 3306,6379 -j ACCEPT允许对方ip访问本机的mysql端口和redis端口 #ip需要配置成对方ip 主： 1234567891011121314151617181920global_defs { router_id casb_device enable_script_security}vrrp_instance VI_REDIS { state MASTER interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.100.237 } notify_master \"/usr/local/enlink/casb_master.sh\" notify_backup \"/usr/local/enlink/casb_backup.sh\"} 备： 123456789101112131415161718192021global_defs { router_id casb_device enable_script_security}vrrp_instance VI_REDIS { state BACKUP #定义主备 interface eth0 virtual_router_id 51 priority 100 advert_int 1 nopreempt #只有在备时配置 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.100.237 #被争夺的虚拟ip } notify_master \"/usr/local/enlink/casb_master.sh\" notify_backup \"/usr/local/enlink/casb_backup.sh\"}","link":"/2019/09/11/keepalived/"},{"title":"karaf 部署应用 ","text":"ONOS编程系列(一)之简单应用开发一个ONOS application是使用mevan做管理的OSGi bundle。 因此，ONOS application 可以归结为Java类和pom文件的集合。本教程以基于intent的交互式转发application为例，讨论了如何从零开始建立一个新应用。本教程假设读者已经具备ONOS的运行经验，能够熟练启动ONOS实例。有关ONOS的启动不做过多描述。如果出现启动上的问题，请移步官方wiki文档自行寻找答案。本文章结束后，你应该学会： 应用的组织与结构； 如何在多个服务中注册你的应用； 北向API的基本应用； 如何运行一个应用。一、在idea中导入工程 1.设置项目的目录结构应用的根目录设置在apps/之下： 2.添加并编辑pom文件在启动karaf/ONOS命令行界面以后，直接用feature:install加上名字，即可安装此应用。然后，编辑apps/pom.xml，在文件中以形式包含该项目： 2.3 在karaf中注册该应用Karaf在运行时若要部署该应用module，需要名为feature.xml的描述性文件，编辑 ${ONOS_ROOT}/features/features.xml： 该应用的核心是名为 IntentReactiveForwarding.java的文件，被定义在${ONOS_ROOT}/apps/ src/main/java/org/onosproject/ifwd/ 里。为了便于对文档进行注释，在main/java/下需要添加一个package-info.java文件，其包含一下内容： 3.1 注册Karaf，使其自动加载 karaf的模块加载机制需要几个annotations，即注解，去注册。可用的注解尤其是以下四个尤为重要： @Component(immediate = true) - declares a class as a component to activate, and forces immediate activation; @Activate - marks a method as the method to call during the component startup routine; @Deactivate - marks a method as the method to call during component shutdown; @Reference(cardinality = ReferenceCardinality.MANDATORY_UNARY) - Marks a service as an application&apos;s dependency,and requires one instance of such a service to be loaded before this application&apos;s activation. 3.2 注册服务 接下来，我们的应用必须使用CoreService注册一个独一无二的application ID，这样才能够使该应用正常使用ONOS的其他服务。我们的应用接下来还要使用PacketService监听PacketIn和PacketOut事件。而PacketService需要一个事件处理器的类，该类用途单一，通常写在其所属类的内部，成为其私有内部类： 3.3增加包处理代码 在上一步的私有内部类ReactivePacketProcessor里，要覆写扩展自接口PacketProcessor的方法process()。每当有网络包进来的时候，PacketService都会调用一下process()函数。这意味着我们可以在这个方法里定义我们自己的包转发行为： 接下来我们要实现上图中用到的三个方法，注意，这些方法定义在私有内部类的外面，是IntentReactiveForwarding的成员： 3.4 编译该应用 3.5 启动该应用 启动分为动态启动与静态启动：动态启动就是用karaf clean命令启动onos之后，在onos命令行下键入feature:installonos-app-ifwd命令安装该应用。静态启动就是修改karaf的启动配置文件。该文件路径为${KARAF_ROOT}/etc/org.apache.karaf.features.cfg，直接将onos-app-ifwd字样缀到featuresBoot变量的尾部即可。启动并加载该应用以后，可已键入： feature:list -i | grepifwd 该应用安装以后，并不能对其做什么操作，也无法看到它的运行情况，这时我们就需要将该应用扩展为一种服务，以便于其它服务或者应用与其交互，并且需要新建一条karaf的命令，用于展示当前应用的相关信息与状态。","link":"/2019/07/26/karaf-部署应用/"},{"title":"limit 用法","text":"limit是限定查询结果的数量(是mysql数据库中特有的语句)(从表中记录0开始，查询数量为5）select empno ,ename from emp limit 5;+——-+——–+| empno | ename |+——-+——–+| 7369 | SMITH || 7499 | ALLEN || 7521 | WARD || 7566 | JONES || 7654 | MARTIM |+——-+——–+5 rows in set (0.00 sec) 2：1中sql语句还可以这样写（m,n)默认从0开始 select empno ,ename from emp limit 0,5;+——-+——–+| empno | ename |+——-+——–+| 7369 | SMITH || 7499 | ALLEN || 7521 | WARD || 7566 | JONES || 7654 | MARTIM |+——-+——–+5 rows in set (0.00 sec) 3：找出公司中工资前5名的员工（limit是在order by之后执行的) select * from emp order by sal desc limit 5;+——-+——-+———–+——+————+———+——+——–+| EMPNO | ENAME | JOB | MGR | HIREDATE | SAL | COMM | DEPTNO |+——-+——-+———–+——+————+———+——+——–+| 7839 | KING | PRESIDENT | NULL | 2017-05-13 | 5000.00 | NULL | 10 || 7902 | FORD | ANALYST | 7566 | 2017-05-13 | 3000.00 | NULL | 20 || 7788 | SCOLL | ANALIST | 7566 | 2017-05-13 | 3000.00 | NULL | 20 || 7566 | JONES | MANAGER | 7839 | 2017-05-13 | 2975.00 | NULL | 20 || 7698 | BLAKE | MANAGER | 7839 | 2017-05-13 | 2850.00 | NULL | 30 |+——-+——-+———–+——+————+———+——+——–+5 rows in set (0.07 sec) 4:找出工资排名在3-9名的员工select * from emp order by sal desc limit 2,7;+——-+———+———–+——+————+———+——–+——–+| EMPNO | ENAME | JOB | MGR | HIREDATE | SAL | COMM | DEPTNO |+——-+———+———–+——+————+———+——–+——–+| 7788 | SCOLL | ANALIST | 7566 | 2017-05-13 | 3000.00 | NULL | 20 || 7566 | JONES | MANAGER | 7839 | 2017-05-13 | 2975.00 | NULL | 20 || 7698 | BLAKE | MANAGER | 7839 | 2017-05-13 | 2850.00 | NULL | 30 || 7782 | CLARK | MANAGERAN | 7839 | 2017-05-13 | 2450.00 | NULL | 10 || 7499 | ALLEN | SALESMAN | 7698 | 2017-05-13 | 1600.00 | 300.00 | 30 || 7844 | IUSRNER | SALESMAN | 7698 | 2017-05-13 | 1500.00 | NULL | 30 || 7934 | MILLER | CLERY | 7782 | 2017-05-13 | 1300.00 | NULL | 10 |+——-+———+———–+——+————+———+——–+——–+7 rows in set (0.00 sec) 5：mysql中通用的分页语句page(每页显示3条记录) 第一页：起始下标，0,3 第二页: 起始下标，3,3 第三页:起始下标，6,3 第四页:起始下标，9,3 每页显示pageSize条记录 第pageNo页:(pageNo-1)*pageSize,pageSize SQL优化之limit 1在某些情况下,如果明知道查询结果只有一个,SQL语句中使用LIMIT 1会提高查询效率。 例如下面的用户表(主键id,邮箱,密码):1 create table t_user( 2 id int primary key auto_increment, 3 email varchar(255), 4 password varchar(255) 5 ); 每个用户的email是唯一的,如果用户使用email作为用户名登陆的话,就需要查询出email对应的一条记录。1 SELECT * FROM t_user WHERE email=?; 上面的语句实现了查询email对应的一条用户信息,但是由于email这一列没有加索引,会导致全表扫描,效率会很低。1 SELECT * FROM t_user WHERE email=? LIMIT 1; 加上LIMIT 1,只要找到了对应的一条记录,就不会继续向下扫描了,效率会大大提高。 LIMIT 1适用于查询结果为1条(也可能为0)会导致全表扫描的的SQL语句。 如果email是索引的话,就不需要加上LIMIT 1,如果是根据主键查询一条记录也不需要LIMIT 1,主键也是索引。例如:1 SELECT * FROM t_user WHERE id=?;就不需要写成:1 SELECT * FROM t_user WHERE id=? LIMIT 1;二者效率没有区别。存储过程生成100万条数据:1 BEGIN2 DECLARE i INT;3 START TRANSACTION;4 SET i=0;5 WHILE i&lt;1000000 DO6 INSERT INTO t_user VALUES(NULL,CONCAT(i+1,‘@xxg.com’),i+1);7 SET i=i+1;8 END WHILE;9 COMMIT;10 END查询语句 SELECT * FROM t_user WHERE email=’ aliyunzixun@xxx.com‘; 耗时0.56 s SELECT * FROM t_user WHERE email=’ aliyunzixun@xxx.com‘ LIMIT 1; 耗时0.00 s","link":"/2019/06/22/limit/"},{"title":"linux 后台运行python脚本","text":"linux 下后台运行python脚本这两天要在服务器端一直运行一个Python脚本，当然就想到了在命令后面加&amp;符号 $ python /data/python/server.py &gt;python.log &amp;说明：1、 &gt; 表示把标准输出（STDOUT）重定向到 那个文件，这里重定向到了python.log2、 &amp; 表示在后台执行脚本这样可以到达目的，但是，我们退出shell窗口的时候，必须用exit命令来退出，否则，退出之后，该进程也会随着shell的消失而消失（退出、关闭） 使用nohup(not hang up)： $ nohup python /data/python/server.py &gt; python.log3 2&gt;&amp;1 &amp;说明：1、1是标准输出（STDOUT）的文件描述符，2是标准错误（STDERR）的文件描述符1&gt; python.log 简化为 &gt; python.log，表示把标准输出重定向到python.log这个文件2、2&gt;&amp;1 表示把标准错误重定向到标准输出，这里&amp;1表示标准输出为什么需要将标准错误重定向到标准输出的原因，是因为标准错误没有缓冲区，而STDOUT有。这就会导致 commond &gt; python.log 2&gt; python.log 文件python.log被两次打开，而STDOUT和 STDERR将会竞争覆盖，这肯定不是我门想要的3、好了，我们现在可以直接关闭shell窗口（我用的是SecureCRT，用的比较多的还有Xshell），而不用再输入exit这个命令来退出shell了$ ps aux|grep pythontomener 1885 0.1 0.4 13120 4528 pts/0 S 15:48 0:00 python /data/python/server.py tomener 1887 0.0 0.0 5980 752 pts/0 S+ 15:48 0:00 grep python 现在当我们直接关闭shell窗口，再连接上服务器，查看Python的进程，发现进程还在 但是，在python运行中却查看不到输出！ 因为： python的输出有缓冲，导致python.log3并不能够马上看到输出。 使用-u参数，使得python不启用缓冲。 所以改正命令，就可以正常使用了 $ nohup python -u test.py &gt; out.log 2&gt;&amp;1 &amp;","link":"/2019/06/29/linux-后台运行python脚本/"},{"title":"logininterceptor（mycatdemo里的）","text":"12345678910111213141516171819202122232425262728293031323334353637383940public class LoginInterceptor extends HandlerInterceptorAdapter { //不需要拦截的url集合 private List&lt;String&gt; IGNORE_URI; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //获取URI后缀 String requestUri = request.getServletPath(); if(requestUri.equalsIgnoreCase(\"/\")) return true; //过滤不需要拦截的地址 for (String uri : IGNORE_URI) { if (requestUri.startsWith(uri)) { return true; } } HttpSession session = request.getSession(); //session中包含登录状态放行 if(session != null &amp;&amp; session.getAttribute(\"login_status\") != null){ return true; }else{ response.sendRedirect(\"/user/login?timeout=true\"); return false; } } public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { super.postHandle(request, response, handler, modelAndView); } public List&lt;String&gt; getIGNORE_URI() { return IGNORE_URI; } public void setIGNORE_URI(List&lt;String&gt; IGNORE_URI) { this.IGNORE_URI = IGNORE_URI; }}","link":"/2019/09/29/loginintercepter/"},{"title":"mac使用sourcetree跳过注册","text":"mac使用sourcetree跳过注册 https://blog.csdn.net/qq_32890891/article/details/89216954打开sourcetree关闭sourcetree命令终端输入defaults write com.torusknot.SourceTreeNotMAS completedWelcomeWizardVersion 3打开sourcetree即可跳过登录","link":"/2019/10/24/mac使用sourcetree跳过注册/"},{"title":"mac虚拟机设置ip","text":"Mac 以太网连接 报无效的服务器地址 BasicIPv6ValidationError用Mac这么久，一直是用WiFi连接网络，没搞过以太网连接，我也是醉了 显然 Mac 不能像 Windows 一样，插入网线就可以自动连接网络。需要设置一下IP地址 然后悲伤的事情发生了，显示无效的服务器地址 BasicIPv6ValidationError 解决方案： 思路是这样的：先关闭IPv6，然后设置IPv4，再重新开启IPv6。 update 2017.03.14 我发现其实可以直接用命令行修改IPv4，不用管IPv6，如果它没报错的话 关闭 IPv6 显然 ”高级“ &gt; “TCP/IP” 下 IPv6 没有提供关闭选项，所以需要用终端命令 网络命令参看这里 终端输入：networksetup -setv6off Ethernet 这时候系统会弹窗要求输入密码，搞定后你会发现 ”高级“ &gt; “TCP/IP” 下 IPv6 多了个关闭选项 设置IPv4地址 终端输入：networksetup -setmanual Ethernet 192.168.31.2 255.255.255.0 192.168.1.1 对应IP地址、子网掩码、路由器 设置完成后，可以看到，以太网显示状态是：已连接 这个时候已经连接网络了，如果还不能正常上网，比如我是 QQ可以连接，但网页打不开，说明 DNS 有问题 设置DNS","link":"/2019/12/03/mac虚拟机设置ip/"},{"title":"markdown 目录","text":"markdown 实现页内跳转和自动生成目录 接上一篇 markdown 基本语法继续： 实现页内跳转 定义锚点&lt;span id=&quot;jump&quot;&gt;请点击跳转&lt;/span&gt; 使用markdown语法[要跳转到的内容](#jump) 实现目录 在要生成目录的地方写：[TOC] 即可按照标题生成目录有些markdown编辑器需要写： @[TOC] 案例： [TOC] # 马什么梅 ## 马冬梅 ### 马冬什么 #### 马冬梅 ##### 什么冬梅 ###### 马冬梅生成的效果图：","link":"/2019/07/25/markdown-目录/"},{"title":"lombok注解介绍","text":"lombok是一个可以帮助我们简化java代码编写的工具类，尤其是简化javabean的编写，即通过采用注解的方式，消除代码中的构造方法，getter/setter等代码，使我们写的类更加简洁，当然，这带来的副作用就是不易阅读…不过，还是能看得懂吧，废话不多说，先看一下lombok支持的一些常见的注解。 1 @NonNull 2 @Cleanup 3 @Getter/@Setter 4 @ToString 5 @EqualsAndHashCode 6 @NoArgsConstructor/@RequiredArgsConstructor/@AllArgsConstructor 7 @Data 8 @Value 9 @SneakyThrows10 @Synchronized11 @Log @NonNull这个注解可以用在成员方法或者构造方法的参数前面，会自动产生一个关于此参数的非空检查，如果参数为空，则抛出一个空指针异常，举个例子来看看： 1234//成员方法参数加上@NonNull注解public String getName(@NonNull Person p){ return p.getName();} 实际效果相当于： 123456public String getName(@NonNull Person p){ if(p==null){ throw new NullPointerException(\"person\"); } return p.getName();} 用在构造方法的参数上效果类似，就不再举例子了。 @Cleanup这个注解用在变量前面，可以保证此变量代表的资源会被自动关闭，默认是调用资源的close()方法，如果该资源有其它关闭方法，可使用@Cleanup(“methodName”)来指定要调用的方法，就用输入输出流来举个例子吧： 12345678910public static void main(String[] args) throws IOException { @Cleanup InputStream in = new FileInputStream(args[0]); @Cleanup OutputStream out = new FileOutputStream(args[1]); byte[] b = new byte[1024]; while (true) { int r = in.read(b); if (r == -1) break; out.write(b, 0, r); } } 实际效果相当于： 12345678910111213141516171819202122 public static void main(String[] args) throws IOException { InputStream in = new FileInputStream(args[0]); try { OutputStream out = new FileOutputStream(args[1]); try { byte[] b = new byte[10000]; while (true) { int r = in.read(b); if (r == -1) break; out.write(b, 0, r); } } finally { if (out != null) { out.close(); } } } finally { if (in != null) { in.close(); } }} 是不是简化了很多。 @Getter/@Setter这一对注解从名字上就很好理解，用在成员变量前面，相当于为成员变量生成对应的get和set方法，同时还可以为生成的方法指定访问修饰符，当然，默认为public，直接来看下面的简单的例子： 1234567891011public class Programmer{ @Getter @Setter private String name; @Setter(AccessLevel.PROTECTED) private int age; @Getter(AccessLevel.PUBLIC) private String language;} 实际效果相当于： 123456789101112131415161718192021public class Programmer{ private String name; private int age; private String language; public void setName(String name){ this.name = name; } public String getName(){ return name; } protected void setAge(int age){ this.age = age; } public String getLanguage(){ return language; }} 这两个注解还可以直接用在类上，可以为此类里的所有非静态成员变量生成对应的get和set方法。 @ToString/@EqualsAndHashCode这两个注解也比较好理解，就是生成toString，equals和hashcode方法，同时后者还会生成一个canEqual方法，用于判断某个对象是否是当前类的实例，生成方法时只会使用类中的非静态和非transient成员变量，这些都比较好理解，就不举例子了。当然，这两个注解也可以添加限制条件，例如用@ToString(exclude={“param1”，“param2”})来排除param1和param2两个成员变量，或者用@ToString(of={“param1”，“param2”})来指定使用param1和param2两个成员变量，@EqualsAndHashCode注解也有同样的用法。 @NoArgsConstructor/@RequiredArgsConstructor /@AllArgsConstructor这三个注解都是用在类上的，第一个和第三个都很好理解，就是为该类产生无参的构造方法和包含所有参数的构造方法，第二个注解则使用类中所有带有@NonNull注解的或者带有final修饰的成员变量生成对应的构造方法，当然，和前面几个注解一样，成员变量都是非静态的，另外，如果类中含有final修饰的成员变量，是无法使用@NoArgsConstructor注解的。三个注解都可以指定生成的构造方法的访问权限，同时，第二个注解还可以用@RequiredArgsConstructor(staticName=”methodName”)的形式生成一个指定名称的静态方法，返回一个调用相应的构造方法产生的对象，下面来看一个生动鲜活的例子： 12345678910@RequiredArgsConstructor(staticName = \"sunsfan\")@AllArgsConstructor(access = AccessLevel.PROTECTED)@NoArgsConstructorpublic class Shape { private int x; @NonNull private double y; @NonNull private String name;} 实际效果相当于： 1234567891011121314151617181920212223public class Shape { private int x; private double y; private String name; public Shape(){ } protected Shape(int x,double y,String name){ this.x = x; this.y = y; this.name = name; } public Shape(double y,String name){ this.y = y; this.name = name; } public static Shape sunsfan(double y,String name){ return new Shape(y,name); }} @Data/@Value@Data注解综合了3,4,5和6里面的@RequiredArgsConstructor注解，其中@RequiredArgsConstructor使用了类中的带有@NonNull注解的或者final修饰的成员变量，它可以使用@Data(staticConstructor=”methodName”)来生成一个静态方法，返回一个调用相应的构造方法产生的对象。这个例子就也省略了吧…@Value注解和@Data类似，区别在于它会把所有成员变量默认定义为private final修饰，并且不会生成set方法。 @SneakyThrows这个注解用在方法上，可以将方法中的代码用try-catch语句包裹起来，捕获异常并在catch中用Lombok.sneakyThrow(e)把异常抛出，可以使用@SneakyThrows(Exception.class)的形式指定抛出哪种异常，很简单的注解，直接看个例子： 1234567891011public class SneakyThrows implements Runnable { @SneakyThrows(UnsupportedEncodingException.class) public String utf8ToString(byte[] bytes) { return new String(bytes, \"UTF-8\"); } @SneakyThrows public void run() { throw new Throwable(); }} 实际效果相当于： 12345678910111213141516171819public class SneakyThrows implements Runnable { @SneakyThrows(UnsupportedEncodingException.class) public String utf8ToString(byte[] bytes) { try{ return new String(bytes, \"UTF-8\"); }catch(UnsupportedEncodingException uee){ throw Lombok.sneakyThrow(uee); } } @SneakyThrows public void run() { try{ throw new Throwable(); }catch(Throwable t){ throw Lombok.sneakyThrow(t); } }} @Synchronized这个注解用在类方法或者实例方法上，效果和synchronized关键字相同，区别在于锁对象不同，对于类方法和实例方法，synchronized关键字的锁对象分别是类的class对象和this对象，而@Synchronized得锁对象分别是私有静态final对象LOCK和私有final对象LOCK和私有final对象lock，当然，也可以自己指定锁对象，例子也很简单，往下看： 123456789101112131415161718public class Synchronized { private final Object readLock = new Object(); @Synchronized public static void hello() { System.out.println(\"world\"); } @Synchronized public int answerToLife() { return 42; } @Synchronized(\"readLock\") public void foo() { System.out.println(\"bar\"); }} 实际效果相当于： 1234567891011121314151617181920212223public class Synchronized { private static final Object $LOCK = new Object[0]; private final Object $lock = new Object[0]; private final Object readLock = new Object(); public static void hello() { synchronized($LOCK) { System.out.println(\"world\"); } } public int answerToLife() { synchronized($lock) { return 42; } } public void foo() { synchronized(readLock) { System.out.println(\"bar\"); } } } @Log 这个注解用在类上，可以省去从日志工厂生成日志对象这一步，直接进行日志记录，具体注解根据日志工具的不同而不同，同时，可以在注解中使用topic来指定生成log对象时的类名。不同的日志注解总结如下(上面是注解，下面是实际作用)： 1234567891011121314@CommonsLogprivate static final org.apache.commons.logging.Log log = org.apache.commons.logging.LogFactory.getLog(LogExample.class);@JBossLogprivate static final org.jboss.logging.Logger log = org.jboss.logging.Logger.getLogger(LogExample.class);@Logprivate static final java.util.logging.Logger log = java.util.logging.Logger.getLogger(LogExample.class.getName());@Log4jprivate static final org.apache.log4j.Logger log = org.apache.log4j.Logger.getLogger(LogExample.class);@Log4j2private static final org.apache.logging.log4j.Logger log = org.apache.logging.log4j.LogManager.getLogger(LogExample.class);@Slf4jprivate static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(LogExample.class);@XSlf4jprivate static final org.slf4j.ext.XLogger log = org.slf4j.ext.XLoggerFactory.getXLogger(LogExample.class);","link":"/2019/10/11/lombok注解介绍/"},{"title":"markdown任务列表语法checkbox","text":"[ ]a GFM task list 1 GFM task list 2 GFM task list 3 GFM task list 3-1 GFM task list 3-2 GFM task list 3-3 GFM task list 4 GFM task list 4-1 GFM task list 4-2","link":"/2020/05/30/md列表/"},{"title":"mybatis","text":"1、mybatis的概述mybatis是一个持久层框架，用java编写的。 它封装了jdbc操作的很多细节，使开发者只需要关注sql语句本身，而无需关注注册驱动，创建连接等繁杂过程 它使用了ORM思想实现了结果集的封装。 ORM： Object Relational Mappging 对象关系映射 简单的说： 就是把数据库表和实体类及实体类的属性对应起来 让我们可以操作实体类就实现操作数据库表。 user User id userId user_name userName 今天我们需要做到 实体类中的属性和数据库表的字段名称保持一致。 user User id id user_name user_name2、mybatis的入门mybatis的环境搭建 第一步：创建maven工程并导入坐标 第二步：创建实体类和dao的接口 第三步：创建Mybatis的主配置文件 SqlMapConifg.xml 第四步：创建映射配置文件 IUserDao.xml 环境搭建的注意事项： 第一个：创建IUserDao.xml 和 IUserDao.java时名称相同是为了和我们之前的知识保持一致。 在Mybatis中它把持久层的操作接口名称和映射文件也叫做：Mapper 所以：IUserDao 和 IUserMapper是一样的 第二个：在idea中创建目录的时候，它和包是不一样的 包在创建时：com.enlink.dao它是三级结构 目录在创建时：com.enlink.dao是一级目录 第三个：mybatis的映射配置文件位置必须和dao接口的包结构相同 第四个：映射配置文件的mapper标签namespace属性的取值必须是dao接口的全限定类名 第五个：映射配置文件的操作配置（select），id属性的取值必须是dao接口的方法名 当我们遵从了第三，四，五点之后，我们在开发中就无须再写dao的实现类。 mybatis的入门案例 第一步：读取配置文件 第二步：创建SqlSessionFactory工厂 第三步：创建SqlSession 第四步：创建Dao接口的代理对象 第五步：执行dao中的方法 第六步：释放资源 注意事项： 不要忘记在映射配置中告知mybatis要封装到哪个实体类中 配置的方式：指定实体类的全限定类名 mybatis基于注解的入门案例： 把IUserDao.xml移除，在dao接口的方法上使用@Select注解，并且指定SQL语句 同时需要在SqlMapConfig.xml中的mapper配置时，使用class属性指定dao接口的全限定类名。 明确： 我们在实际开发中，都是越简便越好，所以都是采用不写dao实现类的方式。 不管使用XML还是注解配置。 但是Mybatis它是支持写dao实现类的。3、自定义Mybatis的分析：mybatis在使用代理dao的方式实现增删改查时做什么事呢？ 只有两件事： 第一：创建代理对象 第二：在代理对象中调用selectList 自定义mybatis能通过入门案例看到类 class Resources class SqlSessionFactoryBuilder interface SqlSessionFactory interface SqlSession","link":"/2019/07/10/mybatis/"},{"title":"mybatis","text":"Mybatis中updateByPrimaryKeySelective和updateByPrimaryKey区别 原文链接：https://blog.csdn.net/a670941001/article/details/54619432 Mybatis @Select注解，使用in传入ids数组作为参数https://blog.csdn.net/qq_2300688967/article/details/81186420","link":"/2020/04/14/mybatis一些用法/"},{"title":"mycat","text":"Mycat前世今生2013年阿里的Cobar在社区使用过程中发现存在一些比较严重的问题，及其使用限制，经过Mycat发起人第一次改良，第一代改良版——Mycat诞生。 Mycat开源以后，一些Cobar的用户参与了Mycat的开发，最终Mycat发展成为一个由众多软件公司的实力派架构师和资深开发人员维护的社区型开源软件。 2014年Mycat首次在上海的《中华架构师》大会上对外宣讲，更多的人参与进来，随后越来越多的项目采用了Mycat。 2015年5月，由核心参与者们一起编写的第一本官方权威指南《Mycat权威指南》电子版发布，累计超过500本，成为开源项目中的首创。 2015年10月为止，Mycat项目总共有16个Committer。 截至2015年11月，超过300个项目采用Mycat，涵盖银行、电信、电子商务、物流、移动应用、O2O的众多领域和公司。 截至2015年12月，超过4000名用户加群或研究讨论或测试或使用Mycat。 Mycat是基于开源cobar演变而来，我们对cobar的代码进行了彻底的重构，使用NIO重构了网络模块，并且优化了Buffer内核，增强了聚合，Join等基本特性，同时兼容绝大多数数据库成为通用的数据库中间件。1.4 版本以后 完全的脱离基本cobar内核，结合Mycat集群管理、自动扩容、智能优化，成为高性能的中间件。我们致力于开发高性能数据库中间而努力。永不收费，永不闭源，持续推动开源社区的发展。 Mycat吸引和聚集了一大批业内大数据和云计算方面的资深工程师，Mycat的发展壮大基于开源社区志愿者的持续努力，感谢社区志愿者的努力让Mycat更加强大，同时我们也欢迎社区更多的志愿者，特别是公司能够参与进来，参与Mycat的开发，一起推动社区的发展，为社区提供更好的开源中间件。 Mycat还不够强大，Mycat还有很多不足，欢迎社区志愿者的持续优化改进。 关键特性支持SQL92标准 遵守Mysql原生协议，跨语言，跨平台，跨数据库的通用中间件代理。 基于心跳的自动故障切换，支持读写分离，支持MySQL主从，以及galera cluster集群。 支持Galera for MySQL集群，Percona Cluster或者MariaDB cluster 基于Nio实现，有效管理线程，高并发问题。 支持数据的多片自动路由与聚合，支持sum,count,max等常用的聚合函数。 支持单库内部任意join，支持跨库2表join，甚至基于caltlet的多表join。 支持通过全局表，ER关系的分片策略，实现了高效的多表join查询。 支持多租户方案。 支持分布式事务（弱xa）。 支持全局序列号，解决分布式下的主键生成问题。 分片规则丰富，插件化开发，易于扩展。 强大的web，命令行监控。 支持前端作为mysq通用代理，后端JDBC方式支持Oracle、DB2、SQL Server 、 mongodb 、巨杉。 支持密码加密 支持服务降级 支持IP白名单 支持SQL黑名单、sql注入攻击拦截 支持分表（1.6） 集群基于ZooKeeper管理，在线升级，扩容，智能优化，大数据处理（2.0开发版）。 Mycat安装与使用下载：https://github.com/MyCATApache/Mycat-download 具体下载哪个版本以发布为准，推荐1.4,1.5. 安装：下载的文件直接解压即可。 运行：linux： ./mycat start 启动 ./mycat stop 停止 ./mycat console 前台运行 ./mycat install 添加到系统自动启动（暂未实现） ./mycat remove 取消随系统自动启动（暂未实现） ./mycat restart 重启服务 ./mycat pause 暂停 ./mycat status 查看启动状态 win：直接运行startup_nowrap.bat，如果出现闪退，在cmd 命令行运行，查看出错原因。 内存配置：启动前，一般需要修改JVM配置参数，打开conf/wrapper.conf文件，如下行的内容为2G和2048，可根据本机配置情况修改为512M或其它值。 以下配置跟jvm参数完全一致，可以根据自己的jvm参数调整。 1234567891011121314151617181920212223242526272829303132333435363738394041Java Additional Parameterswrapper.java.additional.1=wrapper.java.additional.1=-DMYCAT_HOME=.wrapper.java.additional.2=-serverwrapper.java.additional.3=-XX:MaxPermSize=64Mwrapper.java.additional.4=-XX:+AggressiveOptswrapper.java.additional.5=-XX:MaxDirectMemorySize=100mwrapper.java.additional.6=-Dcom.sun.management.jmxremotewrapper.java.additional.7=-Dcom.sun.management.jmxremote.port=1984wrapper.java.additional.8=-Dcom.sun.management.jmxremote.authenticate=falsewrapper.java.additional.9=-Dcom.sun.management.jmxremote.ssl=falsewrapper.java.additional.10=-Xmx100mwrapper.java.additional.11=-Xms100mwrapper.java.additional.12=-XX:+UseParNewGCwrapper.java.additional.13=-XX:+UseConcMarkSweepGCwrapper.java.additional.14=-XX:+UseCMSCompactAtFullCollectionwrapper.java.additional.15=-XX:CMSFullGCsBeforeCompaction=0wrapper.java.additional.16=-XX:CMSInitiatingOccupancyFraction=70以下配置作废：wrapper.java.initmemory=3wrapper.java.maxmemory=64 Mycat连接测试：测试mycat与测试mysql完全一致，mysql怎么连接，mycat就怎么连接。 推荐先采用命令行测试： mysql -uroot -proot -P8066 -h127.0.0.1 如果采用工具连接，1.4,1.3目前部分工具无法连接，会提示database not selected，建议采用高版本，navicat测试。1.5已经修复了部分工具连接。 Mycat配置入门配置： 1234567891011121314151617--bin 启动目录--conf 配置目录存放配置文件：--server.xml：是Mycat服务器参数调整和用户授权的配置文件。 --schema.xml：是逻辑库定义和表以及分片定义的配置文件。 --rule.xml： 是分片规则的配置文件，分片规则的具体一些参数信息单独存放为文件，也在这个目录下，配置文件修改需要重启MyCAT。 --log4j.xml： 日志存放在logs/log中，每天一个文件，日志的配置是在conf/log4j.xml中，根据自己的需要可以调整输出级别为debug debug级别下，会输出更多的信息，方便排查问题。 --autopartition-long.txt,partition-hash-int.txt,sequence_conf.properties， sequence_db_conf.properties 分片相关的id分片规则配置文件 --lib MyCAT自身的jar包或依赖的jar包的存放目录。 --logs MyCAT日志的存放目录。日志存放在logs/log中，每天一个文件 逻辑库配置：配置server.xml添加两个mycat逻辑库：user,paysystem 参数是所有的mycat参数配置，比如添加解析器：defaultSqlParser，其他类推user 是用户参数。 12345678910111213&lt;system&gt; &lt;property name=\"defaultSqlParser\"&gt;druidparser&lt;/property&gt;&lt;/system&gt;&lt;user name=\"mycat\"&gt; &lt;property name=\"password\"&gt;mycat&lt;/property&gt; &lt;property name=\"schemas\"&gt;user,pay&lt;/property&gt;&lt;/user&gt; 编辑schema.xml修改dataHost和schema对应的连接信息，user,pay 垂直切分后的配置如下所示： schema 是实际逻辑库的配置，user，pay分别对应两个逻辑库，多个schema代表多个逻辑库。 dataNode是逻辑库对应的分片，如果配置多个分片只需要多个dataNode即可。 dataHost是实际的物理库配置地址，可以配置多主主从等其他配置，多个dataHost代表分片对应的物理库地址，下面的writeHost、readHost代表该分片是否配置多写，主从，读写分离等高级特性。 以下例子配置了两个writeHost为主从。 12345678910111213141516&lt;schema name=\"user\" checkSQLschema=\"false\" sqlMaxLimit=\"100\" dataNode=\"user\" /&gt;&lt;schema name=\"pay\" checkSQLschema=\"false\" sqlMaxLimit=\"100\" dataNode=\"pay\" &gt; &lt;table name=\"order\" dataNode=\"pay1,pay2\" rule=\"rule1\"/&gt;&lt;/schema&gt;&lt;dataNode name=\"user\" dataHost=\"host\" database=\"user\" /&gt;&lt;dataNode name=\"pay1\" dataHost=\"host\" database=\"pay1\" /&gt;&lt;dataNode name=\"pay2\" dataHost=\"host\" database=\"pay2\" /&gt;&lt;dataHost name=\"host\" maxCon=\"1000\" minCon=\"10\" balance=\"0\" writeType=\"0\" dbType=\"mysql\" dbDriver=\"native\"&gt; &lt;heartbeat&gt;select 1&lt;/heartbeat&gt; &lt;!-- can have multi write hosts --&gt; &lt;writeHost host=\"hostM1\" url=\"192.168.0.2:3306\" user=\"root\" password=\"root\" /&gt; &lt;writeHost host=\"hostM2\" url=\"192.168.0.3:3306\" user=\"root\" password=\"root\" /&gt;&lt;/dataHost&gt; Mycat逻辑库、系统参数配置配置Mycat环境参数 1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;&lt;!DOCTYPE mycat:server SYSTEM \"server.dtd\"&gt;&lt;mycat:server xmlns:mycat=\"http://org.opencloudb/\"&gt; &lt;system&gt; &lt;property name=\"defaultSqlParser\"&gt;druidparser&lt;/property&gt; &lt;/system&gt; &lt;/mycat:server&gt; 如例子中配置的所有的Mycat参数变量都是配置在server.xml 文件中，system标签下配置所有的参数，如果需要配置某个变量添加相应的配置即可，例如添加启动端口8066，默认为8066： &lt;property name=&quot;serverPort&quot;&gt;8066&lt;/property&gt; 其他所有变量类似。 配置Mycat逻辑库与用户 12345678 &lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;&lt;!DOCTYPE mycat:server SYSTEM \"server.dtd\"&gt;&lt;mycat:server xmlns:mycat=\"http://org.opencloudb/\"&gt;&lt;user name=\"mycat\"&gt; &lt;property name=\"password\"&gt;mycat&lt;/property&gt; &lt;property name=\"schemas\"&gt;TESTDB&lt;/property&gt;&lt;/user&gt; &lt;/mycat:server&gt; 如例子中配置的所有的Mycat连接的用户与逻辑库映射都是配置在server.xml 文件中，user标签下配置所有的参数，例如例子中配置了一个mycat用户供应用连接到mycat，同时mycat 在schema.xml中配置后了一个逻辑库TESTDB，配置好逻辑库与用户的映射关系。 逻辑库、表分片配置配置逻辑库（schema）Mycat作为一个中间件，实现mysql协议，那么对前端应用连接来说就是一个数据库，也就有数据库的配置，mycat的数据库配置是在schema.xml中配置，配置好后映射到server.xml里面的用户就可以了。 1234567891011121314151617&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mycat:schema SYSTEM \"schema.dtd\"&gt;&lt;mycat:schema xmlns:mycat=\"http://org.opencloudb/\"&gt; &lt;schema name=\"TESTDB\" checkSQLschema=\"true\" sqlMaxLimit=\"100\" dataNode=\"dn1\"&gt; &lt;table name=\"t_user\" dataNode=\"dn1,dn2\" rule=\"sharding-by-mod2\"/&gt; &lt;table name=\"ht_jy_login_log\" primaryKey=\"ID\" dataNode=\"dn1,dn2\" rule=\"sharding-by-date_jylog\"/&gt; &lt;/schema&gt; &lt;dataNode name=\"dn1\" dataHost=\"localhost1\" database=\"mycat_node1\"/&gt; &lt;dataNode name=\"dn2\" dataHost=\"localhost1\" database=\"mycat_node2\"/&gt; &lt;dataHost name=\"localhost1\" writeType=\"0\" switchType=\"1\" slaveThreshold=\"100\" balance=\"1\" dbType=\"mysql\" maxCon=\"10\" minCon=\"1\" dbDriver=\"native\"&gt; &lt;heartbeat&gt;show status like 'wsrep%'&lt;/heartbeat&gt; &lt;writeHost host=\"hostM1\" url=\"127.0.0.1:3306\" user=\"root\" password=\"root\" &gt; &lt;/writeHost&gt; &lt;/dataHost&gt;&lt;/mycat:schema &gt; 上面例子配置了一个逻辑库TESTDB，同时配置了t_user，ht_jy_login_log两个分片表。 逻辑表配置&lt;table name=&quot;t_user&quot; dataNode=&quot;dn1,dn2&quot; rule=&quot;sharding-by-mod2&quot;/&gt; table 标签 是逻辑表的配置 其中 name代表表名， dataNode代表表对应的分片， Mycat默认采用分库方式，也就是一个表映射到不同的库上， rule代表表要采用的数据切分方式，名称对应到rule.xml中的对应配置，如果要分片必须配置。 配置分片（dataNode）12&lt;dataNode name=\"dn1\" dataHost=\"localhost1\" database=\"mycat_node1\"/&gt;&lt;dataNode name=\"dn2\" dataHost=\"localhost1\" database=\"mycat_node2\"/&gt; 表切分后需要配置映射到哪几个数据库中，Mycat的分片实际上就是库的别名，例如上面例子配置了两个分片dn1，dn2 分别对应到物理机映射dataHost localhost1 的两个库上。 配置物理库分片映射（dataHost）12345&lt;dataHost name=\"localhost1\" writeType=\"0\" switchType=\"1\" slaveThreshold=\"100\" balance=\"1\" dbType=\"mysql\" maxCon=\"10\" minCon=\"1\" dbDriver=\"native\"&gt; &lt;heartbeat&gt;show status like 'wsrep%'&lt;/heartbeat&gt; &lt;writeHost host=\"hostM1\" url=\"127.0.0.1:3306\" user=\"root\" password=\"root\" &gt; &lt;/writeHost&gt; &lt;/dataHost&gt; Mycat作为数据库代理需要逻辑库，逻辑用户，表切分后需要配置分片，分片也就需要映射到真实的物理主机上，至于是映射到一台还是一台的多个实例上，Mycat并不关心，只需要配置好映射即可，例如例子中： 配置了一个名为localhost1的物理主机（dataHost）映射。 heartbeat 标签代表Mycat需要对物理库心跳检测的语句，正常情况下生产案例可能配置主从，或者多写 或者单库，无论哪种情况Mycat都需要维持到数据库的数据源连接，因此需要定时检查后端连接可以性，心跳语句就是来作为心跳检测。 writeHost 此标签代表 一个逻辑主机（dataHost）对应的后端的物理主机映射，例如例子中写库hostM1 映射到127.0.0.1:3306。如果后端需要做读写分离或者多写 或者主从则通过配置 多个writeHost 或者readHost即可。 dataHost 标签中的 writeType balance 等标签则是不同的策略，具体参考指南。Mycat 表切分规则配置表切分规则 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mycat:rule SYSTEM \"rule.dtd\"&gt;&lt;mycat:rule xmlns:mycat=\"http://org.opencloudb/\"&gt; &lt;tableRule name=\"sharding-by-hour\"&gt; &lt;rule&gt; &lt;columns&gt;createTime&lt;/columns&gt; &lt;algorithm&gt;sharding-by-hour&lt;/algorithm&gt; &lt;/rule&gt; &lt;/tableRule&gt; &lt;function name=\"sharding-by-hour\" class=\"org.opencloudb.route.function.LatestMonthPartion\"&gt; &lt;property name=\"splitOneDay\"&gt;24&lt;/property&gt; &lt;/function&gt; &lt;/mycat:rule &gt; 数据切分中作为表切分规则中最重要的配置，表的切分方式决定了数据切分后的性能好坏，因此也是最重要的配置。 如上面例子配置了一个切分规则，名为sharding-by-hour 对应的切分方式（function ）是按日期切分，该配置中： tableRulename 为schema.xml 中table 标签中对应的 rule=”sharding-by-hour” ,也就是配置表的分片规则， columns 是表的切分字段： createTime 创建日期。 algorithm 是规则对应的切分规则：映射到function 的name。 functionfunction 配置是分片规则的配置。 name 为切分规则的名称，名字任意取，但是需要与tableRule 中匹配。 class 是切分规则对应的切分类，写死，需要哪种规则则配置哪种，例如本例子是按小时分片：org.opencloudb.route.function.LatestMonthPartion property 标签是切分规则对应的不同属性，不同的切分规则配置不同。","link":"/2019/09/24/mycat/"},{"title":"onos ENV","text":"虚拟机开发搭建所采用的是SDNHUB上封好的VirtualBox虚拟机（页面上有虚拟机账号密码），下载地址为：http://sdnhub.org/tutorials/sdn-tutorial-vm/ 开始搭建： 1：在第一次启动的话，默认会打开火狐浏览器和一个终端，并且会提示是否更新系统（我和同事都选的否）。 2：在终端中输入（因为onos目录直接在~/下，打开终端直接依此输入以下命令）： 123456789$ cd onos$ source ./tools/dev/bash_profile $ echo $KARAF_ROOT$ mvn clean install -nsu -DskipIT -DskipTests$ onos-setup-karaf clean$ ok cleanhttp://localhost:8181/onos/ui/index.html自带eclipse 真机开发1.check out source code 1234567891011git clone https://gerrit.onosproject.org/onos/cd onos source tools/dev/bash_profile 设置环境量 bazel build onos 编译onos bazel test 测试 bazel run onos-local 在后台启动 onos localhost 进入后台可以 onos-gui localhost 即可跳到gui页面 username:onos password:rocks导入ideabazel project 2.设置环境变量 123vi ～/.bash_profileexport ONOS_ROOT=~/onos/source $ONOS_ROOT/tools/dev/bash_profile 1.查看下本地IP2.设置环境变量export ONOS_IP=本地IP3.启动onosonos-karaf clean","link":"/2019/08/10/onos-ENV/"},{"title":"onos-buck 源码编译安装","text":"老版本 onos-buckONOS新版本源码安装方式发表于 2017-03-08 | 分类于 项目展示 | | 访问量:1.安装JDK（从Oracle官网下载，不要用OpenJDK）1.Download and Extract JDK package, such as jdk-8u121-linux-x64.tar.gz.tar -xzf jdk-8u121-linux-x64.tar.gz and you can see new directory /home/yourname/jdk1.8.0_121.2.Add two environment variables into ~/.bashrc or /etc/profile: 12export JAVA_HOME=/home/yourname/jdk1.8.0_121export PATH=${JAVA_HOME}/bin:$PATH 3.Save and open a new terminal, go on next step. 12345sudo apt-get install software-properties-common -y &amp;&amp; \\sudo add-apt-repository ppa:webupd8team/java -y &amp;&amp; \\sudo apt-get update &amp;&amp; \\echo \"oracle-java8-installer shared/accepted-oracle-license-v1-1 select true\" | sudo debconf-set-selections &amp;&amp; \\sudo apt-get install oracle-java8-installer oracle-java8-set-default -y 2.下载源码并编译 123git clone https://gerrit.onosproject.org/onoscd onos #之后的命令均在此目录下tools/build/onos-buck build onos --show-output 正常的话，会打印出.tar.gz目录，这个就是二进制安装文件。3.本地运行ONOStools/build/onos-buck run onos-local -- clean debug以上命令将会从本地的 onos.tar.gz文件中进行本地安装，并在后台开启ONOS服务。在终端上将会持续打出ONOS的日志内容。clean选项会使其进行ONOS的清洁安装，而debug选项意为默认调试端口5005将会开启。4.设置环境变量在/etc/profile中添加： 12export ONOS_ROOT=/home/yourname/onossource $ONOS_ROOT/tools/dev/bash_profile 5.开启ONOS的CLI控制台在本地开启ONOS之后，在新的终端执行以下命令：tools/test/bin/onos localhost6.开启ONOS的GUI界面在本地开启ONOS之后，在新的终端执行以下命令：tools/test/bin/onos-gui localhost #（实际执行了open http://$host:8181/onos/ui，Ubuntu不支持， #出错Couldn’t get a file descriptor referring to the console）或者访问http://localhost:8181/onos/ui （推荐）默认用户名onos，密码rocks7.ONOS单元测试To execute ONOS unit tests, including code Checkstyle validation, run the following command:tools/build/onos-buck test8.导入工程到IDEIf you want to import the project into IntelliJ, you can generate the hierarchical module structure via the following command:tools/build/onos-buck project Then simply open the onos directory from IntelliJ IDEA.","link":"/2019/07/20/onos-源码编译安装/"},{"title":"mysql共享锁和排它锁","text":"一、相关名词 |–表级锁（锁定整个表） |–页级锁（锁定一页） |–行级锁（锁定一行） |–共享锁（S锁，MyISAM 叫做读锁） |–排他锁（X锁，MyISAM 叫做写锁） |–悲观锁（抽象性，不真实存在这个锁） |–乐观锁（抽象性，不真实存在这个锁） 二、InnoDB与MyISAM Mysql 在5.5之前默认使用 MyISAM 存储引擎，之后使用 InnoDB 。查看当前存储引擎： show variables like ‘%storage_engine%’; MyISAM 操作数据都是使用的表锁，你更新一条记录就要锁整个表，导致性能较低，并发不高。当然同时它也不会存在死锁问题。 而 InnoDB 与 MyISAM 的最大不同有两点：一是 InnoDB 支持事务；二是 InnoDB 采用了行级锁。也就是你需要修改哪行，就可以只锁定哪行。 在 Mysql 中，行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql 语句操作了主键索引，Mysql 就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL会先锁定该非主键索引，再锁定相关的主键索引。 InnoDB 行锁是通过给索引项加锁实现的，如果没有索引，InnoDB 会通过隐藏的聚簇索引来对记录加锁。也就是说：如果不通过索引条件检索数据，那么InnoDB将对表中所有数据加锁，实际效果跟表锁一样。因为没有了索引，找到某一条记录就得扫描全表，要扫描全表，就得锁定表。 三、共享锁与排他锁 1.首先说明：数据库的增删改操作默认都会加排他锁，而查询不会加任何锁。 |–共享锁：对某一资源加共享锁，自身可以读该资源，其他人也可以读该资源（也可以再继续加共享锁，即 共享锁可多个共存），但无法修改。要想修改就必须等所有共享锁都释放完之后。语法为： select * from table lock in share mode |–排他锁：对某一资源加排他锁，自身可以进行增删改查，其他人无法进行任何操作。语法为： select * from table for update 2.下面援引例子说明（援自：http://blog.csdn.net/samjustin1/article/details/52210125）： 这里用T1代表一个数据库执行请求，T2代表另一个请求，也可以理解为T1为一个线程，T2 为另一个线程。 例1：————————————————————————————————————————————- T1:select * from table lock in share mode（假设查询会花很长时间，下面的例子也都这么假设） T2:update table set column1=’hello’ 过程： T1运行（并加共享锁) T2运行 If T1还没执行完 T2等…… else 锁被释放 T2执行 end if T2 之所以要等，是因为 T2 在执行 update 前，试图对 table 表加一个排他锁，而数据库规定同一资源上不能同时共存共享锁和排他锁。所以 T2 必须等 T1 执行完，释放了共享锁，才能加上排他锁，然后才能开始执行 update 语句。 例2：————————————————————————————————————————————- T1:select * from table lock in share mode T2:select * from table lock in share mode 这里T2不用等待T1执行完，而是可以马上执行。 分析： T1运行，则 table 被加锁，比如叫lockA，T2运行，再对 table 加一个共享锁，比如叫lockB，两个锁是可以同时存在于同一资源上的（比如同一个表上）。这被称为共享锁与共享锁兼容。这意味着共享锁不阻止其它人同时读资源，但阻止其它人修改资源。 例3：————————————————————————————————————————————- T1:select * from table lock in share mode T2:select * from table lock in share mode T3:update table set column1=’hello’ T2 不用等 T1 运行完就能运行，T3 却要等 T1 和 T2 都运行完才能运行。因为 T3 必须等 T1 和 T2 的共享锁全部释放才能进行加排他锁然后执行 update 操作。 例4：（死锁的发生）—————————————————————————————————————– T1:begin tran select * from table lock in share mode update table set column1=&apos;hello&apos;T2:begin tran select * from table lock in share mode update table set column1=&apos;world&apos;假设 T1 和 T2 同时达到 select，T1 对 table 加共享锁，T2 也对 table 加共享锁，当 T1 的 select 执行完，准备执行 update 时，根据锁机制，T1 的共享锁需要升级到排他锁才能执行接下来的 update。在升级排他锁前，必须等 table 上的其它共享锁（T2）释放，同理，T2 也在等 T1 的共享锁释放。于是死锁产生了。 例5：————————————————————————————————————————————- T1:begin tran update table set column1=&apos;hello&apos; where id=10T2:begin tran update table set column1=&apos;world&apos; where id=20这种语句虽然最为常见，很多人觉得它有机会产生死锁，但实际上要看情况 |–如果id是主键（默认有主键索引），那么T1会一下子找到该条记录(id=10的记录），然后对该条记录加排他锁，T2，同样，一下子通过索引定位到记录，然后对id=20的记录加排他锁，这样T1和T2各更新各的，互不影响。T2也不需要等。 |–如果id是普通的一列，没有索引。那么当T1对id=10这一行加排他锁后，T2为了找到id=20，需要对全表扫描。但因为T1已经为一条记录加了排他锁，导致T2的全表扫描进行不下去（其实是因为T1加了排他锁，数据库默认会为该表加意向锁，T2要扫描全表，就得等该意向锁释放，也就是T1执行完成），就导致T2等待。 死锁怎么解决呢？一种办法是，如下： 例6：————————————————————————————————————————————- T1:begin tran select * from table for update update table set column1=&apos;hello&apos;T2:begin tran select * from table for update update table set column1=&apos;world&apos;这样，当 T1 的 select 执行时，直接对表加上了排他锁，T2 在执行 select 时，就需要等 T1 事物完全执行完才能执行。排除了死锁发生。但当第三个 user 过来想执行一个查询语句时，也因为排他锁的存在而不得不等待，第四个、第五个 user 也会因此而等待。在大并发情况下，让大家等待显得性能就太友好了。 所以，有些数据库这里引入了更新锁（如Mssql，注意：Mysql不存在更新锁）。 例7：————————————————————————————————————————————- T1:begin tran select * from table [加更新锁操作] update table set column1=&apos;hello&apos;T2:begin tran select * from table [加更新锁操作] update table set column1=&apos;world&apos;更新锁其实就可以看成排他锁的一种变形，只是它也允许其他人读（并且还允许加共享锁）。但不允许其他操作，除非我释放了更新锁。T1 执行 select，加更新锁。T2 运行，准备加更新锁，但发现已经有一个更新锁在那儿了，只好等。当后来有 user3、user4…需要查询 table 表中的数据时，并不会因为 T1 的 select 在执行就被阻塞，照样能查询，相比起例6，这提高了效率。 后面还有意向锁和计划锁： 计划锁，和程序员关系不大，就没去了解。意向锁（innodb特有）分意向共享锁和意向排他锁。意向共享锁：表示事务获取行共享锁时，必须先得获取该表的意向共享锁；意向排他锁：表示事务获取行排他锁时，必须先得获取该表的意向排他锁；我们知道，如果要对整个表加锁，需保证该表内目前不存在任何锁。 因此，如果需要对整个表加锁，那么就可以根据：检查意向锁是否被占用，来知道表内目前是否存在共享锁或排他锁了。而不需要再一行行地去检查每一行是否被加锁。 四、乐观锁与悲观锁 首先说明，乐观锁和悲观锁都是针对读（select）来说的。 案例： 某商品，用户购买后库存数应-1，而某两个或多个用户同时购买，此时三个执行程序均同时读得库存为“n”，之后进行了一些操作，最后将均执行update table set 库存数=n-1，那么，很显然这是错误的。 解决： 使用悲观锁（其实说白了也就是排他锁） |– 程序A在查询库存数时使用排他锁（select * from table where id=10 for update） |– 然后进行后续的操作，包括更新库存数，最后提交事务。 |– 程序B在查询库存数时，如果A还未释放排他锁，它将等待…… |– 程序C同B……使用乐观锁（靠表设计和代码来实现） |– 一般是在该商品表添加version版本字段或者timestamp时间戳字段 |– 程序A查询后，执行更新变成了： update table set num=num-1 where id=10 and version=23 这样，保证了修改的数据是和它查询出来的数据是一致的（其他执行程序肯定未进行修改）。当然，如果更新失败，表示在更新操作之前，有其他执行程序已经更新了该库存数，那么就可以尝试重试来保证更新成功。为了尽可能避免更新失败，可以合理调整重试次数（阿里巴巴开发手册规定重试次数不低于三次）。总结：对于以上，可以看得出来乐观锁和悲观锁的区别： 悲观锁实际使用了排他锁来实现（select **** for update）。文章开头说到，innodb加行锁的前提是：必须是通过索引条件来检索数据，否则会切换为表锁。 因此，悲观锁在未通过索引条件检索数据时，会锁定整张表。导致其他程序不允许“加锁的查询操作”，影响吞吐。故如果在查询居多的情况下，推荐使用乐观锁。 “加锁的查询操作”：加过排他锁的数据行在其他事务中是不能修改的，也不能通过for update或lock in share mode的加锁方式查询，但可以直接通过select …from…查询数据，因为普通查询没有任何锁机制。乐观锁更新有可能会失败，甚至是更新几次都失败，这是有风险的。所以如果写入居多，对吞吐要求不高，可使用悲观锁。也就是一句话：读用乐观锁，写用悲观锁。","link":"/2020/05/29/mysql共享锁和排它锁/"},{"title":"pip安装","text":"123wget https://bootstrap.pypa.io/get-pip.pypython get-pip.pypip -V get-pip.py 已下载到了resource下/data/data/com.termux/files/home/storage/downloads/resource/ pip版本pip-20.0.2 wheel-0.34.2 点击下载：get-pip 下载地址： http://tong.com:8080/index.php?user/publicLink&amp;fid=83e1qeAvyi6t3s5yS9PvxzbYpKHa1_LdsgQa-_WP5Zz_pQ1AOqtsdcYPt9ZGE2bGq3DMbqO-iRrdBtDi2pYXrtMSIyGgEgNHny3KRvXlJqDQuXlToJ3H5yI9rffLaFIxhY3x&amp;file_name=/get-pip.py","link":"/2020/02/22/pip安装/"},{"title":"push existing folder to git","text":"Command line instructions You can also upload existing files from your computer using the instructions below. 123Git global setup git config --global user.name \"Tong\" git config --global user.email \"joetonghao@126.com\" Create a new repository 123456git clone http://tong.lab.com/Joe/springboot-login.git cd springboot-login touch README.md git add README.md git commit -m \"add README\" git push -u origin master Push an existing folder 123456cd existing_folder git init git remote add origin http://tong.lab.com/Joe/springboot-login.git git add .git commit -m \"Initial commit\" git push -u origin master Push an existing Git repository 12345cd existing_repo git remote rename origin old-origin git remote add origin http://tong.lab.com/Joe/springboot-login.git git push -u origin --all git push -u origin --tags git - 查看远程仓库信息 git remote show origin","link":"/2019/07/17/push-existing-folder-to-git/"},{"title":"python字符乱码问题","text":"HTMLTestRunner解决UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xe6 in position 36: ordinal not报错提示如下： UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xe6 in position 36: ordinal not in range(128) 原因是：python的str默认是ascii编码，和unicode编码冲突，就会报这个标题错误 解决的办法是，在开头添加如下代码： 123import sysreload(sys)sys.setdefaultencoding('utf8') 完美解决问题，对了，我使用的Python版本是Python2.7.13，HTMLTestRunner也是Python2的版本。","link":"/2019/06/29/python字符乱码问题/"},{"title":"python读取数据库","text":"123456789101112131415161718192021222324252627# -*- coding: UTF-8 -*-import MySQLdb# 打开数据库连接db = MySQLdb.connect(host=\"localhost\", user=\"root\", passwd=\"123456\", db=\"study\", charset='utf8' )# 使用cursor()方法获取操作游标 cursor = db.cursor()# SQL 查询语句sql = \"SELECT * FROM student\"try: # 执行SQL语句 cursor.execute(sql) # 获取所有记录列表 results = cursor.fetchall() print (\"学号 姓名 年龄\") for it in results: for i in range(len(it)): print it[i], print ('\\n')except: print \"Error: unable to fecth data\"# 关闭数据库连接cursor.close()db.close()","link":"/2019/10/24/python读数据库/"},{"title":"python追加内容","text":"Python追加文件内容 开始用的如下的write()方法，发下会先把原文件的内容清空再写入新的东西，文件里面每次都是最新生成的一个账号 1234mobile = Method.createPhone()file = r'D:\\test.txt'with open(file, 'w+') as f: f.write(mobile) 查了资料，关于open()的mode参数： 123456789101112131415'r'：读'w'：写'a'：追加'r+' == r+w（可读可写，文件若不存在就报错(IOError)）'w+' == w+r（可读可写，文件若不存在就创建）'a+' ==a+r（可追加可写，文件若不存在就创建）对应的，如果是二进制文件，就都加一个b就好啦：'rb' 'wb' 'ab' 'rb+' 'wb+' 'ab+' 发现方法用错了，像这种不断生成新账号 增加写入的，应该用追加‘a’ 改为如下后，解决： 1234mobile = Method.createPhone()file = r'D:\\test.txt'with open(file, 'a+') as f: f.write(mobile+'\\n') ***#加\\n换行显示***","link":"/2019/07/04/python追加内容/"},{"title":"redis share session","text":"redis解决session共享问题@EnableRedisHttpSessionpom.xml 1234567891011121314151617181920212223242526272829303132&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;!-- &lt;version&gt;${spring_versin}&lt;/version&gt; --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;!-- &lt;version&gt;${spring_versin}&lt;/version&gt; --&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-redis&lt;/artifactId&gt; &lt;version&gt;1.4.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.yml的配置 123456789101112131415161718192021logging: config: classpath:logback.xml path: d:/logsserver: port: 8080 session-timeout: 60spring: redis: database: 0 host: 127.0.0.1 port: 6379 password: timeout: 0 pool: max-active: 8 max-wait: -1 max-idle: 8 min-idle: 0 session: store-type: none RedisSessionConfig.javapackage com.fei.springboot.config;import org.springframework.context.annotation.Configuration;import org.springframework.session.data.redis.config.annotation.web.http.EnableRedisHttpSession; 1@Configuration//maxInactiveIntervalInSeconds 默认是1800秒过期，这里测试修改为60秒@EnableRedisHttpSession(maxInactiveIntervalInSeconds=60)public class RedisSessionConfig{ } 如果不需要做特殊处理，只需直接使用注解 @EnableRedisHttpSession即可，打开@EnableRedisHttpSession源码，发现maxInactiveIntervalInSeconds session的过期时间默认是1800秒即30分钟，如果需要修改，注解时进行修改即可。如果想对redisSession做一些特殊处理。看@EnableRedisHttpSession源码，头部的注释，也给出了一些方案。 注解@interface EnableRedisHttpSession的源码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/* * Copyright 2014-2016 the original author or authors. * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.springframework.session.data.redis.config.annotation.web.http; import java.lang.annotation.Documented;import java.lang.annotation.Retention;import java.lang.annotation.Target; import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Import;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.session.SessionRepository;import org.springframework.session.config.annotation.web.http.EnableSpringHttpSession;import org.springframework.session.data.redis.RedisFlushMode;/** * Add this annotation to an {@code @Configuration} class to expose the * SessionRepositoryFilter as a bean named \"springSessionRepositoryFilter\" and backed by * Redis. In order to leverage the annotation, a single {@link RedisConnectionFactory} * must be provided. For example: &lt;pre&gt; * &lt;code&gt; * {@literal @Configuration} * {@literal @EnableRedisHttpSession} * public class RedisHttpSessionConfig { * * {@literal @Bean} * public JedisConnectionFactory connectionFactory() throws Exception { * return new JedisConnectionFactory(); * } * * } * &lt;/code&gt; &lt;/pre&gt; * * More advanced configurations can extend {@link RedisHttpSessionConfiguration} instead. * * @author Rob Winch * @since 1.0 * @see EnableSpringHttpSession */@Retention(java.lang.annotation.RetentionPolicy.RUNTIME)@Target({ java.lang.annotation.ElementType.TYPE })@Documented@Import(RedisHttpSessionConfiguration.class)@Configurationpublic @interface EnableRedisHttpSession { int maxInactiveIntervalInSeconds() default 1800; /** * &lt;p&gt; * Defines a unique namespace for keys. The value is used to isolate sessions by * changing the prefix from \"spring:session:\" to * \"spring:session:&lt;redisNamespace&gt;:\". The default is \"\" such that all Redis * keys begin with \"spring:session\". * &lt;/p&gt; * * &lt;p&gt; * For example, if you had an application named \"Application A\" that needed to keep * the sessions isolated from \"Application B\" you could set two different values for * the applications and they could function within the same Redis instance. * &lt;/p&gt; * * @return the unique namespace for keys */ String redisNamespace() default \"\"; /** * &lt;p&gt; * Sets the flush mode for the Redis sessions. The default is ON_SAVE which only * updates the backing Redis when * {@link SessionRepository#save(org.springframework.session.Session)} is invoked. In * a web environment this happens just before the HTTP response is committed. * &lt;/p&gt; * &lt;p&gt; * Setting the value to IMMEDIATE will ensure that the any updates to the Session are * immediately written to the Redis instance. * &lt;/p&gt; * * @return the {@link RedisFlushMode} to use * @since 1.1 */ RedisFlushMode redisFlushMode() default RedisFlushMode.ON_SAVE;} spring boot中的session redis配置就如此简单。 写个controller测试下 12345678910111213141516171819package com.fei.springboot.controller; import javax.servlet.http.HttpServletRequest; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.ResponseBody;@Controller@RequestMapping(\"/\") public class TestController { @RequestMapping(value=\"/getSessionId\") @ResponseBody public String getSessionId(HttpServletRequest request){ Object o = request.getSession().getAttribute(\"springboot\"); if(o == null){ o = \"spring boot 牛逼了!!!有端口\"+request.getLocalPort()+\"生成\"; request.getSession().setAttribute(\"springboot\", o); } return \"端口=\" + request.getLocalPort() + \" sessionId=\" + request.getSession().getId() +\"&lt;br/&gt;\"+o; } } 写个启动类 12345678910111213141516171819202122package com.fei.springboot;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.builder.SpringApplicationBuilder;import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer;import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer;import org.springframework.boot.web.support.SpringBootServletInitializer;import org.springframework.context.annotation.ComponentScan; @EnableAutoConfiguration@ComponentScan(basePackages={\"com.fei.springboot\"})@SpringBootApplicationpublic class Application extends SpringBootServletInitializer implements EmbeddedServletContainerCustomizer{ @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) { return application.sources(Application.class); } public static void main(String[] args) throws Exception { SpringApplication.run(Application.class, args); } public void customize(ConfigurableEmbeddedServletContainer configurableEmbeddedServletContainer) { // configurableEmbeddedServletContainer.setPort(9090); }} 修改application.yml中的server.port的端口为80,执行启动类，然后修改为8080，再次执行启动类。 浏览器测试http://127.0.0.1/getSessionId得得的结果是 端口=80 sessionId=3312d8db-a8dc-4df3-bc3a-628c311c0b4aspring boot 牛逼了!!!有端口80生成 http://127.0.0.1:8080/getSessionId得到的结果是 端口=8080 sessionId=3312d8db-a8dc-4df3-bc3a-628c311c0b4aspring boot 牛逼了!!!有端口80生成使用redis-client,查看 注意事项：如果启动时发现报错：ERR Unsupported CONFIG parameter: notify-keyspace-events; nested exception is redis.clients.jedis.exceptions.JedisDataException 这是因为redis服务器版本和jar包版本不一致造成的。 比如说，我这用的spring-session-data-redis版本是1.3.0，到maven 仓库中查询http://mvnrepository.com/ 发现redis是2.8.1，看了下我用的服务器是2.6的，我立刻下载了个最新版的3.x，我是本地测试的，用window的。更换redis服务器后，不再报错了。","link":"/2019/07/16/redis-share-session/"},{"title":"ribbon client load balance","text":"微服务之Ribbon的基本使用 上篇文章已经将咱们的电影服务和用户服务注册到我们实现的Eureka Server上了。但是现在有几个问题需要我们去考虑。第一个问题就是当有多个用户服务节点的时候，我们如何让电影服务调用用户服务实现负载均衡。第二个问题就是我们如何让电影服务调用用户服务的时候去使用Eureka Server里面的信息，现在还是通过硬编码去调用的。首先我们去解决第一个问题，大家应该都知道的一种实现负载均衡的方式，就是在用户服务前面去添加一个nginx，这是一种服务器端负载均衡的方式。 既然现在有服务器负载均衡，那就有另外一种实现方式就是客户端负载均衡,这里我就要引出Ribbon这个组件。Ribbon是什么？Ribbon是Netflix发布的开源项目，主要功能是提供客户端的软件负载均衡算法，将Netflix的中间层服务连接在一起。Ribbon客户端组件提供一系列完善的配置项如连接超时，重试等。简单的说，就是在配置文件中列出Load Balancer（简称LB）后面所有的机器，Ribbon会自动的帮助你基于某种规则（如简单轮询，随即连接等）去连接这些机器。我们也很容易使用Ribbon实现自定义的负载均衡算法。 Ribbon工作时分为两步,第一步先选择Eureka Server，它优先选择在同一个Zone且负载较少的Server。第二部再根据用户指定的策略，再从Server取到的服务注册列表中选择一个地址。其中Ribbon提供了多种策略，例如轮询round robin，随机Random，根据相应时间加权等。那么接下来我们就在项目中去引入Robbin。第一步引入Robbin，但是我们不用再去重新添加jar了，在之前的spring-cloud-starter-eureka中就有了。 接下来我们怎么去使用Ribbon。很简单一个注解@LoadBalanced就可以搞定。 为了避免和move服务冲突我们改一下端口，改成8010， 启动服务。 可以去看到时可以正常请求到用户服务的。那么我们怎么去实现负载均衡了，我们的再去启动一个用户服务进行测试。我们再去启动一个用户服务，端口改成7901 我们查看当前服务注册信息，可以看到现在有两个用户服务 请求电影服务我可以看到两个用户服务的控制台都会有日志。好了今天由于时间问题我就先写到这里，后面会持续分享，喜欢Spring cloud的朋友记得关注我，持续学习后面的知识，我的每篇文章都是连贯的，最好不要断开。我是磊哥，每天为你分享java知识。","link":"/2019/07/17/ribbon-client-load-balance/"},{"title":"ribbon loadbalanced","text":"由springcloud ribbon的 @LoadBalanced注解的使用理解 在使用springcloud ribbon客户端负载均衡的时候，可以给RestTemplate bean 加一个@LoadBalanced注解，就能让这个RestTemplate在请求时拥有客户端负载均衡的能力： 12345@Bean @LoadBalanced RestTemplate restTemplate() { return new RestTemplate(); } 是不是很神奇?打开@LoadBalanced的注解源码，并没有什么特殊的东东: 12345678910111213141516171819202122package org.springframework.cloud.client.loadbalancer; import org.springframework.beans.factory.annotation.Qualifier; import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Inherited;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target; /** * Annotation to mark a RestTemplate bean to be configured to use a LoadBalancerClient * @author Spencer Gibb */@Target({ ElementType.FIELD, ElementType.PARAMETER, ElementType.METHOD })@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@Qualifierpublic @interface LoadBalanced {} 唯一不同的地方就是多了一个@Qulifier注解. 搜索@LoadBalanced注解的使用地方，发现只有一处使用了,在LoadBalancerAutoConfiguration这个自动装配类中： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@LoadBalanced@Autowired(required = false)private List&lt;RestTemplate&gt; restTemplates = Collections.emptyList();@Beanpublic SmartInitializingSingleton loadBalancedRestTemplateInitializer( final List&lt;RestTemplateCustomizer&gt; customizers) { return new SmartInitializingSingleton() { @Override public void afterSingletonsInstantiated() { for (RestTemplate restTemplate : LoadBalancerAutoConfiguration.this.restTemplates) { for (RestTemplateCustomizer customizer : customizers) { customizer.customize(restTemplate); } } } };} @Autowired(required = false)private List&lt;LoadBalancerRequestTransformer&gt; transformers = Collections.emptyList();@Bean@ConditionalOnMissingBeanpublic LoadBalancerRequestFactory loadBalancerRequestFactory( LoadBalancerClient loadBalancerClient) { return new LoadBalancerRequestFactory(loadBalancerClient, transformers);} @Configuration@ConditionalOnMissingClass(\"org.springframework.retry.support.RetryTemplate\")static class LoadBalancerInterceptorConfig { @Bean public LoadBalancerInterceptor ribbonInterceptor( LoadBalancerClient loadBalancerClient, LoadBalancerRequestFactory requestFactory) { return new LoadBalancerInterceptor(loadBalancerClient, requestFactory); } @Bean @ConditionalOnMissingBean public RestTemplateCustomizer restTemplateCustomizer( final LoadBalancerInterceptor loadBalancerInterceptor) { return new RestTemplateCustomizer() { @Override public void customize(RestTemplate restTemplate) { List&lt;ClientHttpRequestInterceptor&gt; list = new ArrayList&lt;&gt;( restTemplate.getInterceptors()); list.add(loadBalancerInterceptor); restTemplate.setInterceptors(list); } }; }} 这段自动装配的代码的含义不难理解，就是利用了RestTempllate的拦截器，使用RestTemplateCustomizer对所有标注了@LoadBalanced的RestTemplate Bean添加了一个LoadBalancerInterceptor拦截器，而这个拦截器的作用就是对请求的URI进行转换获取到具体应该请求哪个服务实例ServiceInstance。 那么为什么 123@LoadBalanced@Autowired(required = false)private List&lt;RestTemplate&gt; restTemplates = Collections.emptyList(); 这个restTemplates能够将所有标注了@LoadBalanced的RestTemplate自动注入进来呢？这就要说说@Autowired注解和@Qualifier这两个注解了。大家日常使用很多都是用@Autowired来注入一个bean,其实@Autowired还可以注入List和Map,比如我定义两个Bean: 123456789@Bean(\"user1\")User user1() { return new User(\"1\", \"a\");} @Bean(\"user2\"))User user2() { return new User(\"2\", \"b\");} 然后我写一个Controller: 12345678910111213141516171819202122232425262728import com.example.demo.domain.User;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController; import java.util.Collections;import java.util.HashMap;import java.util.List;import java.util.Map; @RestControllerpublic class MyController { @Autowired(required = false) private List&lt;User&gt; users = Collections.emptyList(); @Autowired(required = false) private Map&lt;String,User&gt; userMap = new HashMap&lt;&gt;(); @RequestMapping(\"/list\") public Object listUsers() { return users; } @RequestMapping(\"/map\") public Object mapUsers() { return userMap; }} 在controller中通过: 12345@Autowired(required = false)private List&lt;User&gt; users = Collections.emptyList(); @Autowired(required = false)private Map&lt;String,User&gt; userMap = new HashMap&lt;&gt;(); 就可以自动将两个bean注入进来，当注入map的时候，map的key必须是String类型，然后bean name将作为map的key,本例，map中将有两个key分别为user1和user2,value分别为对应的User Bean实例。访问http://localhost:8080/map: 12345678910{ \"user1\": { \"id\": \"1\", \"name\": \"a\" }, \"user2\": { \"id\": \"2\", \"name\": \"b\" }} 访问http://localhost:8080/list: 12345678910[ { \"id\": \"1\", \"name\": \"a\" }, { \"id\": \"2\", \"name\": \"b\" }] 然后我们给user1和user2分别打上@Qualifier修饰符: 1234567891011@Bean(\"user1\") @Qualifier(\"valid\") User user1() { return new User(\"1\", \"a\"); } @Bean(\"user2\") @Qualifier(\"invalid\") User user2() { return new User(\"2\", \"b\"); } 然后将controller中的user list 和user map分别也打上@Qualifier修饰符: 12345678910111213141516171819202122232425262728293031import com.example.demo.domain.User;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController; import java.util.Collections;import java.util.HashMap;import java.util.List;import java.util.Map; @RestControllerpublic class MyController { @Autowired(required = false) @Qualifier(\"valid\") private List&lt;User&gt; users = Collections.emptyList(); @Autowired(required = false) @Qualifier(\"invalid\") private Map&lt;String,User&gt; userMap = new HashMap&lt;&gt;(); @RequestMapping(\"/list\") public Object listUsers() { return users; } @RequestMapping(\"/map\") public Object mapUsers() { return userMap; }} 那么所有标注了@Qualifier(“valid”)的user bean都会自动注入到List users中去(本例是user1)，所有标注了@Qualifier(“invalid”)的user bean都会自动注入到Map&lt;String,User&gt; userMap中去（本例是user2），我们再次访问上面两个url: 访问http://localhost:8080/list: 123456[ { \"id\": \"1\", \"name\": \"a\" }] 访问http://localhost:8080/map: 123456{ \"user2\": { \"id\": \"2\", \"name\": \"b\" }} 看到这里我们可以理解@LoadBalanced的用处了，其实就是一个修饰符，和@Qualifier一样，比如我们给user1打上@LoadBalanced: 12345678910@Bean(\"user1\")@LoadBalancedUser user1() { return new User(\"1\", \"a\");} @Bean(\"user2\")User user2() { return new User(\"2\", \"b\");} 然后controller中给List users打上@LoadBalanced注解: 123@Autowired(required = false) @LoadBalanced private List&lt;User&gt; users = Collections.emptyList(); 访问http://localhost:8080/list：[ { “id”: “1”, “name”: “a” }] 和@Qualifier注解效果一样，只有user1被注入进了List，user2没有修饰符，没有被注入进去。另外当spring容器中有多个相同类型的bean的时候，可以通过@Qualifier来进行区分，以便在注入的时候明确表明你要注入具体的哪个bean,消除歧义。","link":"/2019/07/17/ribbon-load-balance/"},{"title":"security 短信登录","text":"1[入门程序]https://blog.csdn.net/yuanlaijike/article/details/802492352[自动登录]https://blog.csdn.net/yuanlaijike/article/details/802498693[异常处理]https://blog.csdn.net/yuanlaijike/article/details/802503894[自定义表单登录]https://blog.csdn.net/yuanlaijike/article/details/802539225[权限控制]https://blog.csdn.net/yuanlaijike/article/details/803278806[登陆管理]https://blog.csdn.net/yuanlaijike/article/details/846387457[认证流程]https://blog.csdn.net/yuanlaijike/article/details/847036908[短信验证码登录]https://blog.csdn.net/yuanlaijike/article/details/861641609[解决 UserNotFoundException 不抛出问题]https://blog.csdn.net/yuanlaijike/article/details/9510455310[角色继承]https://blog.csdn.net/yuanlaijike/article/details/101565313[Spring Security OAuth2 开发指南中文版]https://blog.csdn.net/xqhys/article/details/87178824 [微信登录]https://blog.csdn.net/baidu_34389984/article/details/85778310https://blog.csdn.net/qq_35222232/article/details/83783830 ExceptionUsernameNotFoundException（用户不存在）DisabledException（用户已被禁用）BadCredentialsException（坏的凭据）LockedException（账户锁定）AccountExpiredException （账户过期）CredentialsExpiredException（证书过期） Alias Filter Class Namespace Element or Attribute CHANNEL_FILTER ChannelProcessingFilter http/intercept-url@requires-channel SECURITY_CONTEXT_FILTER SecurityContextPersistenceFilter http CONCURRENT_SESSION_FILTER ConcurrentSessionFilter session-management/concurrency-control HEADERS_FILTER HeaderWriterFilter http/headers CSRF_FILTER CsrfFilter http/csrf LOGOUT_FILTER LogoutFilter http/logout X509_FILTER X509AuthenticationFilter http/x509 PRE_AUTH_FILTER AbstractPreAuthenticatedProcessingFilter N/A CAS_FILTER CasAuthenticationFilter N/A FORM_LOGIN_FILTER UsernamePasswordAuthenticationFilter http/form-login BASIC_AUTH_FILTER BasicAuthenticationFilter http/http-basic SERVLET_API_SUPPORT_FILTER SecurityContextHolderAwareRequestFilter http/@servlet-api-provision JAAS_API_SUPPORT_FILTER JaasApiIntegrationFilter http/@jaas-api-provision REMEMBER_ME_FILTER RememberMeAuthenticationFilter http/remember-me ANONYMOUS_FILTER AnonymousAuthenticationFilter http/anonymous SESSION_MANAGEMENT_FILTER SessionManagementFilter session-management EXCEPTION_TRANSLATION_FILTER ExceptionTranslationFilter http FILTER_SECURITY_INTERCEPTOR FilterSecurityInterceptor http SWITCH_USER_FILTER SwitchUserFilter N/A","link":"/2020/03/12/security 短信登录/"},{"title":"rpm制作","text":"linux下RPM包制作 展开1.rpmbuildrpm是Redhat系linux系统的包管理器，使用rpmbuild工具可以制作rpm包。 2.rpmbuild的配置文件（1）rpmrc配置文件/usr/lib/rpm/rpmrc/usr/lib/rpm/redhat/rpmrc/etc/rpmrc~/.rpmrc（2）macro宏配置文件/usr/lib/rpm/macros/usr/lib/rpm/redhat/macros/etc/rpm/macros~/.rpmmacros 3.使用rpmbuild制作rpm包通用过程通用过程如下：（1）计划做什么rpm包，比如软件包或库等（2）收集源码包（3）如果需要打补丁，收集补丁文件（4）确定依赖关系包（5）开始动手制作RPM包A）设定好目录结构，我们在这些目录中制作我们的RPM包，我们需要下列目录 ： BUILD —— 源代码解压后的存放目录，以及后续执行的configure，make，make install命令都是在该目录下执行的 RPMS —— 制作完成后的RPM包存放目录，里面有与平台相关的子目录 SOURCES —— 收集的源材料，补丁的存放位置 SPECS —— SPEC文件存放目录 SRMPS —— 存放生成的SRMPS包的目录B）把源码包或补丁包放入SOURCES目录中C）创建spec文件，这是纲领文件，rpmbuild命令根据spec文件来制作rpm包D）使用rpmbuild命令制作rpm包（6）测试制作的RPM包（7）为RPM包签名备注：rpm制作中，最关键的步骤是编写spec文件。 4.rpm制作举例以制作tengine的安装包为例来说明：（1）设置制作rpm包的主目录在RedHat/Centos下制作rpm包的缺省目录是/usr/src/redhat. 当然我们可以自定义这个主目录，方法如下：新建~/.rpmmacros文件，增加如下内容：%_topdir /home/wahaha备注：表示制作rpm包的主目录是/home/wahaha，之前所说BUILD|RPMS|SOURCES等目录就是在_topdir代表的目录中。（2）在主目录中创建rpm打包需要用到的子目录cd /home/wahaha &amp;&amp; mkdir -pv {BUILD,BUILDROOT,RPMS,SOURCES,SPECS,SRPMS}（3）把收集的源码放到SOURCES下cp /tmp/tengine-1.4.2.tar.gz /home/wahaha/SOURCES备注：源文件或补丁文件都要放到SOURCES目录中（4）在SPECS下建立重要的spec文件cd /home/wahaha/SPECS &amp;&amp; vim tengine.spec备注：这个步骤非常重要，编写spec文件（5）用rpmbuild命令制作rpm包，rpmbuild命令会根据spec文件来生成rpm包 rpmbuild-ba 既生成src.rpm又生成二进制rpm-bs 只生成src的rpm-bb 只生二进制的rpm-bp 执行到pre段-bc 执行到build段-bi 执行到install段-bl 检测有文件没包含（6）测试生成的rpm包是否可以使用可以通过命令：rpm -ivh tengine-1.4.2-1.el6.x86_64.rpm ##测试安装rpm -e tengine ##测试卸载（7）为rpm包签名使用gpg生成秘钥对，然后使用rpm –addsign为rpm包签名，然后使用rpm –checksig命令即可 5.spec文件的结构spec文件一般包括如下几个部分：（1）introduction section该部分主要定义包的一些基本信息，比如程序名、版本号、发行号以及描述信息等。（2）prep section该部分是以宏%prep开始。prep section用来解压源码包到BUILD目录，并cd到解压后的目录。（3）build section该部分是以宏%build开始。build section是用来编译程序的，例如执行configure、make等命令。值得注意的是，如果源文件不需要编译，这个部分只写一个%build就可以了，其他的留空，既不用写configure，make等命令了。（4）install section该部分是以宏%install开始。install section用来将软件安装到临时目录的，即安装测试。后面rpm打包时会从这个临时安装目录中提取文件的。这个安装的临时目录是我们在introduction section定义的BuildRoot标签来指定的，或者rpmbuild有一个buildroot宏来定义的。（5）clean section该部分是以宏%clean开始。主要用来清除BUILD目录中的内容。（6）files section该部分是以宏%files开始。用于指定最终生成的rpm包中应该包含哪些文件（在BUILDROOT路径下找），files段中指明的文件一定要在BUILDROOT目录下存在，不然会报错哦（7）changelog section该部分是以宏%changelog开始。用来说明该软件的变更历史信息。备注：以上是spec主要的几个部分，其实在复杂的应用场景中，还有其他的section哦，慢慢了解吧 如下为tengine软件的spec文件举例： 0.define section #自定义宏段，这个不是必须的%define nginx_user nginx #这是我们自定义了一个宏，名字为nginx_user值为nginx，%{nginx_user}引用1.The introduction section #介绍区域段Name: tengine #名字为tar包的名字Version: 1.4.2 #版本号，一定要与tar包的一致哦Release: 1%{?dist} #释出号，也就是第几次制作rpmSummary: tengine from TaoBao #软件包简介，最好不要超过50字符Group: System Environment/Daemons #组名，可以通过less /usr/share/doc/rpm-4.8.0/GROUPS选择合适的组License: GPLv2 #许可，GPL还是BSD等URL: http://laoguang.blog.51cto.com #可以写一个网址Packager: Laoguang ibuler@qq.com #rpm包制作作者Vendor: TaoBao.com #rpm包所属组织Source: %{name}-%{version}.tar.gz #定义用到的source，也就是你收集的，可以用宏来表示，也可以直接写名字。如果有多个源文件的话，可以通过Source0、Source1、Source2等来指定patch0: a.patch #rpm包制作需要的补丁，如果有的话，需要写上。如有多个补丁包，可以通过patch0、patch1等来指明BuildRoot: %_topdir/BUILDROOT #这个是软件make install的测试安装目录，也就是测试中的根。我们可以自定义这个目录的位置BuildRequires: gcc,make #编译阶段需要依赖的包Requires: pcre,pcre-devel,openssl,chkconfig #使用rpm -ivh安装已经制作完成的rpm包时需要依赖的包%description #软件包描述，尽情的写吧It is a Nginx from Taobao. #描述内容 2.The Prep section 准备阶段,主要目的解压source并cd进去%prep #以%prep宏开始%setup -q #这个宏的作用静默模式解压并cd%patch0 -p1 #如果需要在这打补丁，依次写 3.The Build Section 编译制作阶段，主要目的就是编译%build #以%build宏开始./configure \\ #./configure 也可以用%configure来替换 –prefix=/usr \\ –sbin-path=/usr/sbin/nginx –conf-path=/etc/nginx/nginx.conf –error-log-path=/var/log/nginx/error.log –http-log-path=/var/log/nginx/access.log –pid-path=/var/run/nginx/nginx.pid –lock-path=/var/lock/nginx.lock –user=nginx –group=nginx –with-http_ssl_module –with-http_flv_module –with-http_stub_status_module –with-http_gzip_static_module –http-client-body-temp-path=/var/tmp/nginx/client/ –http-proxy-temp-path=/var/tmp/nginx/proxy/ –http-fastcgi-temp-path=/var/tmp/nginx/fcgi/ –http-uwsgi-temp-path=/var/tmp/nginx/uwsgi –http-scgi-temp-path=/var/tmp/nginx/scgi –with-pcremake %{?_smp_mflags} #make后面的意思是，如果就多处理器的话make时并行编译 4.Install section 安装阶段%install #以%install宏开始rm -rf %{buildroot} #先删除原来的安装的，如果你不是第一次安装的话make install DESTDIR=%{buildroot} #DESTDIR指定安装的目录，而不是真实的安装目录，所以在introduction section设置的BuildRoot就是在这个地方被引用的 4.1 scripts section #没必要可以不写%pre #rpm安装前执行的脚本if [ $1 == 1 ];then #$1==1 代表的是第一次安装，2代表是升级，0代表是卸载 /usr/sbin/useradd -r nginx 2&gt; /dev/null ##其实这个脚本写的不完整fi%post #安装后执行的脚本 %preun #卸载前执行的脚本if [ $1 == 0 ];then /usr/sbin/userdel -r nginx 2&gt; /dev/nullfi%postun #卸载后执行的脚本 5.clean section 清理段,删除buildroot%clean #以%clean宏开始rm -rf %{buildroot} 6.file section 要包含的文件%files #以%files宏开始%defattr (-,root,root,0755) #设定默认权限，如果下面没有指定权限，则继承默认/etc/ #下面的内容要根据你在%{rootbuild}下生成的来写/usr//var/ 7.chagelog section 改变日志段%changelog #以%changelog宏开始 Fri Dec 29 2012 laoguang ibuler@qq.com - 1.0.14-1 Initial version备注：关于spec文件编写的一些语法知识点（1）定义标签语法TagName: value, ##例如Name: nginx标签可以当做宏来引用，即可以使用%{TagName}或%TagName来引用（引用tag名称的时候不区分大小写）备注：BuildRoot可以通过%{buildroot}，也可以通过$RPM_BUILD_ROOT来引用。（2）自定义宏语法%define macro_name value然后可以通过%{macro_name }或%macro_name来引用。（3）spec文件中注释spec文件中使用#符号，来表示注释（4）rpm制作中涉及到的宏%{?dist} 这是一个宏，？表示有就加上，没有就不加。dist一般是系统的发行版本，比如el5，el6，el7等%{}是宏，像%{__rm}带两个下划线的也是宏，它代表着linux命令。 6.rpm制作和rpm包安装的过程原理（1）rpmbuild制作执行过程rpmbuild制作rpm包是以SPEC文件中的剧本来执行的，如下只是简单地说明下过程：-&gt;将SOURCES目录中源文件，解压到BUILD的目录中。例如BUILD/nginx-1.4/* -&gt;切到BUILD/nginx-1.4/目录执行configure，make等命令-&gt;执行make install将软件安装到BUILDROOT目录中（备注：使用rpmbuild制作包的过程中，会有安装的过程，只是说安装到一个临时的目录中）-&gt;file section字段指定BUILDROOT目录中哪些文件被归档到最终的rpm包中-&gt;打包完成后，就执行clean操作，会删除BUILD目录中内容（2）rpm -ivh安装rpm包的过程我们通过rpm -qpl *.rpm可以看到rpm包中的文件是有目录结构的，当我们执行rpm -ivh命令时，就会按照这个目录结构来建立，并将相应的文件install命令到对应的位置。另外rpm -ivh时，还会执行SPEC中的一些script section的命令哦，例如创建用户、增加开机启动等。（3）rpm -Uvh升级软件的过程通过rpm -Uvh升级软件时，会用新版本rpm包的文件替换到已经安装的系统文件。当然如果是新增的文件，直接install到对应的目录就好了。备注：使用rpm -ivh或rpm -Uvh都会更新rpm的数据库了。 7.rpm制作的最佳实践（1）要使用普通账户制作rpm包，因为在制作rpm包中，会执行spec文件中的一些shell命令，如果命令有问题会带来不确定的后果。（2）在Redhat/Centos 6中，可能需要在软件安装时，能够配置好initd控制脚本以及logrotate日志切割，那我们可以先制作好这些文件，然后作为Source引入到rpm包就可以了。同样地，在RedHat/Centos 7中，可能需要/lib/systemd/system/*.service配置文件，我们也可以先制作好，然后作为源文件引入到rpm包就好了。（3）有些软件需要用特定的账户执行，例如apache服务用apache账户来启动。那么我们在可以在脚本段中去定义这些操作就好了。（4）SRPM包中一般包含了源文件、SPEC文件等内容，我们可以下载一些开源软件的srpm包，做一些修改后再重新制作rpm包。当然我们可以学习SPEC的编写内容哦（5）使用rpm –showrc 或 rpmbuild –showrc命令，可以显示rpmbuild配置文件中定义的宏，灰常重要，例如通过rpm –showrc | grep _topdir就可以查看rpm制作的主目录。当我们编写spec文件中，就可以通过该命令来查看一些宏的具体含义。（6）有时我们需要对源文件做成多个rpm包，例如mysql就有mysql-%{version}.rpm、mysql-server-%{version}.rpm等，这个是可以在spec文件中控制的，已达到分拆rpm包的目的。（7）rpm包搜索网址http://rpmfind.nethttp://rpm.pbone.net/ linux制做RPM包制作rpm包 1.制作流程1.1 前期工作 1）创建打包用的目录rpmbuild/{BUILD,SPECS,RPMS, SOURCES,SRPMS} 建议使用普通用户，在用户家目录中创建 2）确定好制作的对象，是源码包编译打包还只是一些库文件打包 3）编写SPEC文件 4）开始制作 1.2 RPM制作过程 1）读取并解析 filename.spec 文件 2）运行 %prep 部分来将源代码解包到一个临时目录，并应用所有的补丁程序。 3）运行 %build 部分来编译代码。 4）运行 %install 部分将代码安装到构建机器的目录中。 5）读取 %files 部分的文件列表，收集文件并创建二进制和源 RPM 文件。 6）运行 %clean 部分来除去临时构建目录。 补充说明： BUILD目录：制作车间，二进制tar.gz包的解压后存放的位置，并在此目录进行编译安装 SPECS目录：存放SPEC文件 RPMS目录：存放制作好的rpm包 SOURCES目录：存放源代码 SRPMS目录：存放编译好的二进制rpm包 BUILDROOT目录：此目录是在BUILD目录中执行完make install之后生成的目录，里面存放的是编译安装好的文件，他是./configure中—prefix指定path的根目录1.3制作工具：rpmbuild 制作过程的几个状态 rpmbuild -bp 执行到%prep rpmbuild -bc 执行到%build中的config rpmbuild -bi 执行至%build中的install rpmbuild -ba 编译后做成rpm包和src.rpm包rpmbuild -bs 仅制作src.rpm包 rpmbuild -bb 仅制作rpm包 2.SPEC文件2.1 spec文件参数：自定义软件包基本参数： Name 软件包名字 Version 软件包版本 Release 软件包修订号 Summary 软件包简单描述 Group 软件包所属组。必须是系统定义好的组 License 软件授权方式，通常就是GPL Vendor 软件包发型厂商 Packager 软件包打包者 URL 软件包的url Source 定义打包所需的源码包，可以定义多个，后面使用%{SOURCE}调用 Patch 定义补丁文件，后面可以使用%{Patch}调用 BuildRoot 定义打包时的工作目录 BuildRequires 定义打包时依赖的软件包 Requires 定义安装时的依赖包，形式为Package name 或者 Package &gt;= version Prefix %{_prefix}| %{_sysconfdir} ： %{_prefix} 这个主要是为了解决今后安装rpm包时，并不一定把软件安装到rpm中打包的目录的情况。这样，必须在这里定义该标识，并在编写%install脚本的时候引用，才能实现rpm安装时重新指定位置的功能 %{_sysconfdir} 这个原因和上面的一样，但由于%{_prefix}指/usr，而对于其他的文件，例如/etc下的配置文件，则需要用%{_sysconfdir}标识 %package 定义一个子包 %description 详细描述信息 自定义打包参数; %prep 预处理段，默认是解压源码包，可以自定义shell命令和调用RPM宏命令 %post rpm安装后执行的命令，可以自定义shell命令和调用RPM宏命令 %preun rpm卸载前执行的命令，可以自定义shell命令和调用RPM宏命令 %postun rpm卸载后执行的命令，可以自定义shell命令和调用RPM宏命令 %patch 打补丁阶段 %build 编译安装段，此段包含./configure和 make 安装阶段 %install 安装阶段，会把编译好的二进制文件安装到BUILDROOT为根的目录下 %files 文件段，定义软件打包时的文件，分为三类–说明文档（doc），配置文件（config）及执行程序，还可定义文件存取权限，拥有者及组别。其路径为相对路径 %changelog 定义软件包修改的日志 2.2补充：Group：软件包所属类别，具体类别有：Amusements/Games （娱乐/游戏）Amusements/Graphics（娱乐/图形）Applications/Archiving （应用/文档）Applications/Communications（应用/通讯）Applications/Databases （应用/数据库）Applications/Editors （应用/编辑器）Applications/Emulators （应用/仿真器）Applications/Engineering （应用/工程）Applications/File （应用/文件）Applications/Internet （应用/因特网）Applications/Multimedia（应用/多媒体）Applications/Productivity （应用/产品）Applications/Publishing（应用/印刷）Applications/System（应用/系统）Applications/Text （应用/文本）Development/Debuggers （开发/调试器）Development/Languages （开发/语言）Development/Libraries （开发/函数库）Development/System （开发/系统）Development/Tools （开发/工具）Documentation （文档）System Environment/Base（系统环境/基础）System Environment/Daemons （系统环境/守护）System Environment/Kernel （系统环境/内核）System Environment/Libraries （系统环境/函数库）System Environment/Shells （系统环境/接口）User Interface/Desktops（用户界面/桌面）User Interface/X （用户界面/X窗口）User Interface/X Hardware Support （用户界面/X硬件支持） %setup 的用法 %setup 不加任何选项，仅仅打开源码包 %setup -n newdir 将软件包解压至新目录（重命名解压的包），默认 %setup -c 解压缩之前先产生目录。%setup -b num 将第num个source文件解压缩。 %setup -T 不使用default的解压缩操作。 %setup -T -b 0 将第0个源代码文件解压缩。 %setup -c -n newdir 指定目录名称newdir，并在此目录产生rpm套件。 %setup -q 解压不输出信息 %Patch用法 先使用Patch{n}定义补丁包，然后使用%patch{n}或者%{patch{n}}来调用打补丁补丁号命名规则 0-9 Makefile、configure 等的补丁10-39 指定功能或包含他的文件的补丁 40-59 配置文件的补丁 60-79 字体或字符补丁 80-99 通过 xgettexize 得到的目录情况的补丁 100- 其他补丁 %patch 最简单的补丁方式，自动指定patch level。 %patch 0 使用第0个补丁文件，相当于%patch -p 0。 %patch -s 不显示打补丁时的信息。 %patch -T 将所有打补丁时产生的输出文件删除 %patch -b name 在打补丁之前，将源文件加入name，缺省为.org %file用法 %defattr (-,root,root) 指定包装文件的属性，分别是(mode,owner,group)，-表示默认值，对文本文件是0644，可执行文件是0755 %attr(600,work,work) 指定特定的文件目录权限fattr (-,root,root) 本段是文件段，用于定义构成软件包的文件列表，那些文件或目录会放入rpm中，分为三类-说明文档（doc），配置文件（config）及执行程序，还可定义文件存取权限，拥有者及组别。 这里会在虚拟根目录下进行，千万不要写绝对路径，而应用宏或变量表示相对路径。 ※特别需要注意的是：%install部分使用的是绝对路径，而%file部分使用则是相对路径，虽然其描述的是同一个地方。千万不要写错。 %files -f %{name}.lang tui file1 #文件中也可以包含通配符，如* file2 directory #所有文件都放在directory目录下 %dir /etc/xtoolwait #仅是一个空目录/etc/xtoolwait打进包里 %doc 表示这是文档文件，因此如安装时使用–excludedocs将不安装该文件， %doc /usr/X11R6/man/man1/xtoolwait.* #安装该文档 %doc README NEWS #安装这些文档到/usr/share/doc/%{name}-%{version} 或者 /usr/doc或者 %docdir #定义说明文档的目录，例如/root，在这一语句后，所有以/root开头的行都被定义为说明文件。 %config /etc/yp.conf #标志该文件是一个配置文件，升级过程中，RPM会有如下动作。 %config(missisgok) /etc/yp.conf 此配置文件可以丢失，即使丢失了，RPM在卸载软件包时也不认为这是一个错误，并不报错。一般用于那些软件包安装后建立的符号链接文件， /etc/rc.d/rc5.d/S55named文件，此类文件在软件包卸载后可能需要删除，所以丢失了也不要紧。 %config(noreplace) /etc/yp.conf #该配置文件不会覆盖已存在文件(RPM包中文件会以.rpmnew存在于系统，卸载时系统中的该配置文件会以.rpmsave保存下来，如果没有这个选项，安装时RPM包中文件会以.rpmorig存在于系统 ) 覆盖已存在文件(没被修改)，创建新的文件加上扩展后缀.rpmnew(被修改) %{_bindir}/* %config /etc/aa.conf %ghost /etc/yp.conf #该文件不应该包含在包中,一般是日志文件，其文件属性很重要，但是文件内容不重要，用了这个选项后，仅将其文件属性加入包中。 %attr(mode, user, group) filename #控制文件的权限如%attr(0644,root,root) /etc/yp.conf 如果你不想指定值，可以用- %config %attr(-,root,root) filename #设定文件类型和权限 fattr(-,root,root) #设置文件的默认权限,-表示默认值，对文本文件是0644，可执行文件是0755 %lang(en) %{_datadir}/locale/en/LC_MESSAGES/tcsh* #用特定的语言标志文件 %verify(owner group size) filename #只测试owner,group,size，默认测试所有 %verify(not owner) filename #不测试owner，测试其他的属性 所有的认证如下： group： 认证文件的组 maj： 认证文件的主设备号 md5： 认证文件的MD5 min： 认证文件的辅设备号 mode： 认证文件的权限 mtime： 认证文件最后修改时间 owner： 认证文件的所有者 size： 认证文件的大小 symlink：认证符号连接 如果描述为目录，表示目录中出%exclude外的所有文件。 %files fattr(-,root,root) %doc %{_bindir}/* %{_libdir}/liba* %{_datadir}/file %{_infodir}/* %{_mandir}/man[15]/* %{_includedir} %exclude %{_libdir}/debug （%exclude 列出不想打包到rpm中的文件。※小心，如果%exclude指定的文件不存在，也会出错的。） 如果把 %files fattr(-,root,root) %{_bindir} 写成 %files fattr(-,root,root) /usr/bin 则打包的会是根目录下的/usr/bin中所有的文件。 %files libs fattr(-,root,root) %{_libdir}/so. %files devel �fattr(-,root,root) %{_includedir}/* install的用法 -b：为每个已存在的目的地文件进行备份； -d：创建目录，类似mkdir -p-D：创建目的地前的所有目录，然后将来源复制到目的地。复制文件 -g：自行设置所属的组； -m：自行设置权限，而不是默认的rwxr-xr-x -o：自行设置所有者 -p：以来源文件的修改时间作为相应的目的地的文件属性 %pre %post %pretun %postun 用法 rpm提供了一种信号机制：不同的操作会返回不同的信息，并放到默认变量$1中 0代表卸载、1代表安装、2代表升级 if [“$1” = “0” ] ;then comondfi 用于判断rpm的动作 2.3 典型的spec文件案例： 3.技巧：1）如果要避免生成debuginfo包：这个是默认会生成的rpm包。则可以使用下面的命令 echo ‘%debug_package %{nil}’ &gt;&gt; ~/.rpmmacros 2）配置 RPM 在构建时使用新的目录结构，而不是默认的目录结构： echo “%_topdir $HOME/rpmbuild” &gt; ~/.rpmmacros 4.使用FPM制作RPM包1）安装： yum -y install ruby rubygems ruby-devel 安装ruby 和gem gem install fpm 安装fpm工具2）准备编译安装好的源码包 /usr/local/libiconv3）打包： fpm -f -s dir -t rpm -n beyond-libiconv --epoch=0 -v &apos;1.14&apos; -C /usr/local --iteration 1.el6 ./libiconv-1.14参数解释 -f 强制输出，如果文件已存在，将会覆盖源文件 -s 指定源文件为目录 dir -t 指定制作的包类型（rpm，deb solaris etc） -n 指定制作的包名 – epoch 指定时间戳 -v 指定软件版本 -C 指定软件安装的目录 –iteration 指定软件的适用平台 ./libiconv-1.14 本次打包的文件 附加参数： -e 可以在打包之前编译 spec文件 -d 指定依赖的软件包 用法 –d ‘Package’ 或 –d ‘Package &gt; version’ –description 软件包描述 -p 生成的package文件输出位置 –url 说明软件包的url –post-install ：软件包安装完成之后所要运行的脚本；和”–after-install” 意思一样 –pre-install ：软件包安装完成之前所要运行的脚本；和”–before-install” 意思一样 –post-uninstall ：软件包卸载完成之后所要运行的脚本；和”–after-remove”意思一样 –pre-uninstall：软件包卸载完成之前所要运行的脚本；和”–before-remove”意思一样 fpm打包php案例： fpm -f -s dir -t rpm -n beyond-php –epoch=0 -v ‘5.2.14’ -C /usr/local/ -p ./ –iteration 1.el6 -d ‘beyond-libiconv’ -d ‘beyond-libmcrypt’ -d ‘beyond-mcrypt’ -d ‘beyond-mhash’ -d ‘libxml2’ -d ‘libxml2-devel’ -d ‘zlib’ -d ‘zlib-devel’ -d ‘libpng’ -d ‘libpng-devel’ -d ‘freetype’ -d ‘freetype-devel’ -d ‘autoconf’ -d ‘gd’ -d ‘gd-devel’ -d ‘libjpeg’ -d ‘libjpeg-devel’ -d ‘curl’ -d ‘curl-devel’ -d ‘mysql-devel’ -d ‘openssl’ -d ‘openssl-devel’ -d ‘openldap-devel’ -d ‘libtool-ltdl’ -d ‘libtool-ltdl-devel’ –url http://sa.beyond.com/source/php-5.2.14.tar.gz –license GPL –post-install ./preinstall.sh /usr/local/php","link":"/2020/03/18/rpm制作/"},{"title":"security","text":"手把手带你入门 Spring Security！ Spring Security 是 Spring 家族中的一个安全管理框架，实际上，在 Spring Boot 出现之前，Spring Security 就已经发展了多年了，但是使用的并不多，安全管理这个领域，一直是 Shiro 的天下。 相对于 Shiro，在 SSM/SSH 中整合 Spring Security 都是比较麻烦的操作，所以，Spring Security 虽然功能比 Shiro 强大，但是使用反而没有 Shiro 多（Shiro 虽然功能没有 Spring Security 多，但是对于大部分项目而言，Shiro 也够用了）。 自从有了 Spring Boot 之后，Spring Boot 对于 Spring Security 提供了 自动化配置方案，可以零配置使用 Spring Security。 因此，一般来说，常见的安全管理技术栈的组合是这样的： SSM + ShiroSpring Boot/Spring Cloud + Spring Security注意，这只是一个推荐的组合而已，如果单纯从技术上来说，无论怎么组合，都是可以运行的。 我们来看下具体使用。 1.项目创建在 Spring Boot 中使用 Spring Security 非常容易，引入依赖即可： pom.xml 中的 Spring Security 依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 只要加入依赖，项目的所有接口都会被自动保护起来。我们创建一个 HelloController: 1234567@RestControllerpublic class HelloController { @GetMapping(\"/hello\") public String hello() { return \"hello\"; }} 访问 /hello ，需要登录之后才能访问。 当用户从浏览器发送请求访问 /hello 接口时，服务端会返回 302 响应码，让客户端重定向到 /login 页面，用户在 /login 页面登录，登陆成功之后，就会自动跳转到 /hello 接口。 另外，也可以使用 POSTMAN 来发送请求，使用 POSTMAN 发送请求时，可以将用户信息放在请求头中（这样可以避免重定向到登录页面）：通过以上两种不同的登录方式，可以看出，Spring Security 支持两种不同的认证方式： 可以通过 form 表单来认证可以通过 HttpBasic 来认证 3.用户名配置默认情况下，登录的用户名是 user ，密码则是项目启动时随机生成的字符串，可以从启动的控制台日志中看到默认密码： 这个随机生成的密码，每次启动时都会变。对登录的用户名/密码进行配置，有三种不同的方式： 在 application.properties 中进行配置 通过 Java 代码配置在内存中 通过 Java 从数据库中加载前两种比较简单，第三种代码量略大，本文就先来看看前两种，第三种后面再单独写文章介绍，也可以参考我的微人事项目。 3.1 配置文件配置用户名/密码可以直接在 application.properties 文件中配置用户的基本信息： 12spring.security.user.name=javaboyspring.security.user.password=123 3.2 Java 配置用户名/密码也可以在 Java 代码中配置用户名密码，首先需要我们创建一个 Spring Security 的配置类，集成自 WebSecurityConfigurerAdapter 类，如下： 123456789101112131415@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { //下面这两行配置表示在内存中配置了两个用户 auth.inMemoryAuthentication() .withUser(\"javaboy\").roles(\"admin\").password(\"$2a$10$OR3VSksVAmCzc.7WeaRPR.t0wyCsIj24k0Bne8iKWV1o.V9wsP8Xe\") .and() .withUser(\"lisi\").roles(\"user\").password(\"$2a$10$p1H8iWa8I4.CA.7Z8bwLjes91ZpY.rYREGHQEInNtAp4NzL6PLKxi\"); } @Bean PasswordEncoder passwordEncoder() { return new BCryptPasswordEncoder(); }} 这里我们在 configure 方法中配置了两个用户，用户的密码都是加密之后的字符串(明文是 123)，从 Spring5 开始，强制要求密码要加密，如果非不想加密，可以使用一个过期的 PasswordEncoder 的实例 NoOpPasswordEncoder，但是不建议这么做，毕竟不安全。 Spring Security 中提供了 BCryptPasswordEncoder 密码编码工具，可以非常方便的实现密码的加密加盐，相同明文加密出来的结果总是不同，这样就不需要用户去额外保存盐的字段了，这一点比 Shiro 要方便很多。 4.登录配置对于登录接口，登录成功后的响应，登录失败后的响应，我们都可以在 WebSecurityConfigurerAdapter 的实现类中进行配置。例如下面这样： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Autowired VerifyCodeFilter verifyCodeFilter; @Override protected void configure(HttpSecurity http) throws Exception { http.addFilterBefore(verifyCodeFilter, UsernamePasswordAuthenticationFilter.class); http .authorizeRequests()//开启登录配置 .antMatchers(\"/hello\").hasRole(\"admin\")//表示访问 /hello 这个接口，需要具备 admin 这个角色 .anyRequest().authenticated()//表示剩余的其他接口，登录之后就能访问 .and() .formLogin() //定义登录页面，未登录时，访问一个需要登录之后才能访问的接口，会自动跳转到该页面 .loginPage(\"/login_p\") //登录处理接口 .loginProcessingUrl(\"/doLogin\") //定义登录时，用户名的 key，默认为 username .usernameParameter(\"uname\") //定义登录时，用户密码的 key，默认为 password .passwordParameter(\"passwd\") //登录成功的处理器 .successHandler(new AuthenticationSuccessHandler() { @Override public void onAuthenticationSuccess(HttpServletRequest req, HttpServletResponse resp, Authentication authentication) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(\"success\"); out.flush(); } }) .failureHandler(new AuthenticationFailureHandler() { @Override public void onAuthenticationFailure(HttpServletRequest req, HttpServletResponse resp, AuthenticationException exception) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(\"fail\"); out.flush(); } }) .permitAll()//和表单登录相关的接口统统都直接通过 .and() .logout() .logoutUrl(\"/logout\") .logoutSuccessHandler(new LogoutSuccessHandler() { @Override public void onLogoutSuccess(HttpServletRequest req, HttpServletResponse resp, Authentication authentication) throws IOException, ServletException { resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(\"logout success\"); out.flush(); } }) .permitAll() .and() .httpBasic() .and() .csrf().disable(); }} 我们可以在 successHandler 方法中，配置登录成功的回调，如果是前后端分离开发的话，登录成功后返回 JSON 即可，同理，failureHandler 方法中配置登录失败的回调，logoutSuccessHandler 中则配置注销成功的回调。 5.忽略拦截如果某一个请求地址不需要拦截的话，有两种方式实现： 设置该地址匿名访问直接过滤掉该地址，即该地址不走 Spring Security 过滤器链推荐使用第二种方案，配置如下： 1234567@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter { @Override public void configure(WebSecurity web) throws Exception { web.ignoring().antMatchers(\"/vercode\"); }} Spring Security 另外一个强大之处就是它可以结合 OAuth2 ，玩出更多的花样出来，这些我们在后面的文章中再和大家细细介绍。","link":"/2020/03/02/security/"},{"title":"serialVersionUID idea 生成","text":"1.进入到设置2.设置之后，选中对应的类名，然后按 alt+enter 快捷键 的情况如下所示","link":"/2019/07/25/serialVersionUID-idea-生成/"},{"title":"security用户授权","text":"https://segmentfault.com/a/1190000018369146 Spring Security 实现用户授权spring-security授权安全发布于 2019-03-03 约 18 分钟引言上一次，使用Spring Security与Angular实现了用户认证。Spring Security and Angular 实现用户认证 本次，我们通过Spring Security的授权机制，实现用户授权。 实现十分简单，大家认真听，都能听得懂。 实现权限设计前台实现了菜单的权限控制，但后台接口还没进行保护，只要用户登录成功，什么接口都可以调用。 我们希望实现：用户有什么菜单的权限，只能访问后台对应该菜单的接口。 比如，用户有计算机组管理的菜单，就可以访问计算机组相关的增删改查接口，但是其他的接口都不允许访问。 Spring Security的设计依据Spring Security的设计，用户对应角色，角色对应后台接口。这是没什么问题的。 clipboard.png 示例 某接口添加@Secured注解，内部添加权限表达式。 12345@GetMapping@Secured(\"ROLE_ADMIN\")public List&lt;Host&gt; getAll() { return hostService.getAll();} 然后再为用户创建Spring Security中的角色。 这里我们为用户添加ROLE_ADMIN的角色授权，与getAll方法上的@Secured(“ROLE_ADMIN”)注解中的参数一致，表示该用户有权限访问该方法，这就是授权。 12345678910private UserDetails createUser(User user) { logger.debug(\"初始化授权列表\"); List&lt;SimpleGrantedAuthority&gt; authorities = new ArrayList&lt;&gt;(); logger.debug(\"角色授权\"); authorities.add(new SimpleGrantedAuthority(\"ROLE_ADMIN\")); logger.debug(\"构建用户\"); return new org.springframework.security.core.userdetails.User(user.getUsername(), user.getPassword(), authorities);} 不足作为一款优秀的安全框架而言，Spring Security这样设计是没有任何问题的，我们只需要简简单单的几行代码就能实现接口的授权管理。 但是却不符合我们的要求。 我们要求，在我们的系统中，用户对应多角色。 但是我们的角色是要求可以进行动态配置的，今天有一个系统管理员的角色，明天可能又加一个教师的角色。 在用户授权这方面，是可以实现动态配置的，因为用户的权限列表是一个List，我可以从数据库查当前用户的角色，然后add进去。 12345678910private UserDetails createUser(User user) { logger.debug(\"初始化授权列表\"); List&lt;SimpleGrantedAuthority&gt; authorities = new ArrayList&lt;&gt;(); logger.debug(\"角色授权\"); authorities.add(new SimpleGrantedAuthority(\"ROLE_ADMIN\")); logger.debug(\"构建用户\"); return new org.springframework.security.core.userdetails.User(user.getUsername(), user.getPassword(), authorities);} 但是在接口级别，就无法实现动态配置了。大家想想，注解里，要求的参数必须是常量，就是我们想动态配置，也实现不了啊？ 12345@GetMapping@Secured(\"ROLE_ADMIN\")public List&lt;Host&gt; getAll() { return hostService.getAll();} 所以，我们总结，因为注解配置的限制，所以在Spring Security中角色是静态的。 重新设计我们的角色是动态的，而Spring Security中的角色是静态的，所以不能将我们的角色直接映射到Spring Security中的角色，要映射也得拿一个我们系统中静态的对象与之对应。 角色是动态的，这个不行了。但是我们的菜单是静态的啊。 功能模块是我们开发的，菜单就这么固定的几个，用户管理、角色管理、系统设置啥的，在我们开发期间就已经固定下来了，我们是不是可以使用菜单结合Spring Security进行授权呢？ 认真看这张图，看懂了这张图，你应该就明白了我的设计思想。 角色是动态的，我不用它授权，我使用静态的菜单进行授权。 静态的菜单对应Spring Security中静态的角色，角色再对应后台接口，如此设计，就实现了我们的设想：用户拥有哪个菜单的权限，就只拥有被该菜单调用的相应接口权限。 编码设计好了，一起来写代码吧。 授权注解选择Spring Security中有多种授权注解，个人经过对比之后选择@Secured注解，因为我觉得这个注解配置项更容易被人理解。 12345678public @interface Secured { /** * Returns the list of security configuration attributes (e.g. ROLE_USER, ROLE_ADMIN). * * @return String[] The secure method attributes */ public String[]value();} 直接写一个角色的字符串数组传进去即可。 @Secured(“ROLE_ADMIN”) // 需要拥有ROLE_ADMIN角色才可访问@Secured({“ROLE_ADMIN”, “ROLE_TEACHER”}) // 用户拥有ROLE_ADMIN、ROLE_TEACHER二者之一即可访问 ** 注意：这里的字符串一定是以ROLE_开头，Spring Security才把它当成角色的配置，否则无效 ** 启用@Secured注解默认的Spring Security是不进行授权注解拦截的，添加注解@EnableGlobalMethodSecurity以启用@Secured注解的全局方法拦截。 @EnableGlobalMethodSecurity(securedEnabled = true) // 启用全局方法安全，采用@Secured方式 菜单角色映射在菜单中新建一个字段securityRoleName来声明我们的系统菜单对应着哪个Spring Security角色。 // 该菜单在Spring Security环境下的角色名称@Column(nullable = false)private String securityRoleName; 建一个类，用于存放所有Spring Security角色的配置信息，供全局调用。 这里不能用枚举，@Secured注解中要求必须是String数组，如果是枚举，需要通过YunzhiSecurityRoleEnum.ROLE_MAIN.name()格式获取字符串信息，但很遗憾，注解中要求必须是常量。 还记得上次自定义HTTP状态码的时候，吃了枚举类无法扩展的亏，以后再也不用枚举了。就算用枚举，也会设计一个接口，枚举实现该接口，不用枚举声明方法的参数类型，而使用接口声明，方便扩展。 12345678910111213141516171819202122package club.yunzhi.huasoft.security;/** * @author zhangxishuo on 2019-03-02 * Yunzhi Security 角色 * 该角色对应菜单 */public class YunzhiSecurityRole { public static final String ROLE_MAIN = \"ROLE_MAIN\"; public static final String ROLE_HOST = \"ROLE_HOST\"; public static final String ROLE_GROUP = \"ROLE_GROUP\"; public static final String ROLE_USER = \"ROLE_USER\"; public static final String ROLE_ROLE = \"ROLE_ROLE\"; public static final String ROLE_SETTING = \"ROLE_SETTING\";} 示例 12345@GetMapping@Secured({YunzhiSecurityRole.ROLE_HOST, YunzhiSecurityRole.ROLE_GROUP})public List&lt;Host&gt; getAll() { return hostService.getAll();} 用户授权代码体现授权思路：遍历当前用户的菜单，根据菜单中对应的Security角色名进行授权。 123456789101112131415private UserDetails createUser(User user) { logger.debug(\"获取用户的所有授权菜单\"); Set&lt;WebAppMenu&gt; menus = webAppMenuService.getAllAuthMenuByUser(user); logger.debug(\"初始化授权列表\"); List&lt;SimpleGrantedAuthority&gt; authorities = new ArrayList&lt;&gt;(); logger.debug(\"遍历授权菜单，进行角色授权\"); for (WebAppMenu menu : menus) { authorities.add(new SimpleGrantedAuthority(menu.getSecurityRoleName())); } logger.debug(\"构建用户\"); return new org.springframework.security.core.userdetails.User(user.getUsername(), user.getPassword(), authorities);} 注：这里遇到了Hibernate惰性加载引起的错误，启用事务防止Hibernate关闭Session，深层原理目前还在研究。 单元测试单元测试很简单，供写相同功能的人参考。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596@Testpublic void authTest() throws Exception { logger.debug(\"获取基础菜单\"); WebAppMenu hostMenu = webAppMenuRepository.findByRoute(\"/host\"); WebAppMenu groupMenu = webAppMenuRepository.findByRoute(\"/group\"); WebAppMenu settingMenu = webAppMenuRepository.findByRoute(\"/setting\"); logger.debug(\"构造角色\"); List&lt;Role&gt; roleList = new ArrayList&lt;&gt;(); Role roleHost = new Role(); roleHost.setWebAppMenuList(Collections.singletonList(hostMenu)); roleList.add(roleHost); Role roleGroup = new Role(); roleGroup.setWebAppMenuList(Collections.singletonList(groupMenu)); roleList.add(roleGroup); Role roleSetting = new Role(); roleSetting.setWebAppMenuList(Collections.singletonList(settingMenu)); roleList.add(roleSetting); logger.debug(\"保存角色\"); roleRepository.saveAll(roleList); logger.debug(\"构造用户\"); User user = userService.getOneUnSavedUser(); logger.debug(\"获取用户名和密码\"); String username = user.getUsername(); String password = user.getPassword(); logger.debug(\"保存用户\"); userRepository.save(user); logger.debug(\"用户登录\"); String token = this.loginWithUsernameAndPassword(username, password); logger.debug(\"无授权用户访问host，断言403\"); this.mockMvc.perform(MockMvcRequestBuilders.get(HOST_URL) .header(TOKEN_KEY, token)) .andExpect(status().isForbidden()); logger.debug(\"用户授权Host菜单\"); user.getRoleList().clear(); user.getRoleList().add(roleHost); userRepository.save(user); logger.debug(\"重新登录, 重新授权\"); token = this.loginWithUsernameAndPassword(username, password); logger.debug(\"授权Host用户访问，断言200\"); this.mockMvc.perform(MockMvcRequestBuilders.get(HOST_URL) .header(TOKEN_KEY, token)) .andExpect(status().isOk()); logger.debug(\"用户授权Group菜单\"); user.getRoleList().clear(); user.getRoleList().add(roleGroup); userRepository.save(user); logger.debug(\"重新登录, 重新授权\"); token = this.loginWithUsernameAndPassword(username, password); logger.debug(\"授权Group用户访问，断言200\"); this.mockMvc.perform(MockMvcRequestBuilders.get(HOST_URL) .header(TOKEN_KEY, token)) .andExpect(status().isOk()); logger.debug(\"用户授权Setting菜单\"); user.getRoleList().clear(); user.getRoleList().add(roleSetting); userRepository.save(user); logger.debug(\"重新登录, 重新授权\"); token = this.loginWithUsernameAndPassword(username, password); logger.debug(\"授权Setting用户访问，断言403\"); this.mockMvc.perform(MockMvcRequestBuilders.get(HOST_URL) .header(TOKEN_KEY, token)) .andExpect(status().isForbidden());}private String loginWithUsernameAndPassword(String username, String password) throws Exception { logger.debug(\"用户登录\"); byte[] encodedBytes = Base64.encodeBase64((username + \":\" + password).getBytes()); MvcResult mvcResult = this.mockMvc.perform(MockMvcRequestBuilders.get(LOGIN_URL) .header(\"Authorization\", \"Basic \" + new String(encodedBytes))) .andExpect(status().isOk()) .andReturn(); logger.debug(\"从返回体中获取token\"); String json = mvcResult.getResponse().getContentAsString(); JSONObject jsonObject = JSON.parseObject(json); return jsonObject.getString(\"token\");} 总结感谢开源社区，感谢Spring Security。五行代码(不算注释)，一个注解。就解决了一直以来困扰我们的权限问题。","link":"/2020/03/04/security用户授权/"},{"title":"servlet开发","text":"javaweb学习总结(五)——Servlet开发(一)一、Servlet简介 Servlet是sun公司提供的一门用于开发动态web资源的技术。 Sun公司在其API中提供了一个servlet接口，用户若想用发一个动态web资源(即开发一个Java程序向浏览器输出数据)，需要完成以下2个步骤： 1、编写一个Java类，实现servlet接口。 2、把开发好的Java类部署到web服务器中。 按照一种约定俗成的称呼习惯，通常我们也把实现了servlet接口的java程序，称之为Servlet 二、Servlet的运行过程Servlet程序是由WEB服务器调用，web服务器收到客户端的Servlet访问请求后： ①Web服务器首先检查是否已经装载并创建了该Servlet的实例对象。如果是，则直接执行第④步，否则，执行第②步。 ②装载并创建该Servlet的一个实例对象。 ③调用Servlet实例对象的init()方法。 ④创建一个用于封装HTTP请求消息的HttpServletRequest对象和一个代表HTTP响应消息的HttpServletResponse对象，然后调用Servlet的service()方法并将请求和响应对象作为参数传递进去。 ⑤WEB应用程序被停止或重新启动之前，Servlet引擎将卸载Servlet，并在卸载之前调用Servlet的destroy()方法。 三、Servlet调用图 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package gacl.servlet.study;import java.io.IOException;import java.io.PrintWriter;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;public class ServletDemo1 extends HttpServlet { /** * The doGet method of the servlet. &lt;br&gt; * * This method is called when a form has its tag value method equals to get. * * @param request the request send by the client to the server * @param response the response send by the server to the client * @throws ServletException if an error occurred * @throws IOException if an error occurred */ public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.setContentType(\"text/html\"); PrintWriter out = response.getWriter(); out.println(\"&lt;!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\"&gt;\"); out.println(\"&lt;HTML&gt;\"); out.println(\" &lt;HEAD&gt;&lt;TITLE&gt;A Servlet&lt;/TITLE&gt;&lt;/HEAD&gt;\"); out.println(\" &lt;BODY&gt;\"); out.print(\" This is \"); out.print(this.getClass()); out.println(\", using the GET method\"); out.println(\" &lt;/BODY&gt;\"); out.println(\"&lt;/HTML&gt;\"); out.flush(); out.close(); } /** * The doPost method of the servlet. &lt;br&gt; * * This method is called when a form has its tag value method equals to post. * * @param request the request send by the client to the server * @param response the response send by the server to the client * @throws ServletException if an error occurred * @throws IOException if an error occurred */ public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.setContentType(\"text/html\"); PrintWriter out = response.getWriter(); out.println(\"&lt;!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\"&gt;\"); out.println(\"&lt;HTML&gt;\"); out.println(\" &lt;HEAD&gt;&lt;TITLE&gt;A Servlet&lt;/TITLE&gt;&lt;/HEAD&gt;\"); out.println(\" &lt;BODY&gt;\"); out.print(\" This is \"); out.print(this.getClass()); out.println(\", using the POST method\"); out.println(\" &lt;/BODY&gt;\"); out.println(\"&lt;/HTML&gt;\"); out.flush(); out.close(); }} 然后我们就可以通过浏览器访问ServletDemo1这个Servlet，如下图所示： 五、Servlet开发注意细节5.1、Servlet访问URL映射配置 由于客户端是通过URL地址访问web服务器中的资源，所以Servlet程序若想被外界访问，必须把servlet程序映射到一个URL地址上，这个工作在web.xml文件中使用元素和元素完成。 元素用于注册Servlet，它包含有两个主要的子元素：和，分别用于设置Servlet的注册名称和Servlet的完整类名。一个元素用于映射一个已注册的Servlet的一个对外访问路径，它包含有两个子元素：和，分别用于指定Servlet的注册名称和Servlet的对外访问路径。例如： 123456789 &lt;servlet&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;servlet-class&gt;gacl.servlet.study.ServletDemo1&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/ServletDemo1&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 同一个Servlet可以被映射到多个URL上，即多个元素的子元素的设置值可以是同一个Servlet的注册名。 例如： 12345678910111213141516171819202122232425&lt;servlet&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;servlet-class&gt;gacl.servlet.study.ServletDemo1&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/ServletDemo1&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/1.htm&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/2.jsp&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/3.php&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/4.ASPX&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 通过上面的配置，当我们想访问名称是ServletDemo1的Servlet，可以使用如下的几个地址去访问： http://localhost:8080/JavaWeb_Servlet_Study_20140531/servlet/ServletDemo1 http://localhost:8080/JavaWeb_Servlet_Study_20140531/1.htm http://localhost:8080/JavaWeb_Servlet_Study_20140531/2.jsp http://localhost:8080/JavaWeb_Servlet_Study_20140531/3.php http://localhost:8080/JavaWeb_Servlet_Study_20140531/4.ASPX ServletDemo1被映射到了多个URL上。 5.2、Servlet访问URL使用通配符映射 在Servlet映射到的URL中也可以使用通配符，但是只能有两种固定的格式：一种格式是”.扩展名”，另一种格式是以正斜杠（/）开头并以”/“结尾。例如： 12345678 &lt;servlet&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;servlet-class&gt;gacl.servlet.study.ServletDemo1&lt;/servlet-class&gt;&lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; 可以匹配任意的字符，所以此时可以用任意的URL去访问ServletDemo1这个Servlet，如下图所示：对于如下的一些映射关系： Servlet1 映射到 /abc/ Servlet2 映射到 /* Servlet3 映射到 /abc Servlet4 映射到 *.do问题： 1234567891011 当请求URL为“/abc/a.html”，“/abc/*”和“/*”都匹配，哪个servlet响应 Servlet引擎将调用Servlet1。 当请求URL为“/abc”时，“/abc/*”和“/abc”都匹配，哪个servlet响应 Servlet引擎将调用Servlet3。 当请求URL为“/abc/a.do”时，“/abc/*”和“*.do”都匹配，哪个servlet响应 Servlet引擎将调用Servlet1。 当请求URL为“/a.do”时，“/*”和“*.do”都匹配，哪个servlet响应 Servlet引擎将调用Servlet2。 当请求URL为“/xxx/yyy/a.do”时，“/*”和“*.do”都匹配，哪个servlet响应 Servlet引擎将调用Servlet2。 匹配的原则就是\"谁长得更像就找谁\" 5.3、Servlet与普通Java类的区别 Servlet是一个供其他Java程序（Servlet引擎）调用的Java类，它不能独立运行，它的运行完全由Servlet引擎来控制和调度。 针对客户端的多次Servlet请求，通常情况下，服务器只会创建一个Servlet实例对象，也就是说Servlet实例对象一旦创建，它就会驻留在内存中，为后续的其它请求服务，直至web容器退出，servlet实例对象才会销毁。 在Servlet的整个生命周期内，Servlet的init方法只被调用一次。而对一个Servlet的每次访问请求都导致Servlet引擎调用一次servlet的service方法。对于每次访问请求，Servlet引擎都会创建一个新的HttpServletRequest请求对象和一个新的HttpServletResponse响应对象，然后将这两个对象作为参数传递给它调用的Servlet的service()方法，service方法再根据请求方式分别调用doXXX方法。 如果在元素中配置了一个元素，那么WEB应用程序在启动时，就会装载并创建Servlet的实例对象、以及调用Servlet实例对象的init()方法。 举例： 1234567&lt;servlet&gt; &lt;servlet-name&gt;invoker&lt;/servlet-name&gt; &lt;servlet-class&gt; org.apache.catalina.servlets.InvokerServlet &lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; 用途：为web应用写一个InitServlet，这个servlet配置为启动时装载，为整个web应用创建必要的数据库表和数据。 5.4、缺省Servlet 如果某个Servlet的映射路径仅仅为一个正斜杠（/），那么这个Servlet就成为当前Web应用程序的缺省Servlet。 凡是在web.xml文件中找不到匹配的元素的URL，它们的访问请求都将交给缺省Servlet处理，也就是说，缺省Servlet用于处理所有其他Servlet都不处理的访问请求。 例如： 1234567891011&lt;servlet&gt; &lt;servlet-name&gt;ServletDemo2&lt;/servlet-name&gt; &lt;servlet-class&gt;gacl.servlet.study.ServletDemo2&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;!-- 将ServletDemo2配置成缺省Servlet --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo2&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 当访问不存在的Servlet时，就使用配置的默认Servlet进行处理，如下图所示： 在&lt;tomcat的安装目录&gt;\\conf\\web.xml文件中，注册了一个名称为org.apache.catalina.servlets.DefaultServlet的Servlet，并将这个Servlet设置为了缺省Servlet。 12345678910111213141516171819&lt;servlet&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.catalina.servlets.DefaultServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;debug&lt;/param-name&gt; &lt;param-value&gt;0&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;listings&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;!-- The mapping for the default servlet --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 当访问Tomcat服务器中的某个静态HTML文件和图片时，实际上是在访问这个缺省Servlet。5.5、Servlet的线程安全问题当多个客户端并发访问同一个Servlet时，web服务器会为每一个客户端的访问请求创建一个线程，并在这个线程上调用Servlet的service方法，因此service方法内如果访问了同一个资源的话，就有可能引发线程安全问题。例如下面的代码： 不存在线程安全问题的代码： 12345678910111213141516171819202122232425262728293031323334package gacl.servlet.study;import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;public class ServletDemo3 extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { /** * 当多线程并发访问这个方法里面的代码时，会存在线程安全问题吗 * i变量被多个线程并发访问，但是没有线程安全问题，因为i是doGet方法里面的局部变量， * 当有多个线程并发访问doGet方法时，每一个线程里面都有自己的i变量， * 各个线程操作的都是自己的i变量，所以不存在线程安全问题 * 多线程并发访问某一个方法的时候，如果在方法内部定义了一些资源(变量，集合等) * 那么每一个线程都有这些东西，所以就不存在线程安全问题了 */ int i=1; i++; response.getWriter().write(i); } public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); }} 存在线程安全问题的代码： 123456789101112131415161718192021222324252627282930package gacl.servlet.study;import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;public class ServletDemo3 extends HttpServlet { int i=1; public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { i++; try { Thread.sleep(1000*4); } catch (InterruptedException e) { e.printStackTrace(); } response.getWriter().write(i+\"\"); } public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); }} 把i定义成全局变量，当多个线程并发访问变量i时，就会存在线程安全问题了，如下图所示：同时开启两个浏览器模拟并发访问同一个Servlet，本来正常来说，第一个浏览器应该看到2，而第二个浏览器应该看到3的，结果两个浏览器都看到了3，这就不正常。 线程安全问题只存在多个线程并发操作同一个资源的情况下，所以在编写Servlet的时候，如果并发访问某一个资源(变量，集合等)，就会存在线程安全问题，那么该如何解决这个问题呢？ 先看看下面的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041package gacl.servlet.study;import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;public class ServletDemo3 extends HttpServlet { int i=1; public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { /** * 加了synchronized后，并发访问i时就不存在线程安全问题了， * 为什么加了synchronized后就没有线程安全问题了呢？ * 假如现在有一个线程访问Servlet对象，那么它就先拿到了Servlet对象的那把锁 * 等到它执行完之后才会把锁还给Servlet对象，由于是它先拿到了Servlet对象的那把锁， * 所以当有别的线程来访问这个Servlet对象时，由于锁已经被之前的线程拿走了，后面的线程只能排队等候了 * */ synchronized (this) {//在java中，每一个对象都有一把锁，这里的this指的就是Servlet对象 i++; try { Thread.sleep(1000*4); } catch (InterruptedException e) { e.printStackTrace(); } response.getWriter().write(i+\"\"); } } public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); }} 现在这种做法是给Servlet对象加了一把锁，保证任何时候都只有一个线程在访问该Servlet对象里面的资源，这样就不存在线程安全问题了，如下图所示： 这种做法虽然解决了线程安全问题，但是编写Servlet却万万不能用这种方式处理线程安全问题，假如有9999个人同时访问这个Servlet，那么这9999个人必须按先后顺序排队轮流访问。 针对Servlet的线程安全问题，Sun公司是提供有解决方案的：让Servlet去实现一个SingleThreadModel接口，如果某个Servlet实现了SingleThreadModel接口，那么Servlet引擎将以单线程模式来调用其service方法。 查看Sevlet的API可以看到，SingleThreadModel接口中没有定义任何方法和常量，在Java中，把没有定义任何方法和常量的接口称之为标记接口，经常看到的一个最典型的标记接口就是”Serializable”，这个接口也是没有定义任何方法和常量的，标记接口在Java中有什么用呢？主要作用就是给某个对象打上一个标志，告诉JVM，这个对象可以做什么，比如实现了”Serializable”接口的类的对象就可以被序列化，还有一个”Cloneable”接口，这个也是一个标记接口，在默认情况下，Java中的对象是不允许被克隆的，就像现实生活中的人一样，不允许克隆，但是只要实现了”Cloneable”接口，那么对象就可以被克隆了。 让Servlet实现了SingleThreadModel接口，只要在Servlet类的定义中增加实现SingleThreadModel接口的声明即可。 对于实现了SingleThreadModel接口的Servlet，Servlet引擎仍然支持对该Servlet的多线程并发访问，其采用的方式是产生多个Servlet实例对象，并发的每个线程分别调用一个独立的Servlet实例对象。 实现SingleThreadModel接口并不能真正解决Servlet的线程安全问题，因为Servlet引擎会创建多个Servlet实例对象，而真正意义上解决多线程安全问题是指一个Servlet实例对象被多个线程同时调用的问题。事实上，在Servlet API 2.4中，已经将SingleThreadModel标记为Deprecated（过时的）。","link":"/2019/08/08/servlet开发/"},{"title":"scp","text":"scp命令传文件远程ip加端口号 scp -P port username@ip:filepath desfilepathport为远程ip对应的端口号。P大写","link":"/2019/07/05/scp/"},{"title":"@scheduled ","text":"@Scheduled cron表达式一、Cron详解：Cron表达式是一个字符串，字符串以5或6个空格隔开，分为6或7个域，每一个域代表一个含义，Cron有如下两种语法格式：1.Seconds Minutes Hours DayofMonth Month DayofWeek Year2.Seconds Minutes Hours DayofMonth Month DayofWeek每一个域可出现的字符如下：Seconds: 可出现”, - * /“四个字符，有效范围为0-59的整数Minutes: 可出现”, - * /“四个字符，有效范围为0-59的整数Hours: 可出现”, - * /“四个字符，有效范围为0-23的整数DayofMonth :可出现”, - * / ? L W C”八个字符，有效范围为0-31的整数Month: 可出现”, - * /“四个字符，有效范围为1-12的整数或JAN-DEcDayofWeek: 可出现”, - * / ? L C #”四个字符，有效范围为1-7的整数或SUN-SAT两个范围。 1表示星期天，2表示星期一， 依次类推Year: 可出现”, - * /“四个字符，有效范围为1970-2099年 每一个域都使用数字，但还可以出现如下特殊字符，它们的含义是：(1) ：表示匹配该域的任意值，假如在Minutes域使用, 即表示每分钟都会触发事件。(2) ?：只能用在DayofMonth和DayofWeek两个域。它也匹配域的任意值，但实际不会。因为DayofMonth和 DayofWeek会相互影响。例如想在每月的20日触发调度，不管20日到底是星期几，则只能使用如下写法： 13 13 15 20 * ?, 其中最后一位只能用？，而不能使用，如果使用表示不管星期几都会触发，实际上并不是这样。(3) -：表示范围，例如在Minutes域使用5-20，表示从5分到20分钟每分钟触发一次(4) /：表示起始时间开始触发，然后每隔固定时间触发一次，例如在Minutes域使用5/20,则意味着5分钟触发一次，而25，45等分别触发一次.(5) ,：表示列出枚举值值。例如：在Minutes域使用5,20，则意味着在5和20分每分钟触发一次。(6) L：表示最后，只能出现在DayofWeek和DayofMonth域，如果在DayofWeek域使用5L,意味着在最后的一个星期四触发。(7) W：表示有效工作日(周一到周五),只能出现在DayofMonth域，系统将在离指定日期的最近的有效工作日触发事件。例如：在 DayofMonth使用5W，如果5日是星期六，则将在最近的工作日：星期五，即4日触发。如果5日是星期天，则在6日(周一)触发；如果5日在星期一 到星期五中的一天，则就在5日触发。另外一点，W的最近寻找不会跨过月份。(8) LW：这两个字符可以连用，表示在某个月最后一个工作日，即最后一个星期五。(9) #：用于确定每个月第几个星期几，只能出现在DayofMonth域。例如在4#2，表示某月的第二个星期三。 举几个例子:每隔5秒执行一次：”*/5 * * * * ?”每隔1分钟执行一次：”0 */1 * * * ?”每天23点执行一次：”0 0 23 * * ?”每天凌晨1点执行一次：”0 0 1 * * ?”每月1号凌晨1点执行一次：”0 0 1 1 * ?”每月最后一天23点执行一次：”0 0 23 L * ?”每周星期天凌晨1点实行一次：”0 0 1 ? * L”在26分、29分、33分执行一次：”0 26,29,33 * * * ?”每天的0点、13点、18点、21点都执行一次：”0 0 0,13,18,21 * * ?”表示在每月的1日的凌晨2点调度任务：”0 0 2 1 * ? *”表示周一到周五每天上午10：15执行作业：”0 15 10 ? * MON-FRI”表示2002-2006年的每个月的最后一个星期五上午10:15执行：”0 15 10 ? 6L 2002-2006” 注意：由于”月份中的日期”和”星期中的日期”这两个元素互斥的,必须要对其中一个设置?二、启动服务时执行任务通过@Scheduled cron可以执行定时任务，而想要在服务启动时就先执行一次代码可以通过 implements InitializingBean 重写 afterPropertiesSet 方法来调用一遍定时任务的方法来达到启动服务时执行一遍工作的效果 12345678910111213141516public class FuncReportScheduled implements InitializingBean{ /** * 每月月初执行一次 */ @Scheduled(cron=\"0 0 0 1 * ?\") @Transactional public void funcReportInit(){ log.info(\"---------------- 定时任务 ----------------\"); } @Override public void afterPropertiesSet() throws Exception { this.funcReportInit(); }}","link":"/2019/07/17/scheduled/"},{"title":"spring security","text":"摘自https://blog.csdn.net/liushangzaibeijing/article/details/81220610 最近在公司的项目中使用了spring security框架，所以有机会来学习一下，公司的项目是使用springboot搭建 springBoot版本1.59 spring security 版本4.2.3 （个人理解可能会有偏差，希望有不正确之处，大家能够指出来，共同探讨交流。） [TOCM] 一、Spring security框架简介1、简介一个能够为基于Spring的企业应用系统提供声明式的安全訪问控制解决方式的安全框架（简单说是对访问权限进行控制嘛），应用的安全性包括用户认证（Authentication）和用户授权（Authorization）两个部分。用户认证指的是验证某个用户是否为系统中的合法主体，也就是说用户能否访问该系统。用户认证一般要求用户提供用户名和密码。系统通过校验用户名和密码来完成认证过程。用户授权指的是验证某个用户是否有权限执行某个操作。在一个系统中，不同用户所具有的权限是不同的。比如对一个文件来说，有的用户只能进行读取，而有的用户可以进行修改。一般来说，系统会为不同的用户分配不同的角色，而每个角色则对应一系列的权限。 spring security的主要核心功能为 认证和授权，所有的架构也是基于这两个核心功能去实现的。2、框架原理众所周知 想要对对Web资源进行保护，最好的办法莫过于Filter，要想对方法调用进行保护，最好的办法莫过于AOP。所以springSecurity在我们进行用户认证以及授予权限的时候，通过各种各样的拦截器来控制权限的访问，从而实现安全。 如下为其主要过滤器 WebAsyncManagerIntegrationFilter SecurityContextPersistenceFilter HeaderWriterFilter CorsFilter LogoutFilter RequestCacheAwareFilter SecurityContextHolderAwareRequestFilter AnonymousAuthenticationFilter SessionManagementFilter ExceptionTranslationFilter FilterSecurityInterceptor UsernamePasswordAuthenticationFilter BasicAuthenticationFilter3、框架的核心组件 SecurityContextHolder：提供对SecurityContext的访问 SecurityContext,：持有Authentication对象和其他可能需要的信息 AuthenticationManager 其中可以包含多个AuthenticationProvider ProviderManager对象为AuthenticationManager接口的实现类 AuthenticationProvider 主要用来进行认证操作的类 调用其中的authenticate()方法去进行认证操作 Authentication：Spring Security方式的认证主体 GrantedAuthority：对认证主题的应用层面的授权，含当前用户的权限信息，通常使用角色表示 UserDetails：构建Authentication对象必须的信息，可以自定义，可能需要访问DB得到 UserDetailsService：通过username构建UserDetails对象，通过loadUserByUsername根据userName获取UserDetail对象 （可以在这里基于自身业务进行自定义的实现 如通过数据库，xml,缓存获取等） 二、自定义安全配置的加载机制1、前提 基于自身业务需要有关springSecrity安全框架的理解参考：springSecurity安全框架介绍 自定义了一个springSecurity安全框架的配置类 继承WebSecurityConfigurerAdapter，重写其中的方法configure，但是并不清楚自定义的类是如何被加载并起到作用，这里一步步通过debug来了解其中的加载原理。 其实在我们实现该类后，在web容器启动的过程中该类实例对象会被WebSecurityConfiguration类处理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Configurationpublic class SpringSecurityConfig extends WebSecurityConfigurerAdapter { @Autowired private AccessDeniedHandler accessDeniedHandler; @Autowired private CustAuthenticationProvider custAuthenticationProvider; // roles admin allow to access /admin/** // roles user allow to access /user/** // custom 403 access denied handler //重写了其中的configure（）方法设置了不同url的不同访问权限 @Override protected void configure(HttpSecurity http) throws Exception { http.csrf().disable() .authorizeRequests() .antMatchers(\"/home\", \"/about\",\"/img/*\").permitAll() .antMatchers(\"/admin/**\",\"/upload/**\").hasAnyRole(\"ADMIN\") .antMatchers(\"/order/**\").hasAnyRole(\"USER\",\"ADMIN\") .antMatchers(\"/room/**\").hasAnyRole(\"USER\",\"ADMIN\") .anyRequest().authenticated() .and() .formLogin() .loginPage(\"/login\") .permitAll() .and() .logout() .permitAll() .and() .exceptionHandling().accessDeniedHandler(accessDeniedHandler); } // create two users, admin and user @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception { // auth.inMemoryAuthentication()// .withUser(\"user\").password(\"user\").roles(\"USER\")// .and()// .withUser(\"admin\").password(\"admin\").roles(\"ADMIN\"); // auth.jdbcAuthentication() auth.authenticationProvider(custAuthenticationProvider); } 2、WebSecurityConfiguration类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278@Configurationpublic class WebSecurityConfiguration implements ImportAware, BeanClassLoaderAware { private WebSecurity webSecurity; private Boolean debugEnabled; private List&lt;SecurityConfigurer&lt;Filter, WebSecurity&gt;&gt; webSecurityConfigurers; private ClassLoader beanClassLoader; ...省略部分代码 @Bean( name = {\"springSecurityFilterChain\"} ) public Filter springSecurityFilterChain() throws Exception { boolean hasConfigurers = this.webSecurityConfigurers != null &amp;&amp; !this.webSecurityConfigurers.isEmpty(); if(!hasConfigurers) { WebSecurityConfigurerAdapter adapter = (WebSecurityConfigurerAdapter) this.objectObjectPostProcessor .postProcess(new WebSecurityConfigurerAdapter() { }); this.webSecurity.apply(adapter); } return (Filter)this.webSecurity.build(); } /*1、先执行该方法将我们自定义springSecurity配置实例 （可能还有系统默认的有关安全的配置实例 ） 配置实例中含有我们自定义业务的权限控制配置信息 放入到该对象的list数组中webSecurityConfigurers中 使用@Value注解来将实例对象作为形参注入 */ @Autowired( required = false ) public void setFilterChainProxySecurityConfigurer(ObjectPostProcessor&lt;Object&gt; objectPostProcessor, @Value(\"#{@autowiredWebSecurityConfigurersIgnoreParents.getWebSecurityConfigurers()}\") List&lt;SecurityConfigurer&lt;Filter, WebSecurity&gt;&gt; webSecurityConfigurers) throws Exception { //创建一个webSecurity对象 this.webSecurity = (WebSecurity)objectPostProcessor.postProcess(new WebSecurity(objectPostProcessor)); if(this.debugEnabled != null) { this.webSecurity.debug(this.debugEnabled.booleanValue()); } //对所有配置类的实例进行排序 Collections.sort(webSecurityConfigurers, WebSecurityConfiguration.AnnotationAwareOrderComparator.INSTANCE); Integer previousOrder = null; Object previousConfig = null; //迭代所有配置类的实例 判断其order必须唯一 Iterator var5; SecurityConfigurer config; for(var5 = webSecurityConfigurers.iterator(); var5.hasNext(); previousConfig = config) { config = (SecurityConfigurer)var5.next(); Integer order = Integer.valueOf(WebSecurityConfiguration.AnnotationAwareOrderComparator.lookupOrder(config)); if(previousOrder != null &amp;&amp; previousOrder.equals(order)) { throw new IllegalStateException(\"@Order on WebSecurityConfigurers must be unique. Order of \" + order + \" was already used on \" + previousConfig + \", so it cannot be used on \" + config + \" too.\"); } previousOrder = order; } //将所有的配置实例添加到创建的webSecutity对象中 var5 = webSecurityConfigurers.iterator(); while(var5.hasNext()) { config = (SecurityConfigurer)var5.next(); this.webSecurity.apply(config); } //将webSercurityConfigures 实例放入该对象的webSecurityConfigurers属性中 this.webSecurityConfigurers = webSecurityConfigurers; } } 2.1、 setFilterChainProxySecurityConfigurer（）方法@Value(\"#{@autowiredWebSecurityConfigurersIgnoreParents.getWebSecurityConfigurers()}\") List&lt;SecurityConfigurer&lt;Filter, WebSecurity&gt;&gt; webSecurityConfigurers 该参数webSecurityConfigurers会将所有的配置实例放入该形参中 该方法中 主要执行如下 1、创建webSecurity对象 2、主要检验了配置实例的order顺序（order唯一 否则会报错） 3、将所有的配置实例存放进入到webSecurity对象中，其中配置实例中含有我们自定义业务的权限控制配置信息 2.2、springSecurityFilterChain()方法 调用springSecurityFilterChain()方法，这个方法会判断我们上一个方法中有没有获取到webSecurityConfigurers，没有的话这边会创建一个WebSecurityConfigurerAdapter实例，并追加到websecurity中。接着调用websecurity的build方法。实际调用的是websecurity的父类AbstractSecurityBuilder的build方法 ，最终返回一个名称为springSecurityFilterChain的过滤器链。里面有众多Filter(springSecurity其实就是依靠很多的Filter来拦截url从而实现权限的控制的安全框架)3、AbstractSecurityBuilder类public abstract class AbstractSecurityBuilder&lt;O&gt; implements SecurityBuilder&lt;O&gt; { private AtomicBoolean building = new AtomicBoolean(); private O object; //调用build方法来返回过滤器链，还是调用SecurityBuilder的dobuild()方法 public final O build() throws Exception { if(this.building.compareAndSet(false, true)) { this.object = this.doBuild(); return this.object; } else { throw new AlreadyBuiltException(\"This object has already been built\"); } } //...省略部分代码} 3.1 调用子类的doBuild()方法public abstract class AbstractConfiguredSecurityBuilder&lt;O, B extends SecurityBuilder&lt;O&gt;&gt; extends AbstractSecurityBuilder&lt;O&gt; { private final Log logger; private final LinkedHashMap&lt;Class&lt;? extends SecurityConfigurer&lt;O, B&gt;&gt;, List&lt;SecurityConfigurer&lt;O, B&gt;&gt;&gt; configurers; private final List&lt;SecurityConfigurer&lt;O, B&gt;&gt; configurersAddedInInitializing; private final Map&lt;Class&lt;? extends Object&gt;, Object&gt; sharedObjects; private final boolean allowConfigurersOfSameType; private AbstractConfiguredSecurityBuilder.BuildState buildState; private ObjectPostProcessor&lt;Object&gt; objectPostProcessor; //doBuild()核心方法 init(),configure(),perFormBuild() protected final O doBuild() throws Exception { LinkedHashMap var1 = this.configurers; synchronized(this.configurers) { this.buildState = AbstractConfiguredSecurityBuilder.BuildState.INITIALIZING; this.beforeInit(); this.init(); this.buildState = AbstractConfiguredSecurityBuilder.BuildState.CONFIGURING; this.beforeConfigure(); this.configure(); this.buildState = AbstractConfiguredSecurityBuilder.BuildState.BUILDING; O result = this.performBuild(); this.buildState = AbstractConfiguredSecurityBuilder.BuildState.BUILT; return result; } } protected abstract O performBuild() throws Exception; //调用init方法 调用配置类WebSecurityConfigurerAdapter的init()方法 private void init() throws Exception { Collection&lt;SecurityConfigurer&lt;O, B&gt;&gt; configurers = this.getConfigurers(); Iterator var2 = configurers.iterator(); SecurityConfigurer configurer; while(var2.hasNext()) { configurer = (SecurityConfigurer)var2.next(); configurer.init(this); } var2 = this.configurersAddedInInitializing.iterator(); while(var2.hasNext()) { configurer = (SecurityConfigurer)var2.next(); configurer.init(this); } } private void configure() throws Exception { Collection&lt;SecurityConfigurer&lt;O, B&gt;&gt; configurers = this.getConfigurers(); Iterator var2 = configurers.iterator(); while(var2.hasNext()) { SecurityConfigurer&lt;O, B&gt; configurer = (SecurityConfigurer)var2.next(); configurer.configure(this); } } private Collection&lt;SecurityConfigurer&lt;O, B&gt;&gt; getConfigurers() { List&lt;SecurityConfigurer&lt;O, B&gt;&gt; result = new ArrayList(); Iterator var2 = this.configurers.values().iterator(); while(var2.hasNext()) { List&lt;SecurityConfigurer&lt;O, B&gt;&gt; configs = (List)var2.next(); result.addAll(configs); } return result; } //...省略部分代码}3.2 先调用本类的init()方法build过程主要分三步，init-&gt;configure-&gt;peformBuild 1 init方法做了两件事，一个就是调用getHttp()方法获取一个http实例，并通过web.addSecurityFilterChainBuilder方法把获取到的实例赋值给WebSecurity的securityFilterChainBuilders属性，这个属性在我们执行build的时候会用到，第二个就是为WebSecurity追加了一个postBuildAction，在build都完成后从http中拿出FilterSecurityInterceptor对象并赋值给WebSecurity。 2 getHttp()方法，这个方法在当我们使用默认配置时（大多数情况下）会为我们追加各种SecurityConfigurer的具体实现类到httpSecurity中，如exceptionHandling()方法会追加一个ExceptionHandlingConfigurer，sessionManagement()方法会追加一个SessionManagementConfigurer,securityContext()方法会追加一个SecurityContextConfigurer对象，这些SecurityConfigurer的具体实现类最终会为我们配置各种具体的filter。3 另外getHttp()方法的最后会调用configure(http)，这个方法也是我们继承WebSecurityConfigurerAdapter类后最可能会重写的方法 。4 configure(HttpSecurity http)方法，默认的configure(HttpSecurity http)方法继续向httpSecurity类中追加SecurityConfigurer的具体实现类，如authorizeRequests()方法追加一个ExpressionUrlAuthorizationConfigurer，formLogin()方法追加一个FormLoginConfigurer。 其中ExpressionUrlAuthorizationConfigurer这个实现类比较重要，因为他会给我们创建一个非常重要的对象FilterSecurityInterceptor对象，FormLoginConfigurer对象比较简单，但是也会为我们提供一个在安全认证过程中经常用到会用的一个Filter：UsernamePasswordAuthenticationFilter。 以上三个方法就是WebSecurityConfigurerAdapter类中init方法的主要逻辑，public abstract class WebSecurityConfigurerAdapter implements WebSecurityConfigurer&lt;WebSecurity&gt; { public void init(final WebSecurity web) throws Exception { final HttpSecurity http = this.getHttp(); web.addSecurityFilterChainBuilder(http).postBuildAction(new Runnable() { public void run() { FilterSecurityInterceptor securityInterceptor = (FilterSecurityInterceptor)http.getSharedObject(FilterSecurityInterceptor.class); web.securityInterceptor(securityInterceptor); } }); } protected final HttpSecurity getHttp() throws Exception { if(this.http != null) { return this.http; } else { DefaultAuthenticationEventPublisher eventPublisher = (DefaultAuthenticationEventPublisher)this.objectPostProcessor.postProcess(new DefaultAuthenticationEventPublisher()); //添加认证的事件的发布者this.localConfigureAuthenticationBldr.authenticationEventPublisher(eventPublisher);//获取AuthenticationManager对象其中一至多个进行认证处理的对象实例，后面会进行讲解 AuthenticationManager authenticationManager = this.authenticationManager(); this.authenticationBuilder.parentAuthenticationManager(authenticationManager); Map&lt;Class&lt;? extends Object&gt;, Object&gt; sharedObjects = this.createSharedObjects(); this.http = new HttpSecurity(this.objectPostProcessor, this.authenticationBuilder, sharedObjects); if(!this.disableDefaults) { ((HttpSecurity)((DefaultLoginPageConfigurer)((HttpSecurity)((HttpSecurity)((HttpSecurity)((HttpSecurity)((HttpSecurity)((HttpSecurity)((HttpSecurity)((HttpSecurity)this.http.csrf().and()).addFilter(new WebAsyncManagerIntegrationFilter()).exceptionHandling().and()).headers().and()).sessionManagement().and()).securityContext().and()).requestCache().and()).anonymous().and()).servletApi().and()).apply(new DefaultLoginPageConfigurer())).and()).logout(); ClassLoader classLoader = this.context.getClassLoader(); List&lt;AbstractHttpConfigurer&gt; defaultHttpConfigurers = SpringFactoriesLoader.loadFactories(AbstractHttpConfigurer.class, classLoader); Iterator var6 = defaultHttpConfigurers.iterator(); while(var6.hasNext()) { AbstractHttpConfigurer configurer = (AbstractHttpConfigurer)var6.next(); this.http.apply(configurer); } } //最终调用我们的继承的WebSecurityConfigurerAdapter中重写的configure() //将我们业务相关的权限配置规则信息进行初始化操作 this.configure(this.http); return this.http; } } protected AuthenticationManager authenticationManager() throws Exception { if(!this.authenticationManagerInitialized) { this.configure(this.localConfigureAuthenticationBldr); if(this.disableLocalConfigureAuthenticationBldr) { this.authenticationManager = this.authenticationConfiguration.getAuthenticationManager(); } else { this.authenticationManager = (AuthenticationManager)this.localConfigureAuthenticationBldr.build(); } this.authenticationManagerInitialized = true; } return this.authenticationManager; } } 3.3、第二步configure configure方法最终也调用到了WebSecurityConfigurerAdapter的configure(WebSecurity web)方法，默认实现中这个是一个空方法，具体应用中也经常重写这个方法来实现特定需求。3.4、第三步 peformBuild 具体的实现逻辑在WebSecurity类中这个方法中最主要的任务就是遍历securityFilterChainBuilders属性中的SecurityBuilder对象，并调用他的build方法。这个securityFilterChainBuilders属性我们前面也有提到过，就是在WebSecurityConfigurerAdapter类的init方法中获取http后赋值给了WebSecurity。因此这个地方就是调用httpSecurity的build方法。 httpSecurity的build方式向其中追加一个个过滤器 123456789101112131415161718192021222324252627282930313233343536373839404142public final class WebSecurity extends AbstractConfiguredSecurityBuilder&lt;Filter, WebSecurity&gt; implements SecurityBuilder&lt;Filter&gt;, ApplicationContextAware { ...省略部分代码 //调用该方法通过securityFilterChainBuilder.build()方法来创建securityFilter过滤器 //并添加到securityFilterChains对象中，包装成FilterChainProxy 返回 protected Filter performBuild() throws Exception { Assert.state(!this.securityFilterChainBuilders.isEmpty(), \"At least one SecurityBuilder&lt;? extends SecurityFilterChain&gt; needs to be specified. Typically this done by adding a @Configuration that extends WebSecurityConfigurerAdapter. More advanced users can invoke \" + WebSecurity.class.getSimpleName() + \".addSecurityFilterChainBuilder directly\"); int chainSize = this.ignoredRequests.size() + this.securityFilterChainBuilders.size(); List&lt;SecurityFilterChain&gt; securityFilterChains = new ArrayList(chainSize); Iterator var3 = this.ignoredRequests.iterator(); while(var3.hasNext()) { RequestMatcher ignoredRequest = (RequestMatcher)var3.next(); securityFilterChains.add(new DefaultSecurityFilterChain(ignoredRequest, new Filter[0])); } var3 = this.securityFilterChainBuilders.iterator(); while(var3.hasNext()) { SecurityBuilder&lt;? extends SecurityFilterChain&gt; securityFilterChainBuilder = (SecurityBuilder)var3.next(); securityFilterChains.add(securityFilterChainBuilder.build()); } FilterChainProxy filterChainProxy = new FilterChainProxy(securityFilterChains); if(this.httpFirewall != null) { filterChainProxy.setFirewall(this.httpFirewall); } filterChainProxy.afterPropertiesSet(); Filter result = filterChainProxy; if(this.debugEnabled) { this.logger.warn(\"\\n\\n********************************************************************\\n********** Security debugging is enabled. *************\\n********** This may include sensitive information. *************\\n********** Do not use in a production system! *************\\n********************************************************************\\n\\n\"); result = new DebugFilter(filterChainProxy); } this.postBuildAction.run(); return (Filter)result; } } 4、举例说明如何将一个Configurer转换为filterExpressionUrlAuthorizationConfigurer的继承关系ExpressionUrlAuthorizationConfigurer-&gt;AbstractInterceptUrlConfigurer-&gt;AbstractHttpConfigurer-&gt;SecurityConfigurerAdapter-&gt;SecurityConfigurer对应的init方法在SecurityConfigurerAdapter类中，是个空实现，什么也没有做，configure方法在SecurityConfigurerAdapter类中也有一个空实现，在AbstractInterceptUrlConfigurer类中进行了重写 Abstractintercepturlconfigurer.java代码 1234567891011121314151617181920212223242526272829303132@Override public void configure(H http) throws Exception { FilterInvocationSecurityMetadataSource metadataSource = createMetadataSource(http); if (metadataSource == null) { return; } FilterSecurityInterceptor securityInterceptor = createFilterSecurityInterceptor( http, metadataSource, http.getSharedObject(AuthenticationManager.class)); if (filterSecurityInterceptorOncePerRequest != null) { securityInterceptor .setObserveOncePerRequest(filterSecurityInterceptorOncePerRequest); } securityInterceptor = postProcess(securityInterceptor); http.addFilter(securityInterceptor); http.setSharedObject(FilterSecurityInterceptor.class, securityInterceptor); } ... private AccessDecisionManager createDefaultAccessDecisionManager(H http) { AffirmativeBased result = new AffirmativeBased(getDecisionVoters(http)); return postProcess(result); } ... private FilterSecurityInterceptor createFilterSecurityInterceptor(H http, FilterInvocationSecurityMetadataSource metadataSource, AuthenticationManager authenticationManager) throws Exception { FilterSecurityInterceptor securityInterceptor = new FilterSecurityInterceptor(); securityInterceptor.setSecurityMetadataSource(metadataSource); securityInterceptor.setAccessDecisionManager(getAccessDecisionManager(http)); securityInterceptor.setAuthenticationManager(authenticationManager); securityInterceptor.afterPropertiesSet(); return securityInterceptor; } 4.1、 在这个类的configure中创建了一个FilterSecurityInterceptor，并且也可以明确看到spring security默认给我们创建的AccessDecisionManager是AffirmativeBased。 4.2、.最后再看下HttpSecurity类执行build的最后一步 performBuild，这个方法就是在HttpSecurity中实现的 Httpsecurity.java代码 123456789@Override protected DefaultSecurityFilterChain performBuild() throws Exception { Collections.sort(filters, comparator); return new DefaultSecurityFilterChain(requestMatcher, filters); } 可以看到，这个类只是把我们追加到HttpSecurity中的security进行了排序，用的排序类是FilterComparator，从而保证我们的filter按照正确的顺序执行。接着将filters构建成filterChian返回。在前面WebSecurity的performBuild方法中，这个返回值会被包装成FilterChainProxy，并作为WebSecurity的build方法的放回值。从而以springSecurityFilterChain这个名称注册到springContext中（在WebSecurityConfiguration中做的） 4.3.在WebSecurity的performBuild方法的最后一步还执行了一个postBuildAction.run，这个方法也是spring security给我们提供的一个hooks，可以在build完成后再做一些事情，比如我们在WebSecurityConfigurerAdapter类的init方法中我们利用这个hook在构建完成后将FilterSecurityInterceptor赋值给了webSecurity类的filterSecurityInterceptor属性 三、用户登录的验证和授权过程1、用户一次完整的登录验证和授权，是一个请求经过 层层拦截器从而实现权限控制，整个web端配置为DelegatingFilterProxy（springSecurity的委托过滤其代理类 ），它并不实现真正的过滤，而是所有过滤器链的代理类，真正执行拦截处理的是由spring 容器管理的个个filter bean组成的filterChain. 调用实际的FilterChainProxy 的doFilterInternal()方法 去获取所有的拦截器并进行过滤处理如下是DelegatingFilterProxy的doFilter（）方法 1234567891011121314151617181920212223public void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain) throws ServletException, IOException { Filter delegateToUse = this.delegate; if(delegateToUse == null) { Object var5 = this.delegateMonitor; synchronized(this.delegateMonitor) { delegateToUse = this.delegate; if(delegateToUse == null) { WebApplicationContext wac = this.findWebApplicationContext(); if(wac == null) { throw new IllegalStateException(\"No WebApplicationContext found: no ContextLoaderListener or DispatcherServlet registered?\"); } delegateToUse = this.initDelegate(wac); } this.delegate = delegateToUse; } } //调用实际的FilterChainProxy 的doFilterInternal()方法 去获取所有的拦截器并进行过滤处理 this.invokeDelegate(delegateToUse, request, response, filterChain); }调用实际的FilterChainProxy 的doFilter()方法 去获取所有的拦截器并进行过滤处理。 2、FilterChainProxy类最终调用FilterChainProxy 的doFilterInternal()方法，获取所有的过滤器实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { boolean clearContext = request.getAttribute(FILTER_APPLIED) == null; if(clearContext) { try { request.setAttribute(FILTER_APPLIED, Boolean.TRUE); //doFilter 调用doFilterInternal方法 this.doFilterInternal(request, response, chain); } finally { SecurityContextHolder.clearContext(); request.removeAttribute(FILTER_APPLIED); } } else { this.doFilterInternal(request, response, chain); } } private void doFilterInternal(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { FirewalledRequest fwRequest = this.firewall.getFirewalledRequest((HttpServletRequest)request); HttpServletResponse fwResponse = this.firewall.getFirewalledResponse((HttpServletResponse)response); //过去所有的过滤器 List&lt;Filter&gt; filters = this.getFilters((HttpServletRequest)fwRequest); if(filters != null &amp;&amp; filters.size() != 0) { FilterChainProxy.VirtualFilterChain vfc = new FilterChainProxy.VirtualFilterChain(fwRequest, chain, filters); vfc.doFilter(fwRequest, fwResponse); } else { if(logger.isDebugEnabled()) { logger.debug(UrlUtils.buildRequestUrl(fwRequest) + (filters == null?\" has no matching filters\":\" has an empty filter list\")); } fwRequest.reset(); chain.doFilter(fwRequest, fwResponse); } } private List&lt;Filter&gt; getFilters(HttpServletRequest request) { //遍历所有的matcher类 如果支持就继续获取 Iterator var2 = this.filterChains.iterator(); SecurityFilterChain chain; do { if(!var2.hasNext()) { return null; } chain = (SecurityFilterChain)var2.next(); } while(!chain.matches(request)); //后去匹配中的所有过滤器 return chain.getFilters(); } 如上 其实是获取到本次请求的所有filter 并安装指定顺序进行执行doFilter()方法这是笔者本次业务请求所要执行的所有过滤器 1234567891011 WebAsyncManagerIntegrationFilter SecurityContextPersistenceFilter HeaderWriterFilter LogoutFilter UsernamePasswordAuthenticationFilter RequestCacheAwareFilter SecurityContextHolderAwareRequestFilter AnonymousAuthenticationFilter SessionManagementFilter ExceptionTranslationFilter FilterSecurityInterceptor 关于springSecutity拦截器的介绍请参考如下链接地址https://blog.csdn.net/dushiwodecuo/article/details/78913113 http://blog.didispace.com/xjf-spring-security-4/ https://www.cnblogs.com/HHR-SUN/p/7095720.html https://blog.csdn.net/zheng963/article/details/50427320 https://blog.csdn.net/m0_37834471/article/details/81142246 https://www.cnblogs.com/mingluosunshan/p/5485259.html","link":"/2020/02/29/spring security/"},{"title":"spring session实现session共享","text":"使用Spring Session redis进行Session共享 在搭建完集群环境后，不得不考虑的一个问题就是用户访问产生的session如何处理。 session的处理有很多种方法，详情见转载的上篇博客：集群/分布式环境下5种session处理策略 在这里我们讨论其中的第三种方法：session共享。 redis集群做主从复制，利用redis数据库的最终一致性，将session信息存入redis中。当应用服务器发现session不在本机内存的时候，就去redis数据库中查找，因为redis数据库是独立于应用服务器的数据库，所以可以做到session的共享和高可用。 不足： 1.redis需要内存较大，否则会出现用户session从Cache中被清除。 2.需要定期的刷新缓存 初步结构如下 但是这个结构仍然存在问题，redis master是一个重要瓶颈，如果master崩溃的时候，但是redis不会主动的进行master切换，这时session服务中断。 但是我们先做到这个结构，后面再进行优化修改。Spring Boot提供了Spring Session来完成session共享。 官方文档：http://docs.spring.io/spring-session/docs/current/reference/html5/guides/boot.html#boot-sample 首先创建简单的Controller： 12345678910111213141516171819202122232425262728293031@Controllerpublic class UserController { @RequestMapping(value=\"/main\", method=RequestMethod.GET) public String main(HttpServletRequest request) { HttpSession session = request.getSession(); String sessionId = (String) session.getAttribute(\"sessionId\"); if (null != sessionId) { // sessionId不为空 System.out.println(\"main sessionId:\" + sessionId); return \"main\"; } else { // sessionId为空 return \"redirect:/login\"; } } @RequestMapping(value=\"/login\", method=RequestMethod.GET) public String login() { return \"login\"; } @RequestMapping(value=\"/doLogin\", method=RequestMethod.POST) public String doLogin(HttpServletRequest request) { System.out.println(\"I do real login here\"); HttpSession session = request.getSession(); String sessionId = UUID.randomUUID().toString(); session.setAttribute(\"sessionId\", sessionId); System.out.println(\"login sessionId:\" + sessionId); return \"redirect:/main\"; }} 简单来说就是模拟一下权限控制，如果sessionId存在就访问主页，否则就跳转到登录页面。那么如何实现session共享呢？ 加入以下依赖： 1234567891011&lt;!-- spring session --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session&lt;/artifactId&gt; &lt;version&gt;1.3.0.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- redis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-redis&lt;/artifactId&gt;&lt;/dependency&gt; 增加配置类： 1234567@EnableRedisHttpSessionpublic class HttpSessionConfig { @Bean public JedisConnectionFactory connectionFactory() { return new JedisConnectionFactory(); }} 这个配置类有什么用呢？官方文档： The Spring configuration is responsible for creating a Servlet Filter that replaces the HttpSession implementation with an implementation backed by Spring Session. 也就是说，这个配置类可以创建一个过滤器，这个过滤器支持Spring Session代替HttpSession发挥作用。 The @EnableRedisHttpSession annotation creates a Spring Bean with the name of springSessionRepositoryFilter that implements Filter. The filter is what is in charge of replacing the HttpSession implementation to be backed by Spring Session. In this instance Spring Session is backed by Redis. @EnableRedisHttpSession注解会创建一个springSessionRepositoryFilter的bean对象去实现这个过滤器。过滤器负责代替HttpSession。 也就是说，HttpSession不再发挥作用，而是通过过滤器使用redis直接操作Session。 在application.properties中添加redis的配置： 123spring.redis.host=localhostspring.redis.password=spring.redis.port=6379 这样，就完成了Session共享了。是不是很简单？业务代码甚至不需要一点点的修改。验证： 一开始redis数据库是空的。 运行项目： 访问页面之后，可以在redis中看到session的信息。 随便登陆之后： 进入到了main中。说明当前这个session中是存在sessionId的。 我们查看当前页面的cookie。也就是说，这个cookie是存在sessionId的。 再运行一个新的项目，端口为8081。在原本的浏览器中直接打开一个新的标签页，我们知道，这个时候cookie是共享的。访问localhost:8081/main 我们直接访问新的项目成功了！！同一个cookie，可以做到session在不同web服务器中的共享。 最后再次强调： ** HttpSession的实现被Spring Session替换，操作HttpSession等同于操作redis中的数据。**","link":"/2019/10/13/spring-seesion/"},{"title":"spring task","text":"基于Spring Task的定时任务调度器实现在很多时候，我们会需要执行一些定时任务 ，Spring团队提供了Spring Task模块对定时任务的调度提供了支持，基于注解式的任务使用也非常方便。只要跟需要定时执行的方法加上类似 @Scheduled(cron = “0 1 * * * *”) 的注解就可以实现方法的定时执行。cron 是一种周期的表达式，六位从右至左分别对应的是年、月、日、时、分、秒，数字配合各种通配符可以表达种类丰富的定时执行周期。 12345678910111213141516171819/*** Cron Example patterns:* &lt;li&gt;\"0 0 * * * *\" = the top of every hour of every day.&lt;/li&gt;* &lt;li&gt;\"0 0 8-10 * * *\" = 8, 9 and 10 o'clock of every day.&lt;/li&gt;* &lt;li&gt;\"0 0/30 8-10 * * *\" = 8:00, 8:30, 9:00, 9:30 and 10 o'clock every day.&lt;/li&gt;* &lt;li&gt;\"0 0 9-17 * * MON-FRI\" = on the hour nine-to-five weekdays&lt;/li&gt;* &lt;li&gt;\"0 0 0 25 12 ?\" = every Christmas Day at midnight&lt;/li&gt;*/基于注解的使用案列：import org.springframework.stereotype.Component; @Component(“task”) public class Task { @Scheduled(cron = \"0 1 * * * *\") // 每分钟执行一次 public void job1() { System.out.println(“任务进行中。。。”); } } 基于注解方式的定时任务，启动会依赖于系统的启动。如果需要通过代码或前台操作触发定时任务，就需要进行包装了。下面是一个可以直接提供业务代码调用的定时任务调度器。调用 schedule(Runnable task, String cron) 传入要执行的任务task和定时周期cron就可以了。注：基于注解方式需要在注解扫描范围内。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.louis.merak.schedule;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler;import org.springframework.scheduling.support.CronTrigger;import org.springframework.stereotype.Component; @Componentpublic class MerakTaskScheduler { @Autowired private ThreadPoolTaskScheduler threadPoolTaskScheduler; @Bean public ThreadPoolTaskScheduler threadPoolTaskScheduler(){ return new ThreadPoolTaskScheduler(); } /** * Cron Example patterns: * &lt;li&gt;\"0 0 * * * *\" = the top of every hour of every day.&lt;/li&gt; * &lt;li&gt;\"0 0 8-10 * * *\" = 8, 9 and 10 o'clock of every day.&lt;/li&gt; * &lt;li&gt;\"0 0/30 8-10 * * *\" = 8:00, 8:30, 9:00, 9:30 and 10 o'clock every day.&lt;/li&gt; * &lt;li&gt;\"0 0 9-17 * * MON-FRI\" = on the hour nine-to-five weekdays&lt;/li&gt; * &lt;li&gt;\"0 0 0 25 12 ?\" = every Christmas Day at midnight&lt;/li&gt; */ public void schedule(Runnable task, String cron){ if(cron == null || \"\".equals(cron)) { cron = \"0 * * * * *\"; } threadPoolTaskScheduler.schedule(task, new CronTrigger(cron)); } /** * shutdown and init * @param task * @param cron */ public void reset(){ threadPoolTaskScheduler.shutdown(); threadPoolTaskScheduler.initialize(); } /** * shutdown before a new schedule operation * @param task * @param cron */ public void resetSchedule(Runnable task, String cron){ shutdown(); threadPoolTaskScheduler.initialize(); schedule(task, cron); } /** * shutdown */ public void shutdown(){ threadPoolTaskScheduler.shutdown(); }} 如果是需要通过前台操作调用RESTful执行定时任务的调度，使用以下Controller即可。 1234567891011121314151617181920212223242526272829303132package com.louis.merak.common.schedule;import java.text.SimpleDateFormat;import java.util.Date;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class MerakTaskSchedulerController { @Autowired MerakTaskScheduler taskScheduler; @RequestMapping(\"/schedule\") public String schedule(@RequestParam String cron) { if(cron == null) { cron = \"0/5 * * * * *\"; } Runnable runnable = new Runnable() { public void run() { String time = new SimpleDateFormat(\"yy-MM-dd HH:mm:ss\").format(new Date()); System.out.println(\"Test GETaskScheduler Success at \" + time); } }; taskScheduler.schedule(runnable, cron); return \"Test TaskScheduler Interface.\"; }}","link":"/2019/07/17/spring-task/"},{"title":"spring task @scheduled","text":"Spring 4.x Task 和 Schedule 概述摘要在很多业务场景中，系统都需要用到任务调度系统。例如定期地清理Redis 缓存，周期性地检索某一条件并更新系统的资源等。在现代的应用系统中，快速地响应用户的请求，是用户体验最主要的因素之一。因此在Web 系统中异步地执行任务，也会在很多场景中经常涉及到。本文对任务调度和异步执行的Java 实现进行了总结，主要讲述一下内容：Java 对异步执行和任务调度的支持Spring 4.X 的异步执行和任务调度实现Java 对异步执行和任务调度的支持异步执行和任务调度底层的语言支撑都是Java 的多线程技术。线程是系统进行独立运行和调度的基本单位。拥有了多线程，系统就拥有了同时处理多项任务的能力。Java 实现异步调用在Java 中要实现多线程有实现Runnable 接口和扩展Thread 类两种方式。只要将需要异步执行的任务放在run() 方法中，在主线程中启动要执行任务的子线程就可以实现任务的异步执行。如果需要实现基于时间点触发的任务调度，就需要在子线程中循环的检查系统当前的时间跟触发条件是否一致，然后触发任务的执行。该内容属于Java 多线程的基础知识，此处略过不讲。Java Timer 和 TimeTask 实现任务调度为了便于开发者快速地实现任务调度，Java JDK 对任务调度的功能进行了封装，实现了Timer 和TimerTask 两个工具类。TimerTask 类由上图，我们可以看出TimeTask 抽象类在实现Runnable 接口的基础上增加了任务cancel() 和任务scheduledExecuttionTime() 两个方法。Timer 类上图为调度类Timer 的实现。从Timer类的源码，可以看到其采用TaskQueue 来实现对多个TimeTask 的管理。TimerThread 集成自Thread 类，其mainLoop() 用来对任务进行调度。而Timer 类提供了四种重载的schedule() 方法和重载了两种sheduleAtFixedRate() 方法来实现几种基本的任务调度类型。下面的代码是采用Timer 实现的定时系统时间打印程序。 1234567891011public class PrintTimeTask extends TimerTask { @Override public void run() { System.out.println(new Date().toString()); } public static void main(String[] args) { Timer timer = new Timer(\"hello\"); timer.schedule(new PrintTimeTask(), 1000L, 2000L); }} Spring 4.x 中的异步执行和任务调度Spring 4.x 中的异步执行Spring 作为一站式框架，为开发者提供了异步执行和任务调度的抽象接口TaskExecutor 和TaskScheduler。Spring 对这些接口的实现类支持线程池(Thread Pool) 和代理。Spring 提供了对JDK 中Timer和开源的流行任务调度框架Quartz的支持。Spring 通过将关联的Schedule 转化为FactoryBean 来实现。通过Spring 调度框架，开发者可以快速地通过MethodInvokingFactoryBean 来实现将POJO 类的方法转化为任务。Spring TaskExecutorTaskExecutor 接口扩展自java.util.concurrent.Executor 接口。TaskExecutor 被创建来为其他组件提供线程池调用的抽象。ThreadPoolTaskExecutor 是TaskExecutor 的最主要实现类之一。该类的核心继承关系如下图所示。ThreadPooltaskexecutor 类ThreadPoolTaskExecutor 接口扩展了重多的接口，让其具备了更多的能力。要实现异步需要标注@Async 注解：AsyncTaskExecutor 增加了返回结果为Future 的submit() 方法，该方法的参数为Callable 接口。相比Runnable 接口，多了将执行结果返回的功能。AsyncListenableTaskExecutor 接口允许返回拥有回调功能的ListenableFuture 接口，这样在结果执行完毕是，能够直接回调处理。 123456789101112131415161718192021222324252627282930public class ListenableTask { @Async public ListenableFuture&lt;Integer&gt; compute(int n) { int sum = 0; for (int i = 0; i &lt; n; i++) { sum += i; } return new AsyncResult&lt;&gt;(sum); } static class CallBackImpl implements ListenableFutureCallback&lt;Integer&gt; { @Override public void onFailure(Throwable ex) { System.out.println(ex.getMessage()); } @Override public void onSuccess(Integer result) { System.out.println(result); } } public static void main(String[] args) { ListenableTask listenableTask = new ListenableTask(); ListenableFuture&lt;Integer&gt; listenableFuture = listenableTask.compute(10); listenableFuture.addCallback(new CallBackImpl()); }} ThreadFactory 定义了创建线程的工厂方法，可以扩展该方法实现对Thread 的改造。基于Java Config基于注解 当采用基于Java Config 注解配置时，只需要在主配置添加@EnableAsync 注解，Spring 会自动的创建基于ThreadPoolTaskExecutor 实例注入到上下文中。 1234@Configuration@EnableAsyncpublic class AppConfig {} 基于AsyncConfigurer接口自定义 开发者可以自定义Executor 的类型，并且注册异常处理器。 123456789101112131415161718192021@Configurationpublic class TaskConfig implements AsyncConfigurer { @Override public Executor getAsyncExecutor() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setMaxPoolSize(100); executor.setCorePoolSize(10); return executor; } @Override public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() { return new AsyncUncaughtExceptionHandler() { @Override public void handleUncaughtException(Throwable ex, Method method, Object... params) { System.out.println(ex.getMessage()); } }; }} 基于XML Config基于传统XML的配置 基于XML 的形式，采用传统的Java Bean的形式配置ThreadPoolTaskExecutor。然后采用自动注入(autowire, resource,name)的可以直接在Spring Component 中注入Executor。以编程的形式实现异步任务。 123456&lt;bean id=\"taskExecutor\" class=\"org.springframework.scheduling.concurrent. ThreadPoolTaskExecutor\"&gt; &lt;property name=\"corePoolSize\" value=\"5\" /&gt; &lt;property name=\"maxPoolSize\" value=\"10\" /&gt; &lt;property name=\"queueCapacity\" value=\"25\" /&gt;&lt;/bean&gt; 基于task 命名空间的配置 Spring 为任务的执行提供了便利的task 命名空间。当采用基于XML 配置时Spring 会自动地为开发者创建Executor。同时可以在annotation-driven 标签上注册实现了AsyncUncaughtExceptionHandler 接口的异常处理器。 123&lt;!-- config exception handler --&gt;&lt;bean id=\"taskAsyncExceptionHandler\" class=\"org.zzy.spring4.application.schedulie.TaskAsyncExceptionHandler\"/&gt;&lt;task:annotation-driven exception-handler=\"taskAsyncExceptionHandler\" scheduler=\"scheduler\" executor=\"executor\"/&gt; 异步执行的异常处理除了上文提到的两种异常处理方式，Spring 还提供了基于SimpleApplicationEventMulticaster 类的异常处理方式。 123456789101112@Beanpublic SimpleApplicationEventMulticaster eventMulticaster(TaskExecutor taskExecutor) { SimpleApplicationEventMulticaster eventMulticaster = new SimpleApplicationEventMulticaster(); eventMulticaster.setTaskExecutor(taskExecutor); eventMulticaster.setErrorHandler(new ErrorHandler() { @Override public void handleError(Throwable t) { System.out.println(t.getMessage()); } }); return eventMulticaster;} Spring 4.x 中任务调度实现Spring 的任务调度主要基于TaskScheduler 接口。ThreadPoolTaskScheduler 是Spring 任务调度的核心实现类。该类提供了大量的重载方法进行任务调度。Trigger 定义了任务被执行的触发条件。Spring 提供了基于Corn 表达式的CornTrigger实现。TaskScheduler 如下图所示。 ThreadPoolTaskExecutor 类实现TaskScheduler 接口的ThreadPoolTaskExecutor 继承关系。 ThreadPoolTaskExecutor 类基于Java Config基于注解的配置 当采用基于Java Config 注解配置时，只需要在主配置添加@EnableScheduling 注解，Spring 会自动的创建基于ThreadPoolTaskExecutor 实例注入到上下文中。 123456789101112131415161718@Configuration@EnableSchedulingpublic class AppConfig {}基于SchedulingConfigurer接口自定义@Configurationpublic class ScheduleConfig implements SchedulingConfigurer { @Override public void configureTasks(ScheduledTaskRegistrar taskRegistrar) { taskRegistrar.setTaskScheduler(new ThreadPoolTaskScheduler()); taskRegistrar.getScheduler().schedule(new Runnable() { @Override public void run() { System.out.println(\"hello\"); } }, new CronTrigger(\"0 15 9-17 * * MON-FRI\")); }} 基于XML Config&lt;task:annotation-driven scheduler=”myScheduler”/&gt;&lt;task:scheduler id=”myScheduler” pool-size=”10”/&gt;@Scheduled 注解的使用当某个Bean 由Spring 管理生命周期时，就可以方便的使用@Shcheduled 注解将该Bean 的方法准换为基于任务调度的策略。 123456789@Scheduled(initialDelay=1000, fixedRate=5000)public void doSomething() { // something that should execute periodically}@Scheduled(cron=\"*/5 * * * * MON-FRI\")public void doSomething() { // something that should execute on weekdays only} task 命名空间中的task:scheduled-tasks该元素能够实现快速地将一个普通Bean 的方法转换为Scheduled 任务的途径。具体如下： 123456&lt;task:scheduled-tasks scheduler=\"myScheduler\"&gt; &lt;task:scheduled ref=\"beanA\" method=\"methodA\" fixed-delay=\"5000\" initial-delay=\"1000\"/&gt; &lt;task:scheduled ref=\"beanB\" method=\"methodB\" fixed-rate=\"5000\"/&gt; &lt;task:scheduled ref=\"beanC\" method=\"methodC\" cron=\"*/5 * * * * MON-FRI\"/&gt;&lt;/task:scheduled-tasks&gt;&lt;task:scheduler id=\"myScheduler\" pool-size=\"10\"/&gt; 总结本文着重介绍了JDK 为任务调度提供的基础类Timer。并在此基础上详细介绍了Spring 4.x 的异步执行和任务调度的底层接口设计。并针对常用的模式进行了讲解，并附带了源代码。第三方开源的Quartz 实现了更为强大的任务调度系统，Spring 也对集成Quartz 提供了转换。之后会择机再详细的介绍Quartz 的应用和设计原理。同时，Servlet 3.x 为Web 的异步调用提供了AsyncContext，对基于Web 的异步调用提供了原生的支持，后续的文章也会对此有相应的介绍。","link":"/2019/07/17/spring-task-scheduled/"},{"title":"springboot jdbctemplate 分页查询","text":"https://blog.csdn.net/iamcnnetiger/article/details/8028159","link":"/2019/09/06/springboot-jdbctemplate-分页查询/"},{"title":"springboot+jpa2","text":"这里以用户表User作为基本的操作数据库的实体类为例。 导入pom.xml的依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt; 如果你是jdk8或者10，可能会遇到一个问题，javax/xml/bind/JAXBException，具体查看这篇文章解决javax/xml/bind/JAXBException 编写一个实体类（bean）和数据表进行映射，并且配置好映射关系 1234567891011121314151617181920212223242526272829303132333435/** * @author 欧阳思海 * @date 2018/7/26 16:09 *///使用JPA注解配置映射关系@Entity //告诉JPA这是一个实体类（和数据表映射的类）@Table(name = \"user\") //@Table来指定和哪个数据表对应;如果省略默认表名就是user；public class User { @Id //这是一个主键 @GeneratedValue(strategy = GenerationType.IDENTITY)//自增主键 private Integer id; @Column(name = \"username\",length = 50) //这是和数据表对应的一个列 private String username; @Column(name = \"password\",length = 50) //这是和数据表对应的一个列 private String password; public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public String getPassword() { return password; } public void setPassword(String password) { this.password = password; }} 关于注解的意义，在上面的注释中都注明了。如果要查看更加详细的解释，查看我的另外的文章。 编写一个Dao接口来操作实体类对应的数据表（Repository） 123456/** * @author 欧阳思海 * @date 2018/7/27 9:39 */public interface UserDao extends JpaRepository&lt;User,Integer&gt; {} 解释：这个JpaRepository里面已经包括了基本的crud方法，我们可以看一下源代码。 我们可以发现这个借口，已经实现了我们需要的基本的crud，给我们带来了很大的方便。 我们再看看他的整体的类的结构 事实上JpaRepository继承自PagingAndSortingRepository，我们也可以很方便的实现分页的功能，这样对于我们开发来说还是省了不少事。 数据源application.properties配置 1234spring.datasource.url=jdbc:mysql://localhost:3306/testspring.datasource.username=rootspring.datasource.password=0911SIHAIspring.datasource.driver-class-name=com.mysql.jdbc.Driver 创建service进行简单的测试 12345678910111213141516171819202122232425262728293031/** * @author 欧阳思海 * @date 2018/7/26 16:11 */@Servicepublic class UserServiceImpl implements UserService { @Autowired private UserDao userDao; @Override public void add(String username, String password) { User user = new User(); user.setPassword(password); user.setUsername(username); userDao.save(user); } @Override public void deleteByName(String userName) { User user = new User(); user.setUsername(userName); userDao.delete(user); } @Override public List&lt;User&gt; getAllUsers() { List&lt;User&gt; users = userDao.findAll(); return users; }} 测试类 12345678910111213141516171819202122232425262728293031/** * @author 欧阳思海 * @date 2018/7/26 16:22 */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserTest { @Autowired private UserService userService; @Test public void testAdd(){ userService.add(\"sihai\",\"abc\"); userService.add(\"yan\",\"abc\"); } @Test public void testDelete(){ userService.deleteByName(\"sihai\"); } @Test public void testQuery(){ List&lt;User&gt; users = userService.getAllUsers(); Assert.assertEquals(2, users.size()); }} 到这里关于springboot整合jpa的基本操作已经完成了，关于更加复杂的jpa的操作可以查看jpa官网，或者查看jpa简单教程 后续也会有jpa的相关教程，敬请查看!","link":"/2019/08/31/springboot-jpa2/"},{"title":"springboot aop","text":"【SpringBoot】SpingBoot整合AOPSpring的AOP是通过JDK的动态代理和CGLIB实现的。 说起spring，我们知道其最核心的两个功能就是AOP（面向切面）和IOC（控制反转），这边文章来总结一下SpringBoot如何整合使用AOP。 一、AOP的术语：aop 有一堆术语，非常难以理解，简单说一下 通知(有的地方叫增强)(Advice)需要完成的工作叫做通知，就是你写的业务逻辑中需要比如事务、日志等先定义好，然后需要的地方再去用 连接点(Join point)就是spring中允许使用通知的地方，基本上每个方法前后抛异常时都可以是连接点 切点(Poincut)其实就是筛选出的连接点，一个类中的所有方法都是连接点，但又不全需要，会筛选出某些作为连接点做为切点。如果说通知定义了切面的动作或者执行时机的话，切点则定义了执行的地点 切面(Aspect)其实就是通知和切点的结合，通知和切点共同定义了切面的全部内容，它是干什么的，什么时候在哪执行引入(Introduction)在不改变一个现有类代码的情况下，为该类添加属性和方法,可以在无需修改现有类的前提下，让它们具有新的行为和状态。其实就是把切面（也就是新方法属性：通知定义的）用到目标类中去 目标(target)被通知的对象。也就是需要加入额外代码的对象，也就是真正的业务逻辑被组织织入切面。 织入(Weaving)把切面加入程序代码的过程。切面在指定的连接点被织入到目标对象中，在目标对象的生命周期里有多个点可以进行织入： 编译期：切面在目标类编译时被织入，这种方式需要特殊的编译器类加载期：切面在目标类加载到JVM时被织入，这种方式需要特殊的类加载器，它可以在目标类被引入应用之前增强该目标类的字节码运行期：切面在应用运行的某个时刻被织入，一般情况下，在织入切面时，AOP容器会为目标对象动态创建一个代理对象，Spring AOP就是以这种方式织入切面的。 例： 12345public class UserService{ void save(){} List list(){} ....} 在UserService中的save()方法前需要开启事务，在方法后关闭事务，在抛异常时回滚事务。那么,UserService中的所有方法都是连接点(JoinPoint),save()方法就是切点(Poincut)。需要在save()方法前后执行的方法就是通知(Advice)，切点和通知合起来就是一个切面(Aspect)。save()方法就是目标(target)。把想要执行的代码动态的加入到save()方法前后就是织入(Weaving)。有的地方把通知称作增强是有道理的，在业务方法前后加上其它方法，其实就是对该方法的增强。 二、常用AOP通知(增强)类型 before(前置通知)： 在方法开始执行前执行after(后置通知)： 在方法执行后执行afterReturning(返回后通知)： 在方法返回后执行afterThrowing(异常通知)： 在抛出异常时执行around(环绕通知)： 在方法执行前和执行后都会执行 三、执行顺序around &gt; before &gt; around &gt; after &gt; afterReturning 四、先说一下SpringAop非常霸道又用的非常少的功能 –引入(Introduction) 配置类: 12345678@Aspect@Componentpublic class IntroductionAop { @DeclareParents(value = \"com.jiuxian..service..*\", defaultImpl = DoSthServiceImpl.class) public DoSthService doSthService;} service代码: 12345678910111213141516171819202122232425262728public interface DoSthService { void doSth();}@Servicepublic class DoSthServiceImpl implements DoSthService { @Override public void doSth() { System.out.println(\"do sth ....\"); } }public interface UserService { void testIntroduction();}@Servicepublic class UserServiceImpl implements UserService { @Override public void testIntroduction() { System.out.println(\"do testIntroduction\"); }} 测试代码 1234567@Testpublic void testIntroduction() { userService.testIntroduction(); //Aop 让UserService方法拥有 DoSthService的方法 DoSthService doSthService = (DoSthService) userService; doSthService.doSth();} 结果 12do testIntroductiondo sth .... 五、五种通知（增强）代码实现 配置类(1) 对方法 1234567891011121314151617181920212223242526272829303132333435363738394041@Aspect@Componentpublic class TransactionAop { @Pointcut(\"execution(* com.jiuxian..service.*.*(..))\") public void pointcut() { } @Before(\"pointcut()\") public void beginTransaction() { System.out.println(\"before beginTransaction\"); } @After(\"pointcut()\") public void commit() { System.out.println(\"after commit\"); } @AfterReturning(\"pointcut()\", returning = \"returnObject\") public void afterReturning(JoinPoint joinPoint, Object returnObject) { System.out.println(\"afterReturning\"); } @AfterThrowing(\"pointcut()\") public void afterThrowing() { System.out.println(\"afterThrowing afterThrowing rollback\"); } @Around(\"pointcut()\") public Object around(ProceedingJoinPoint joinPoint) throws Throwable { try { System.out.println(\"around\"); return joinPoint.proceed(); } catch (Throwable e) { e.printStackTrace(); throw e; } finally { System.out.println(\"around\"); } }} (2) 对注解 12345@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface Log { String value() default \"\";} 1234567891011121314151617181920212223242526272829@Aspect@Componentpublic class AnnotationAop { @Pointcut(value = \"@annotation(log)\", argNames = \"log\") public void pointcut(Log log) { } @Around(value = \"pointcut(log)\", argNames = \"joinPoint,log\") public Object around(ProceedingJoinPoint joinPoint, Log log) throws Throwable { try { System.out.println(log.value()); System.out.println(\"around\"); return joinPoint.proceed(); } catch (Throwable throwable) { throw throwable; } finally { System.out.println(\"around\"); } }} @Before(\"@annotation(com.jiuxian.annotation.Log)\") public void before(JoinPoint joinPoint) { MethodSignature signature = (MethodSignature) joinPoint.getSignature(); Method method = signature.getMethod(); Log log = method.getAnnotation(Log.class); System.out.println(\"注解式拦截 \" + log.value()); } service 方法实现 1234567891011121314151617181920212223242526public interface UserService { String save(String user); void testAnnotationAop();}@Servicepublic class UserServiceImpl implements UserService { @Override public String save(String user) { System.out.println(\"保存用户信息\"); if (\"a\".equals(user)) { throw new RuntimeException(); } return user; } @Log(value = \"test\") @Override public void testAnnotationAop() { System.out.println(\"testAnnotationAop\"); }} 测试类 1234567891011121314151617181920212223@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringbootAopApplicationTests { @Resource private UserService userService; @Test public void testAop1() { userService.save(\"张三\"); Assert.assertTrue(true); } @Test public void testAop2() { userService.save(\"a\"); } @Test public void testAop3() { userService.testAnnotationAop(); }} 结果执行testAop1时 123456aroundbefore beginTransaction保存用户信息aroundafter commitafterReturning :: 张三 执行testAop2时 123456aroundbefore beginTransaction保存用户信息aroundafter commitafterThrowing rollback 执行testAop3时 1234testaroundtestAnnotationAoparound pom文件 123456789101112131415 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 六、最常用的execution解释 例: execution(* com.jiuxian..service.*.*(..)) execution 表达式的主体第一个* 代表任意的返回值com.jiuxian aop所横切的包名包后面.. 表示当前包及其子包第二个* 表示类名，代表所有类.*(..) 表示任何方法,括号代表参数 .. 表示任意参数 例:execution(* com.jiuxian..service.*Service.add*(String)) 表示： com.jiuxian 包及其子包下的service包下，类名以Service结尾，方法以add开头，参数类型为String的方法的切点。 七、特别的用法 12345678@Pointcut(\"execution(public * *(..))\")private void anyPublicOperation() {} @Pointcut(\"within(com.xyz.someapp.trading..*)\")private void inTrading() {} @Pointcut(\"anyPublicOperation() &amp;&amp; inTrading()\")private void tradingOperation() {} 可以使用 &amp;&amp;, ||, ! 运算符来定义切点 八、更多详细介绍请参阅官网springAOP官网地址 一、示例应用场景：对所有的web请求做切面来记录日志。 1、pom中引入SpringBoot的web模块和使用AOP相关的依赖： 12345678910111213141516171819 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjrt&lt;/artifactId&gt; &lt;version&gt;1.6.11&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.6.11&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt;&lt;/dependency&gt; 其中：cglib包是用来动态代理用的,基于类的代理；aspectjrt和aspectjweaver是与aspectj相关的包,用来支持切面编程的；aspectjrt包是aspectj的runtime包；aspectjweaver是aspectj的织入包； 2、实现一个简单的web请求入口（实现传入name参数，返回“hello xxx”的功能）： 注意：在完成了引入AOP依赖包后，一般来说并不需要去做其他配置。使用过Spring注解配置方式的人会问是否需要在程序主类中增加@EnableAspectJAutoProxy来启用，实际并不需要。因为在AOP的默认配置属性中，spring.aop.auto属性默认是开启的，也就是说只要引入了AOP依赖后，默认已经增加了@EnableAspectJAutoProxy。 3、定义切面类，实现web层的日志切面 ** 要想把一个类变成切面类，需要两步 ** ① 在类上使用 @Component 注解 把切面类加入到IOC容器中② 在类上使用 @Aspect 注解 使之成为切面类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.example.aop;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Component;import org.springframework.web.context.request.RequestContextHolder;import org.springframework.web.context.request.ServletRequestAttributes;import javax.servlet.http.HttpServletRequest;import java.util.Arrays;/** * Created by lmb on 2018/9/5. */@Aspect@Componentpublic class WebLogAcpect { private Logger logger = LoggerFactory.getLogger(WebLogAcpect.class); /** * 定义切入点，切入点为com.example.aop下的所有函数 */ @Pointcut(\"execution(public * com.example.aop..*.*(..))\") public void webLog(){} /** * 前置通知：在连接点之前执行的通知 * @param joinPoint * @throws Throwable */ @Before(\"webLog()\") public void doBefore(JoinPoint joinPoint) throws Throwable { // 接收到请求，记录请求内容 ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = attributes.getRequest(); // 记录下请求内容 logger.info(\"URL : \" + request.getRequestURL().toString()); logger.info(\"HTTP_METHOD : \" + request.getMethod()); logger.info(\"IP : \" + request.getRemoteAddr()); logger.info(\"CLASS_METHOD : \" + joinPoint.getSignature().getDeclaringTypeName() + \".\" + joinPoint.getSignature().getName()); logger.info(\"ARGS : \" + Arrays.toString(joinPoint.getArgs())); } @AfterReturning(returning = \"ret\",pointcut = \"webLog()\") public void doAfterReturning(Object ret) throws Throwable { // 处理完请求，返回内容 logger.info(\"RESPONSE : \" + ret); }} 以上的切面类通过 @Pointcut定义的切入点为com.example.aop包下的所有函数做切入，通过 @Before实现切入点的前置通知，通过 @AfterReturning记录请求返回的对象。访问http://localhost:8004/hello?name=lmb得到控制台输出如下： https://github.com/LiuMengBing/demo-web/tree/master/src/main/java/com/example/aop二、AOP支持的通知 1、前置通知@Before：在某连接点之前执行的通知，除非抛出一个异常，否则这个通知不能阻止连接点之前的执行流程。 123456789101112131415161718192021222324252627282930313233343536373839/** * 前置通知，方法调用前被调用 * @param joinPoint/null */ @Before(value = POINT_CUT)public void before(JoinPoint joinPoint){ logger.info(\"前置通知\"); //获取目标方法的参数信息 Object[] obj = joinPoint.getArgs(); //AOP代理类的信息 joinPoint.getThis(); //代理的目标对象 joinPoint.getTarget(); //用的最多 通知的签名 Signature signature = joinPoint.getSignature(); //代理的是哪一个方法 logger.info(\"代理的是哪一个方法\"+signature.getName()); //AOP代理类的名字 logger.info(\"AOP代理类的名字\"+signature.getDeclaringTypeName()); //AOP代理类的类（class）信息 signature.getDeclaringType(); //获取RequestAttributes RequestAttributes requestAttributes = RequestContextHolder.getRequestAttributes(); //从获取RequestAttributes中获取HttpServletRequest的信息 HttpServletRequest request = (HttpServletRequest) requestAttributes.resolveReference(RequestAttributes.REFERENCE_REQUEST); //如果要获取Session信息的话，可以这样写： //HttpSession session = (HttpSession) requestAttributes.resolveReference(RequestAttributes.REFERENCE_SESSION); //获取请求参数 Enumeration&lt;String&gt; enumeration = request.getParameterNames(); Map&lt;String,String&gt; parameterMap = Maps.newHashMap(); while (enumeration.hasMoreElements()){ String parameter = enumeration.nextElement(); parameterMap.put(parameter,request.getParameter(parameter)); } String str = JSON.toJSONString(parameterMap); if(obj.length &gt; 0) { logger.info(\"请求的参数信息为：\"+str); } } 注意：这里用到了JoinPoint和RequestContextHolder。1）通过JoinPoint可以获得通知的签名信息，如目标方法名、目标方法参数信息等；2）通过RequestContextHolder来获取请求信息，Session信息； 2、后置通知@AfterReturning：在某连接点之后执行的通知，通常在一个匹配的方法返回的时候执行（可以在后置通知中绑定返回值）。 12345678910111213141516171819/** * 后置返回通知 * 这里需要注意的是: * 如果参数中的第一个参数为JoinPoint，则第二个参数为返回值的信息 * 如果参数中的第一个参数不为JoinPoint，则第一个参数为returning中对应的参数 * returning：限定了只有目标方法返回值与通知方法相应参数类型时才能执行后置返回通知，否则不执行， * 对于returning对应的通知方法参数为Object类型将匹配任何目标返回值 * @param joinPoint * @param keys */ @AfterReturning(value = POINT_CUT,returning = \"keys\") public void doAfterReturningAdvice1(JoinPoint joinPoint,Object keys){ logger.info(\"第一个后置返回通知的返回值：\"+keys); } @AfterReturning(value = POINT_CUT,returning = \"keys\",argNames = \"keys\") public void doAfterReturningAdvice2(String keys){ logger.info(\"第二个后置返回通知的返回值：\"+keys); } 3、后置异常通知@AfterThrowing：在方法抛出异常退出时执行的通知。 12345678910111213141516/** * 后置异常通知 * 定义一个名字，该名字用于匹配通知实现方法的一个参数名，当目标方法抛出异常返回后，将把目标方法抛出的异常传给通知方法； * throwing:限定了只有目标方法抛出的异常与通知方法相应参数异常类型时才能执行后置异常通知，否则不执行， * 对于throwing对应的通知方法参数为Throwable类型将匹配任何异常。 * @param joinPoint * @param exception */ @AfterThrowing(value = POINT_CUT,throwing = \"exception\") public void doAfterThrowingAdvice(JoinPoint joinPoint,Throwable exception){ //目标方法名： logger.info(joinPoint.getSignature().getName()); if(exception instanceof NullPointerException){ logger.info(\"发生了空指针异常!!!!!\"); } } 4、后置最终通知@After：当某连接点退出时执行的通知（不论是正常返回还是异常退出）。 12345678/** * 后置最终通知（目标方法只要执行完了就会执行后置通知方法） * @param joinPoint */ @After(value = POINT_CUT) public void doAfterAdvice(JoinPoint joinPoint){ logger.info(\"后置最终通知执行了!!!!\"); } 5、环绕通知@Around：包围一个连接点的通知，如方法调用等。这是最强大的一种通知类型。环绕通知可以在方法调用前后完成自定义的行为，它也会选择是否继续执行连接点或者直接返回它自己的返回值或抛出异常来结束执行。 环绕通知最强大，也最麻烦，是一个对方法的环绕，具体方法会通过代理传递到切面中去，切面中可选择执行方法与否，执行几次方法等。环绕通知使用一个代理ProceedingJoinPoint类型的对象来管理目标对象，所以此通知的第一个参数必须是ProceedingJoinPoint类型。在通知体内调用ProceedingJoinPoint的proceed()方法会导致后台的连接点方法执行。proceed()方法也可能会被调用并且传入一个Object[]对象，该数组中的值将被作为方法执行时的入参。 12345678910111213141516/** * 环绕通知： * 环绕通知非常强大，可以决定目标方法是否执行，什么时候执行，执行时是否需要替换方法参数，执行完毕是否需要替换返回值。 * 环绕通知第一个参数必须是org.aspectj.lang.ProceedingJoinPoint类型 */ @Around(value = POINT_CUT) public Object doAroundAdvice(ProceedingJoinPoint proceedingJoinPoint){ logger.info(\"环绕通知的目标方法名：\"+proceedingJoinPoint.getSignature().getName()); try { Object obj = proceedingJoinPoint.proceed(); return obj; } catch (Throwable throwable) { throwable.printStackTrace(); } return null; } 6、有时候我们定义切面的时候，切面中需要使用到目标对象的某个参数，如何使切面能得到目标对象的参数呢？可以使用args来绑定。如果在一个args表达式中应该使用类型名字的地方使用一个参数名字，那么当通知执行的时候对象的参数值将会被传递进来。 12345@Before(\"execution(* findById*(..)) &amp;&amp;\" + \"args(id,..)\") public void twiceAsOld1(Long id){ System.err.println (\"切面before执行了。。。。id==\" + id); } ** 注意：任何通知方法都可以将第一个参数定义为org.aspectj.lang.JoinPoint类型（环绕通知需要定义第一个参数为ProceedingJoinPoint类型，它是 JoinPoint 的一个子类）。 ** JoinPoint接口提供了一系列有用的方法，比如: [] getArgs()（返回方法参数）、 [] getThis()（返回代理对象）、 [] getTarget()（返回目标）、 [] getSignature()（返回正在被通知的方法相关信息）和 [] toString()（打印出正在被通知的方法的有用信息）。 三、切入点表达式定义切入点的时候需要一个包含名字和任意参数的签名，还有一个切入点表达式，如: execution(public * com.example.aop…(..))** 切入点表达式的格式：execution([可见性]返回类型[声明类型].方法名(参数)[异常]) ** 其中[]内的是可选的，其它的还支持通配符的使用： 1) *：匹配所有字符2) ..：一般用于匹配多个包，多个参数3) +：表示类及其子类4)运算符有：&amp;&amp;,||,! 切入点表达式关键词用例： 12345678910111213141516171819202122232425262728293031323334353637383940414243441）execution：用于匹配子表达式。 //匹配com.cjm.model包及其子包中所有类中的所有方法，返回类型任意，方法参数任意 @Pointcut(“execution(* com.cjm.model...(..))”) public void before(){}2）within：用于匹配连接点所在的Java类或者包。 //匹配Person类中的所有方法 @Pointcut(“within(com.cjm.model.Person)”) public void before(){} //匹配com.cjm包及其子包中所有类中的所有方法 @Pointcut(“within(com.cjm..*)”) public void before(){}3） this：用于向通知方法中传入代理对象的引用。 @Before(“before() &amp;&amp; this(proxy)”) public void beforeAdvide(JoinPoint point, Object proxy){ //处理逻辑 }4）target：用于向通知方法中传入目标对象的引用。 @Before(“before() &amp;&amp; target(target) public void beforeAdvide(JoinPoint point, Object proxy){ //处理逻辑 }5）args：用于将参数传入到通知方法中。 @Before(“before() &amp;&amp; args(age,username)”) public void beforeAdvide(JoinPoint point, int age, String username){ //处理逻辑 }6）@within ：用于匹配在类一级使用了参数确定的注解的类，其所有方法都将被匹配。 @Pointcut(“@within(com.cjm.annotation.AdviceAnnotation)”) － 所有被@AdviceAnnotation标注的类都将匹配 public void before(){}7）@target ：和@within的功能类似，但必须要指定注解接口的保留策略为RUNTIME。 @Pointcut(“@target(com.cjm.annotation.AdviceAnnotation)”) public void before(){}8）@args ：传入连接点的对象对应的Java类必须被@args指定的Annotation注解标注。 @Before(“@args(com.cjm.annotation.AdviceAnnotation)”) public void beforeAdvide(JoinPoint point){ //处理逻辑 }9）@annotation ：匹配连接点被它参数指定的Annotation注解的方法。也就是说，所有被指定注解标注的方法都将匹配。 @Pointcut(“@annotation(com.cjm.annotation.AdviceAnnotation)”) public void before(){}10）bean：通过受管Bean的名字来限定连接点所在的Bean。该关键词是Spring2.5新增的。 @Pointcut(“bean(person)”) public void before(){}","link":"/2019/07/15/springboot-aop/"},{"title":"springboot+jpa","text":"SpringBoot+JPA进行增删改查 最近公司领导要求让用SpringBoot+JPA进行项目开发。所以安排我搭建项目框架。第一步.POM文件的配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.4.1.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;!--去除 spring-boot-starter-web 自带的下方jar包--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;log4j-over-slf4j&lt;/artifactId&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;org.springframework.boot&lt;/artifactId&gt; &lt;groupId&gt;spring-boot-starter-logging&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!--jpa 相关jar包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- MySql 相关JAR包 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--spring-boot整合redis包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--添加spring整合shiro的jar包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--JSON.toJSONString()依赖的JSON jar包--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.37&lt;/version&gt; &lt;/dependency&gt; &lt;!-- httpClient的jar包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--log4j依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;!--log4j2日志依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 热部署的jar包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 添加SpringBoot热部署插件 --&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin &lt;/artifactId&gt; &lt;dependencies&gt; &lt;!--springloaded hot deploy --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;version&gt;1.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 第二步骤.application.properties配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253spring.datasource.driver-class-name=com.mysql.jdbc.Driverspring.datasource.url=jdbc:mysql://localhost:3306/testspring.datasource.username=rootspring.datasource.password=admin spring.jpa.hibernate.ddl-auto=updatespring.jpa.show-sql=truespring.jackson.serialization.indent_output=true#数据库方言spring.jpa.database-platform=org.hibernate.dialect.MySQL5Dialect######################################################## ###Redis (Redis配置)######################################################## #设置使用哪一个redis数据库【redis默认有16个数据库0-15】 spring.redis.database=1 #redis主机IP spring.redis.host=127.0.0.1 #redis端口 spring.redis.port=6379 #redis密码 #spring.redis.password= spring.redis.pool.max-idle=8 spring.redis.pool.min-idle=0 spring.redis.pool.max-active=8 spring.redis.pool.max-wait=-1 #设置客户端闲置5s后关闭连接 spring.redis.timeout=5000 第三步.创建实体类。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import javax.persistence.Column;import javax.persistence.Entity;import javax.persistence.GeneratedValue;import javax.persistence.Id;import javax.persistence.Table; @Entity@Table(name=\"user2\")public class User2 { @Id @GeneratedValue private Integer id; @Column(name=\"name\") private String name; @Column(name=\"password\") private String password; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } public User2(String name,String password) { this.password = password; this.name = name; } @Override public String toString() { return \"User2 [id=\" + id + \", name=\" + name + \", password=\" + password + \"]\"; } public User2() { }} 第四步。创建Dao层集成JpaRepository 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.util.List; import org.springframework.data.domain.Page;import org.springframework.data.domain.Pageable;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.Modifying;import org.springframework.data.jpa.repository.Query; public interface User2JpaDao extends JpaRepository&lt;User2, Long&gt; { /* * Jpa命名规范，查询 */ User2 findByNameAndPasswordAndId(String name,String password,Integer id); /* * (non-Javadoc)新增用户 * @see org.springframework.data.repository.CrudRepository#save(S) */ User2 save(User2 user2); //查询全部 List&lt;User2&gt; findAll(); //分页查询 //Page&lt;User2&gt; findAll(PageRequest pageRequest); Page&lt;User2&gt; findAll(Pageable pageable); @Modifying @Query(\"update User2 as c set c.name = ?1 where c.password=?2\") int updateNameByPassword(String name, String password); void delete(User2 entity); } 第五步。创建Service层。 123456789101112131415161718192021222324252627282930313233import java.util.List; import org.springframework.data.domain.Page;import org.springframework.data.domain.Pageable; public interface User2Service { User2 findByNameAndPasswordAndId(String name,String password,Integer id); User2 save(User2 user2); List&lt;User2&gt; findAll(); int updateNameByPassword(String name, String password); //Page&lt;User2&gt; findAll(PageRequest pageRequest); Page&lt;User2&gt; findAll(Pageable pageable); //删除用户 void delete(User2 entity);} 第六步。创建ServiceImpl层。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import java.util.List; import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.domain.Page;import org.springframework.data.domain.Pageable;import org.springframework.stereotype.Service;@Servicepublic class User2ServiceImpl implements User2Service { @Autowired private User2JpaDao user2JpaDao; @Override public User2 findByNameAndPasswordAndId(String name, String password, Integer id) { // TODO Auto-generated method stub return user2JpaDao.findByNameAndPasswordAndId(name, password,id); } @Override public User2 save(User2 user2) { // TODO Auto-generated method stub return user2JpaDao.save(user2); } @Override public List&lt;User2&gt; findAll() { // TODO Auto-generated method stub return user2JpaDao.findAll(); } @Override public int updateNameByPassword(String name, String password) { // TODO Auto-generated method stub return user2JpaDao.updateNameByPassword(name, password); } @Override public void delete(User2 entity) { // TODO Auto-generated method stub user2JpaDao.delete(entity); } @Override public Page&lt;User2&gt; findAll(Pageable pageable) { // TODO Auto-generated method stub return user2JpaDao.findAll(pageable); }} 第七步 controller层 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159import java.util.ArrayList;import java.util.List; import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.domain.Page;import org.springframework.data.domain.PageRequest;import org.springframework.data.domain.Pageable;import org.springframework.data.domain.Sort;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController; @RestControllerpublic class User2Controller { @Autowired private User2Service user2Service; @Autowired private RedisService redisService; @RequestMapping(\"/getRedis\") public String getRedis(){ String string2=\"zouyunke111111\"; redisService.set(string2, \"nihao\"); String string=(String) redisService.get(\"zouyunke111111\"); return string; } @RequestMapping(\"/findByNameAndPasswordAndId\") public User2 findByNameAndPassword(){ User2 user2=user2Service.findByNameAndPasswordAndId(\"zouyunke\", \"yunke123\", 3); //User2 user2=user2Service.findByNameAndPwdAndId(\"zouyunke\", \"yunke123\",1); return user2; } @RequestMapping(\"/saveUser2\") public List&lt;User2&gt; saveUser2(){ List&lt;User2&gt; user3=new ArrayList&lt;User2&gt;(); for (int i=0;i&lt;10;i++){ User2 user2=new User2(\"zouyunke12\"+i,\"zouyunke12\"+i); user3.add(user2); } for (User2 user2 : user3) { User2 user=user2Service.save(user2); } //User2 user2=user2Service.findByNameAndPwdAndId(\"zouyunke\", \"yunke123\",1); return user2Service.findAll(); } @RequestMapping(\"/updateUser2\") public User2 updateUser2(){ User2 user2=new User2(); user2.setId(1); user2.setName(\"zhaodanya\"); user2.setPassword(\"123\"); return user2Service.save(user2); } /** * 分页查询 * @author Administrator * 2018年4月22日 * @return * TODO */ @RequestMapping(\"/deleteUser2\") public void deleteUser2(){ User2 user2=new User2(); user2.setId(3); user2.setName(\"zouyunke120\"); user2.setPassword(\"zouyunke120\"); user2Service.delete(user2); } @RequestMapping(value = \"/pageUser2\") public Page&lt;User2&gt; pageUser2() { Sort sort = new Sort(Sort.Direction.ASC,\"id\"); Pageable pageable = new PageRequest(2,6,sort); Page&lt;User2&gt; page = user2Service.findAll(pageable); return page; } } 至此，整合完毕。上面写的都是jpa的东西。再写一点和jpa无关的东西。自定义sql语句，由于项目的进展，需要一些自定义sql语句，所以封装了一个dao和一个实现类来自定义sql语句。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package cn.dao.base;import java.util.List;import java.util.Map; /*** 类说明: 原生SQL操作数据库的Dao* @author 邹运坷* @version 创建时间：2018年4月26日 下午12:10:26*/public interface BaseDao { /** * 公共查询方法【手动进行参数的拼接】 * @return */ List&lt;Map&gt; commonQueryMethod(String sql); /** * 增删改共用方法【手动进行参数的拼接】 * @param sql */ int addOrDelOrUpdateMethod(String sql); /** * 公共查询方法【传递参数集合自动绑定参数】参数集合中的key要和SQL中的命名参数名称一致 * @return */ List&lt;Map&gt; commonQueryMethod(String sql,Map&lt;String,Object&gt; param); /** * 增删改共用方法【传递参数集合自动绑定参数】参数集合中的key要和SQL中的命名参数名称一致 * @param sql */ int addOrDelOrUpdateMethod(String sql,Map&lt;String,Object&gt; param);} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161package cn.dao.base.impl; import java.util.List;import java.util.Map; import javax.persistence.EntityManager;import javax.persistence.PersistenceContext; import org.hibernate.Criteria;import org.hibernate.Query;import org.hibernate.Session;import org.springframework.stereotype.Repository; import cn.yuanyue.ycmusic.dao.base.BaseDao; /*** 类说明:BaseDao实现类* @author 邹运坷* @version 创建时间：2018年4月26日 下午12:13:18*/@Repositorypublic class BaseDaoImpl implements BaseDao { @PersistenceContext private EntityManager entityManager; /** * 获取HibernateSession */ private Session getHibernateSession() { //获取Hibernate中的Session Session hibernateSession = entityManager.unwrap(org.hibernate.Session.class); return hibernateSession; } /** * 公共查询方法【手动拼接参数到SQL中】 * @param sql */ @Override public List&lt;Map&gt; commonQueryMethod(String sql) { //执行SQL查询【设置返回结果为Map】 Query result = getHibernateSession().createSQLQuery(sql).setResultTransformer(Criteria.ALIAS_TO_ENTITY_MAP); return result.list(); } /** * 增删改共用方法【手动拼接参数到SQL中】 * @param sql */ @Override public int addOrDelOrUpdateMethod(String sql) { Query result = getHibernateSession().createSQLQuery(sql); int executeUpdate = result.executeUpdate(); return executeUpdate; } /** * 公共查询方法【传递参数集合自动绑定参数】参数集合中的key要和SQL中的命名参数名称一致 * select * from xx where id = :key * put(\"key\",'1') * @return */ @Override public List&lt;Map&gt; commonQueryMethod(String sql, Map&lt;String, Object&gt; param) { Query result = getHibernateSession().createSQLQuery(sql).setProperties(param).setResultTransformer(Criteria.ALIAS_TO_ENTITY_MAP); return result.list(); } /** * 增删改共用方法【传递参数集合自动绑定参数】参数集合中的key要和SQL中的命名参数名称一致 * @param sql */ @Override public int addOrDelOrUpdateMethod(String sql, Map&lt;String, Object&gt; param) { Query result = getHibernateSession().createSQLQuery(sql).setProperties(param); int executeUpdate = result.executeUpdate(); return executeUpdate; }} 上面就是自定义sql的源码。一个dao一个实现类。使用的时候只需要在serviceimpl里注入就行了。注入方式如下 1234@Autowiredprivate BaseDao baseDao;String sql=\"select name from user\";List&lt;Map&gt;result=baseDao.commonQueryMethod(sql); 这样就实现了自定义sql语句的问题。","link":"/2019/07/09/springboot-jpa/"},{"title":"springboot+mybatis","text":"SpringBoot整合Mybatis完整详细版 后来进了新公司，用不到而且忙于任务，今天重温一遍居然有些忘了，看来真是好记性不如烂笔头。于是写下本篇SpringBoot整合Mybatis的文章，做个笔记。 本章节主要搭建框架，下章节实现登录注册以及拦截器的配置：SpringBoot整合Mybatis完整详细版二：注册、登录、拦截器配置 本章项目源码下载：springBoot整合mybatis完整详细版 github地址：https://github.com/wjup/springBoot_Mybatis IDE：idea、DB：mysql 新建一个Spring Initializr项目 创建项目的文件结构以及jdk的版本选择项目所需要的依赖修改项目名，finish完成 来看下建好后的pom 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;demo&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; 修改配置文件本文不使用application.properties文件 而使用更加简洁的application.yml文件。将resource文件夹下原有的application.properties文件删除，创建application.yml配置文件（备注：其实SpringBoot底层会把application.yml文件解析为application.properties） 本文创建了两个yml文件（application.yml和application-dev.yml），分别来看一下内容 12345application.ymlspring: profiles: active: dev 12345678910111213141516171819202122application-dev.ymlserver: port: 8080 spring: datasource: username: root password: 1234 url: jdbc:mysql://localhost:3306/springboot?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=true&amp;serverTimezone=UTC driver-class-name: com.mysql.jdbc.Driver mybatis: mapper-locations: classpath:mapping/*Mapper.xml type-aliases-package: com.example.entity #showSqllogging: level: com: example: mapper : debug 两个文件的意思是： 在项目中配置多套环境的配置方法。因为现在一个项目有好多环境，开发环境，测试环境，准生产环境，生产环境，每个环境的参数不同，所以我们就可以把每个环境的参数配置到yml文件中，这样在想用哪个环境的时候只需要在主配置文件中将用的配置文件写上就行如application.yml 笔记：在Spring Boot中多环境配置文件名需要满足application-{profile}.yml的格式，其中{profile}对应你的环境标识，比如： application-dev.yml：开发环境 application-test.yml：测试环境 application-prod.yml：生产环境 至于哪个具体的配置文件会被加载，需要在application.yml文件中通过spring.profiles.active属性来设置，其值对应{profile}值。 还有配置文件中最好不要有中文注释，会报错。 解决方法（未测试）：spring boot application.yml文件中文注释乱码 接下来把启动文件移到com.example下，而且springboot的启动类不能放在java目录下！！！必须要个包将它包进去 否则会报错误： Your ApplicationContext is unlikely to start due to a @ComponentScan of the default package.这个原因值得注意就是因为有时候很难在IDEA中的项目目录认出来这个错误并且还容易扫描不到一些类，传送门：SpringBoot扫描不到controller 然后开始创建实体类实现业务流程创建包controller、entity、mapper、service。resources下创建mapping文件夹，用于写sql语句，也可以用注解的方式直接写在mapper文件里。下面直接贴代码 数据库表结构（之前小项目的表，直接拿来用） 1234567CREATE TABLE `user` ( `id` int(32) NOT NULL AUTO_INCREMENT, `userName` varchar(32) NOT NULL, `passWord` varchar(50) NOT NULL, `realName` varchar(32) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8; entity.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.example.entity; /** * @Author:wjup * @Date: 2018/9/26 0026 * @Time: 14:39 */public class User { private Integer id; private String userName; private String passWord; private String realName; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getUserName() { return userName; } public void setUserName(String userName) { this.userName = userName; } public String getPassWord() { return passWord; } public void setPassWord(String passWord) { this.passWord = passWord; } public String getRealName() { return realName; } public void setRealName(String realName) { this.realName = realName; } @Override public String toString() { return \"User{\" + \"id=\" + id + \", userName='\" + userName + '\\'' + \", passWord='\" + passWord + '\\'' + \", realName='\" + realName + '\\'' + '}'; }} UserController.java 12345678910111213141516171819202122232425262728package com.example.controller; import com.example.entity.User;import com.example.service.UserService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController; /** * @Author:wjup * @Date: 2018/9/26 0026 * @Time: 14:42 */ @RestController@RequestMapping(\"/testBoot\")public class UserController { @Autowired private UserService userService; @RequestMapping(\"getUser/{id}\") public String GetUser(@PathVariable int id){ return userService.Sel(id).toString(); }} UserService.java 1234567891011121314151617181920package com.example.service; import com.example.entity.User;import com.example.mapper.UserMapper;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service; /** * @Author:wjup * @Date: 2018/9/26 0026 * @Time: 15:23 */@Servicepublic class UserService { @Autowired UserMapper userMapper; public User Sel(int id){ return userMapper.Sel(id); }} UserMapper.java 12345678910111213141516package com.example.mapper; import com.example.entity.User;import org.apache.ibatis.annotations.Select;import org.springframework.stereotype.Repository; /** * @Author:wjup * @Date: 2018/9/26 0026 * @Time: 15:20 */@Repositorypublic interface UserMapper { User Sel(int id);} UserMapping.xml 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.example.mapper.UserMapper\"&gt; &lt;resultMap id=\"BaseResultMap\" type=\"com.example.entity.User\"&gt; &lt;result column=\"id\" jdbcType=\"INTEGER\" property=\"id\" /&gt; &lt;result column=\"userName\" jdbcType=\"VARCHAR\" property=\"userName\" /&gt; &lt;result column=\"passWord\" jdbcType=\"VARCHAR\" property=\"passWord\" /&gt; &lt;result column=\"realName\" jdbcType=\"VARCHAR\" property=\"realName\" /&gt; &lt;/resultMap&gt; &lt;select id=\"Sel\" resultType=\"com.example.entity.User\"&gt; select * from user where id = #{id} &lt;/select&gt; &lt;/mapper&gt; 最终框架结构 完成以上，下面在启动类里加上注解用于给出需要扫描的mapper文件路径@MapperScan(“com.example.mapper”) 1234567891011121314package com.example; import org.mybatis.spring.annotation.MapperScan;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication; @MapperScan(\"com.example.mapper\") //扫描的mapper@SpringBootApplicationpublic class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); }} 最后启动，浏览器输入地址看看吧：http://localhost:8080/testBoot/getUser/1 测试成功，就这样基本框架就搭建成功了 最后给个番外篇如何更改启动时显示的字符拼成的字母，就是更改下图标红框的地方 其实很好改，只需要在resources下新建一个txt文件就可以，命名为banner.txt，那这种字符该怎么拼出来呢，下面推荐一个网址，有这种工具，链接传送门：字母转字符。如下： 直接输入要生成的字母，系统会自动转换，然后复制下面转换好的字符到新建的banner.txt文件中，重新启动项目就可以了。","link":"/2019/07/10/springboot-mybatis/"},{"title":"springboot login","text":"springboot系列（一）：初次使用与登录验证实现 听说过很多springboot如何流行，以及如何简化了我们的应用开发，却没有真正使用过springboot，现在终于要动手了！打算自己动手的这个项目，结果不会是促成某个真正的项目，学习为主，把学习过程分享出来。开篇探索以以下两个方面为主：1、从mvc经典框架到springboot2、最原始的方式实现登录验证一、建一个使用springboot的项目springboot是建立在的spring生态的基础上的，以前看spring的时候，有两大概念是纠结了很久的，IOC（控制反转）以及AOP（面向切面的编程）。其实控制反转就是依赖注入，spring提供了一个IOC容器来初始化对象，解决对象间依赖管理和对象的使用，如@Bean、@Autowired等可以实现依赖注入，AOP目前理解是在一些日志框架场景中用到。平时我们常见的web项目开发，使用的mvc框架、maven管理在springboot依然使用到，springboot最明显的好处是简化了新建一个项目时的各种配置过程，就连tomcat都不用配置了。可以看到我新建一个maven项目大目录结构是这样的 跟着上图目录结构，我们来看看springboot是如何完成一个普通的web项目所需要做的事的a.web容器：springboot内嵌了Tomcat,我们不需要手动配置tomcat以及以war包形式部署项目，启动入口为目录中App.java中的main函数，直接运行即可。 @SpringBootApplicationpublic class App {public static void main(String[] args) {SpringApplication.run(App.class,args);} } b.依赖管理:spring提供了一系列的starter pom来简化Maven的依赖加载，如一个web项目pom.xm部分配置如下： 1234567891011121314151617181920&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.4.3.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;!--Spring Boot的核心启动器，包含了自动配置、日志和YAML--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--支持全栈式Web开发，包括Tomcat和spring-webmvc--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; c.配置管理：目录中src/main/resource/application.properties为配置文件，springboot可以自动读取，或者使用java配置可以看到没有web.xml，没有spring相关的配置文件，整个项目看起来非常的简洁。 #数据库配置spring.datasource.url=jdbc:mysql://localhost:3306/world?characterEncoding=UTF-8&amp;useUnicode=truespring.datasource.username=darrenspring.datasource.password=darren123spring.datasource.driver-class-name=com.mysql.jdbc.Driverspring.jpa.properties.hibernate.hbm2ddl.auto=update一个springboot web项目大概就包含这些 二、原始方式实现登录验证流程为：登录页面发起请求–&gt;拦截器拦截匹配的url判断session–&gt;后台验证/设置session–&gt;返回a、这里主要通过自定义拦截器的方式，继承WebMvcConfigurerAdapter和HandlerInterceptorAdapter来实现拦截器对登录请求进行拦截和session的判断，我这里都写在WebSecurityConfig.java中其中WebMvcConfigurerAdapter是Spring提供的基础类，可以通过重写 addInterceptors 方法添加注册拦截器来组成一个拦截链，以及用于添加拦截规则和排除不用的拦截，如下： 12345678public void addInterceptors(InterceptorRegistry registry){ InterceptorRegistration addInterceptor = registry.addInterceptor(getSecurityInterceptor()); addInterceptor.excludePathPatterns(\"/error\"); addInterceptor.excludePathPatterns(\"/login**\"); addInterceptor.addPathPatterns(\"/**\");} 其中HandlerInterceptorAdapter是spring mvc提供的适配器，继承此类，可以非常方便的实现自己的拦截器，它有三个方法：preHandle、postHandle、afterCompletion。preHandle在业务处理器处理请求之前被调用。预处理，可以进行编码、安全控制等处理；postHandle在业务处理器处理请求执行完成后，生成视图之前执行。afterCompletion在DispatcherServlet完全处理完请求后被调用，可用于清理资源等。我项目中只重写了preHandle,对请求进行session判断和跳转到自定义的页面，如下： 12345678910111213141516 private class SecurityInterceptor extends HandlerInterceptorAdapter{ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response,Object handler) throws IOException { HttpSession session = request.getSession(); // 判断是否已有该用户登录的session if(session.getAttribute(SESSION_KEY) != null){ return true; }// 跳转到登录页 String url = \"/login\"; response.sendRedirect(url); return false; } } b.controller中对登录请求进行验证以及页面的跳转，如下 12345678910111213141516171819202122232425262728293031323334353637@Controllerpublic class LoginController { @Autowired private LoginService loginService; @GetMapping(\"/\") public String index(@SessionAttribute(WebSecurityConfig.SESSION_KEY)String account,Model model){ return \"index\"; } @GetMapping(\"/login\") public String login(){ return \"login\"; } @PostMapping(\"/loginVerify\") public String loginVerify(String username,String password,HttpSession session){ User user = new User(); user.setUsername(username); user.setPassword(password); boolean verify = loginService.verifyLogin(user); if (verify) { session.setAttribute(WebSecurityConfig.SESSION_KEY, username); return \"index\"; } else { return \"redirect:/login\"; } } @GetMapping(\"/logout\") public String logout(HttpSession session){ session.removeAttribute(WebSecurityConfig.SESSION_KEY); return \"redirect:/login\"; } controller代码解释：loginVerify是对登录请求到数据库中进行验证用户名和密码，验证通过以后设置session，否则跳转到登录页面。@GetMapping是一个组合注解，是@RequestMapping(method = RequestMethod.GET)的缩写,@PostMapping同理。 ps:实际项目登录验证会使用登录验证框架：spring security 、shiro等，以及登录过程密码加密传输保存等，这里仅仅用于了解。","link":"/2019/07/17/springboot-login/"},{"title":"springboot+mycat数据库中间件+mysql（一主一备）主从复制","text":"链接","link":"/2019/09/06/springboot-mycat数据库中间件-mysql（一主一备）主从复制/"},{"title":"springboot osgi 系统监控","text":"Springboot 集成osgi 实现本机性能监控 &lt;dependency&gt; &lt;groupId&gt;com.github.oshi&lt;/groupId&gt; &lt;artifactId&gt;oshi-core&lt;/artifactId&gt; &lt;version&gt;3.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt; &lt;artifactId&gt;jna&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt; &lt;artifactId&gt;jna-platform&lt;/artifactId&gt; &lt;/dependency&gt;第二步：CPU、内存、虚拟机、系统、系统文件属性基础封装 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.zzg.monit.domain.server; import com.zzg.util.Arith; import lombok.Getter;import lombok.Setter; public class Cpu { /** * 核心数 */ @Getter @Setter private int cpuNum; /** * CPU总的使用率 */ @Setter private double total; /** * CPU系统使用率 */ @Setter private double sys; /** * CPU用户使用率 */ @Setter private double used; /** * CPU当前等待率 */ @Setter private double wait; /** * CPU当前空闲率 */ @Setter private double free; public double getTotal() { return Arith.round(Arith.mul(total, 100), 2); } public double getSys() { return Arith.round(Arith.mul(sys / total, 100), 2); } public double getUsed() { return Arith.round(Arith.mul(used / total, 100), 2); } public double getWait() { return Arith.round(Arith.mul(wait / total, 100), 2); } public double getFree() { return Arith.round(Arith.mul(free / total, 100), 2); }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.zzg.monit.domain.server; import java.lang.management.ManagementFactory; import com.zzg.util.Arith;import com.zzg.util.DateUtils; import lombok.Getter;import lombok.Setter; /** * 虚拟机相关设置 * @author zzg * */public class Jvm { /** * 当前JVM占用的内存总数(M) */ @Setter private double total; /** * JVM最大可用内存总数(M) */ @Setter private double max; /** * JVM空闲内存(M) */ @Setter private double free; /** * JDK版本 */ @Getter @Setter private String version; /** * JDK路径 */ @Getter @Setter private String home; public double getTotal() { return Arith.div(total, (1024 * 1024), 2); } public double getMax() { return Arith.div(max, (1024 * 1024), 2); } public double getFree() { return Arith.div(free, (1024 * 1024), 2); } public double getUsed() { return Arith.div(total - free, (1024 * 1024), 2); } public double getUsage() { return Arith.mul(Arith.div(total - free, total, 4), 100); } /** * 获取JDK名称 */ public String getName() { return ManagementFactory.getRuntimeMXBean().getVmName(); } /** * JDK启动时间 */ public String getStartTime() { return DateUtils.parseDateToStr(DateUtils.YYYY_MM_DD_HH_MM_SS, DateUtils.getServerStartDate()); } /** * JDK运行时间 */ public String getRunTime() { return DateUtils.getDatePoor(DateUtils.getNowDate(), DateUtils.getServerStartDate()); }} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.zzg.monit.domain.server; import com.zzg.util.Arith; import lombok.Setter; /** * 内存相关信息 * @author zzg * */public class Mem { /** * 内存总量 */ @Setter private double total; /** * 已用内存 */ @Setter private double used; /** * 剩余内存 */ @Setter private double free; public double getTotal() { return Arith.div(total, (1024 * 1024 * 1024), 2); } public double getUsed() { return Arith.div(used, (1024 * 1024 * 1024), 2); } public double getFree() { return Arith.div(free, (1024 * 1024 * 1024), 2); } public double getUsage() { return Arith.mul(Arith.div(used, total, 4), 100); }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106package com.zzg.monit.domain.server; import lombok.Getter;import lombok.Setter; /** * 系统相关信息 * @author zzg * */public class Sys { /** * 服务器名称 */ @Getter @Setter private String computerName; /** * 服务器Ip */ @Getter @Setter private String computerIp; /** * 项目路径 */ @Getter @Setter private String userDir; /** * 操作系统 */ @Getter @Setter private String osName; /** * 系统架构 */ @Getter @Setter private String osArch; }package com.zzg.monit.domain.server; import lombok.Getter;import lombok.Setter; /** * 系统文件相关信息 * @author zzg * */public class SysFile { /** * 盘符路径 */ @Getter @Setter private String dirName; /** * 盘符类型 */ @Getter @Setter private String sysTypeName; /** * 文件类型 */ @Getter @Setter private String typeName; /** * 总大小 */ @Getter @Setter private String total; /** * 剩余大小 */ @Getter @Setter private String free; /** * 已经使用量 */ @Getter @Setter private String used; /** * 资源的使用率 */ @Getter @Setter private double usage;} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190package com.zzg.monit.domain; import java.io.Serializable;import java.net.UnknownHostException;import java.util.LinkedList;import java.util.List;import java.util.Properties;import com.zzg.monit.domain.server.Cpu;import com.zzg.monit.domain.server.Jvm;import com.zzg.monit.domain.server.Mem;import com.zzg.monit.domain.server.Sys;import com.zzg.monit.domain.server.SysFile;import com.zzg.util.Arith;import com.zzg.util.IpUtils; import lombok.Getter;import lombok.Setter;import oshi.SystemInfo;import oshi.hardware.CentralProcessor;import oshi.hardware.GlobalMemory;import oshi.hardware.HardwareAbstractionLayer;import oshi.hardware.CentralProcessor.TickType;import oshi.software.os.FileSystem;import oshi.software.os.OSFileStore;import oshi.software.os.OperatingSystem;import oshi.util.Util; public class Server implements Serializable { /** * */ private static final long serialVersionUID = 1L; private static final int OSHI_WAIT_SECOND = 1000; /** * CPU相关信息 */ @Getter @Setter private Cpu cpu = new Cpu(); /** * 內存相关信息 */ @Getter @Setter private Mem mem = new Mem(); /** * JVM相关信息 */ @Getter @Setter private Jvm jvm = new Jvm(); /** * 服务器相关信息 */ @Getter @Setter private Sys sys = new Sys(); /** * 磁盘相关信息 */ @Getter @Setter private List&lt;SysFile&gt; sysFiles = new LinkedList&lt;SysFile&gt;(); public void copyTo() throws Exception { SystemInfo si = new SystemInfo(); HardwareAbstractionLayer hal = si.getHardware(); setCpuInfo(hal.getProcessor()); setMemInfo(hal.getMemory()); setSysInfo(); setJvmInfo(); setSysFiles(si.getOperatingSystem()); } /** * 设置CPU信息 */ private void setCpuInfo(CentralProcessor processor) { // CPU信息 long[] prevTicks = processor.getSystemCpuLoadTicks(); Util.sleep(OSHI_WAIT_SECOND); long[] ticks = processor.getSystemCpuLoadTicks(); long nice = ticks[TickType.NICE.getIndex()] - prevTicks[TickType.NICE.getIndex()]; long irq = ticks[TickType.IRQ.getIndex()] - prevTicks[TickType.IRQ.getIndex()]; long softirq = ticks[TickType.SOFTIRQ.getIndex()] - prevTicks[TickType.SOFTIRQ.getIndex()]; long steal = ticks[TickType.STEAL.getIndex()] - prevTicks[TickType.STEAL.getIndex()]; long cSys = ticks[TickType.SYSTEM.getIndex()] - prevTicks[TickType.SYSTEM.getIndex()]; long user = ticks[TickType.USER.getIndex()] - prevTicks[TickType.USER.getIndex()]; long iowait = ticks[TickType.IOWAIT.getIndex()] - prevTicks[TickType.IOWAIT.getIndex()]; long idle = ticks[TickType.IDLE.getIndex()] - prevTicks[TickType.IDLE.getIndex()]; long totalCpu = user + nice + cSys + idle + iowait + irq + softirq + steal; cpu.setCpuNum(processor.getLogicalProcessorCount()); cpu.setTotal(totalCpu); cpu.setSys(cSys); cpu.setUsed(user); cpu.setWait(iowait); cpu.setFree(idle); } /** * 设置内存信息 */ private void setMemInfo(GlobalMemory memory) { mem.setTotal(memory.getTotal()); mem.setUsed(memory.getTotal() - memory.getAvailable()); mem.setFree(memory.getAvailable()); } /** * 设置服务器信息 */ private void setSysInfo() { Properties props = System.getProperties(); sys.setComputerName(IpUtils.getHostName()); sys.setComputerIp(IpUtils.getHostIp()); sys.setOsName(props.getProperty(\"os.name\")); sys.setOsArch(props.getProperty(\"os.arch\")); sys.setUserDir(props.getProperty(\"user.dir\")); } /** * 设置Java虚拟机 */ private void setJvmInfo() throws UnknownHostException { Properties props = System.getProperties(); jvm.setTotal(Runtime.getRuntime().totalMemory()); jvm.setMax(Runtime.getRuntime().maxMemory()); jvm.setFree(Runtime.getRuntime().freeMemory()); jvm.setVersion(props.getProperty(\"java.version\")); jvm.setHome(props.getProperty(\"java.home\")); } /** * 设置磁盘信息 */ private void setSysFiles(OperatingSystem os) { FileSystem fileSystem = os.getFileSystem(); OSFileStore[] fsArray = fileSystem.getFileStores(); for (OSFileStore fs : fsArray) { long free = fs.getUsableSpace(); long total = fs.getTotalSpace(); long used = total - free; SysFile sysFile = new SysFile(); sysFile.setDirName(fs.getMount()); sysFile.setSysTypeName(fs.getType()); sysFile.setTypeName(fs.getName()); sysFile.setTotal(convertFileSize(total)); sysFile.setFree(convertFileSize(free)); sysFile.setUsed(convertFileSize(used)); sysFile.setUsage(Arith.mul(Arith.div(used, total, 4), 100)); sysFiles.add(sysFile); } } /** * 字节转换 * * @param size 字节大小 * @return 转换后值 */ public String convertFileSize(long size) { long kb = 1024; long mb = kb * 1024; long gb = mb * 1024; if (size &gt;= gb) { return String.format(\"%.1f GB\" , (float) size / gb); } else if (size &gt;= mb) { float f = (float) size / mb; return String.format(f &gt; 100 ? \"%.0f MB\" : \"%.1f MB\" , f); } else if (size &gt;= kb) { float f = (float) size / kb; return String.format(f &gt; 100 ? \"%.0f KB\" : \"%.1f KB\" , f); } else { return String.format(\"%d B\" , size); } }} 第三步：控制层编写： 1234567891011121314151617181920212223242526272829303132333435package com.zzg.controller; import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.servlet.ModelAndView;import com.zzg.monit.domain.Server;import io.swagger.annotations.Api;import io.swagger.annotations.ApiOperation;import lombok.extern.slf4j.Slf4j; /** * 服务器监控 * @author zzg * */@Api(value = \"服务器监控\", description = \"服务器监控管理\")@Controller@RequestMapping(\"/server\")@Slf4jpublic class ServerController { /**服务器监控首页 * @throws Exception **/ @ApiOperation(value = \"服务器监控首页\", notes = \"服务器监控首页\") @GetMapping(\"/server\") public ModelAndView user() throws Exception{ Server server = new Server(); server.copyTo(); ModelAndView modelAndView = new ModelAndView(); modelAndView.setViewName(\"admin/server/server\"); modelAndView.addObject(\"server\", server); return modelAndView; } }","link":"/2019/07/25/springboot-osgi-系统监控/"},{"title":"springboot 日志","text":"SpringBoot默认日志框架配置今天来介绍下Spring Boot如何配置日志logback,我刚学习的时候，是带着下面几个问题来查资料的，你呢 如何引入日志？ 日志输出格式以及输出方式如何配置？ 代码中如何使用？ 正文Spring Boot在所有内部日志中使用Commons Logging，但是默认配置也提供了对常用日志的支持，如：Java Util Logging，Log4J, Log4J2和Logback。每种Logger都可以通过配置使用控制台或者文件输出日志内容。 默认日志LogbackSLF4J——Simple Logging Facade For Java，它是一个针对于各类Java日志框架的统一Facade抽象。Java日志框架众多——常用的有java.util.logging, log4j, logback，commons-logging, Spring框架使用的是Jakarta Commons Logging API (JCL)。而SLF4J定义了统一的日志抽象接口，而真正的日志实现则是在运行时决定的——它提供了各类日志框架的binding。 Logback是log4j框架的作者开发的新一代日志框架，它效率更高、能够适应诸多的运行环境，同时天然支持SLF4J。 默认情况下，Spring Boot会用Logback来记录日志，并用INFO级别输出到控制台。在运行应用程序和其他例子时，你应该已经看到很多INFO级别的日志了。 从上图可以看到，日志输出内容元素具体如下： 时间日期：精确到毫秒 日志级别：ERROR, WARN, INFO, DEBUG or TRACE 进程ID 分隔符：— 标识实际日志的开始 线程名：方括号括起来（可能会截断控制台输出） Logger名：通常使用源代码的类名 日志内容 默认配置属性支持 Spring Boot为我们提供了很多默认的日志配置，所以，只要将spring-boot-starter-logging作为依赖加入到当前应用的classpath，则“开箱即用”。 下面介绍几种在application.properties就可以配置的日志相关属性。 控制台输出 日志级别从低到高分为TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL，如果设置为WARN，则低于WARN的信息都不会输出。 Spring Boot中默认配置ERROR、WARN和INFO级别的日志输出到控制台。您还可以通过启动您的应用程序–debug标志来启用“调试”模式（开发的时候推荐开启）,以下两种方式皆可： 在运行命令后加入–debug标志，如：$ java -jar springTest.jar –debug 在application.properties中配置debug=true，该属性置为true的时候，核心Logger（包含嵌入式容器、hibernate、spring）会输出更多内容，但是你自己应用的日志并不会输出为DEBUG级别。 文件输出 默认情况下，Spring Boot将日志输出到控制台，不会写到日志文件。如果要编写除控制台输出之外的日志文件，则需在application.properties中设置logging.file或logging.path属性。 logging.file，设置文件，可以是绝对路径，也可以是相对路径。如：logging.file=my.log logging.path，设置目录，会在该目录下创建spring.log文件，并写入日志内容，如：logging.path=/var/log 如果只配置 logging.file，会在项目的当前路径下生成一个 xxx.log 日志文件。 如果只配置 logging.path，在 /var/log文件夹生成一个日志文件为 spring.log 注：二者不能同时使用，如若同时使用，则只有logging.file生效 默认情况下，日志文件的大小达到10MB时会切分一次，产生新的日志文件，默认级别为：ERROR、WARN、INFO 级别控制所有支持的日志记录系统都可以在Spring环境中设置记录级别（例如在application.properties中） 格式为：’logging.level.* = LEVEL’ logging.level：日志级别控制前缀，*为包名或Logger名 LEVEL：选项TRACE, DEBUG, INFO, WARN, ERROR, FATAL, OFF 举例： logging.level.com.dudu=DEBUG：com.dudu包下所有class以DEBUG级别输出 logging.level.root=WARN：root日志以WARN级别输出 自定义日志配置由于日志服务一般都在ApplicationContext创建前就初始化了，它并不是必须通过Spring的配置文件控制。因此通过系统属性和传统的Spring Boot外部配置文件依然可以很好的支持日志控制和管理。 根据不同的日志系统，你可以按如下规则组织配置文件名，就能被正确加载： Logback：logback-spring.xml, logback-spring.groovy, logback.xml, logback.groovy Log4j：log4j-spring.properties, log4j-spring.xml, log4j.properties, log4j.xml Log4j2：log4j2-spring.xml, log4j2.xml JDK (Java Util Logging)：logging.properties Spring Boot官方推荐优先使用带有-spring的文件名作为你的日志配置（如使用logback-spring.xml，而不是logback.xml），命名为logback-spring.xml的日志配置文件，spring boot可以为它添加一些spring boot特有的配置项（下面会提到）。 上面是默认的命名规则，并且放在src/main/resources下面即可。 如果你即想完全掌控日志配置，但又不想用logback.xml作为Logback配置的名字，可以通过logging.config属性指定自定义的名字： logging.config=classpath:logging-config.xml虽然一般并不需要改变配置文件的名字，但是如果你想针对不同运行时Profile使用不同的日 志配置，这个功能会很有用。 下面我们来看看一个普通的logback-spring.xml例子 根节点包含的属性scan:当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。 scanPeriod:设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。 debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 根节点的子节点： 下面一共有2个属性，3个子节点，分别是： 属性一：设置上下文名称每个logger都关联到logger上下文，默认上下文名称为“default”。但可以使用设置成其他名字，用于区分不同应用程序的记录。一旦设置，不能修改,可以通过%contextName来打印日志上下文名称。 logback属性二：设置变量用来定义变量值的标签， 有两个属性，name和value；其中name的值是变量的名称，value的值时变量定义的值。通过定义的值会被插入到logger上下文中。定义变量后，可以使“${}”来使用变量。 子节点一 appender用来格式化日志输出节点，有俩个属性name和class，class用来指定哪种输出策略，常用就是控制台输出策略和文件输出策略。 控制台输出ConsoleAppender： 表示对日志进行编码： %d{HH: mm:ss.SSS}——日志输出时间 %thread——输出日志的进程名字，这在Web应用以及异步任务处理中很有用 %-5level——日志级别，并且使用5个字符靠左对齐 %logger{36}——日志输出者的名字 %msg——日志消息 %n——平台的换行符 ThresholdFilter为系统定义的拦截器，例如我们用ThresholdFilter来过滤掉ERROR级别以下的日志不输出到文件中。如果不用记得注释掉，不然你控制台会发现没日志~ 输出到文件RollingFileAppender另一种常见的日志输出到文件，随着应用的运行时间越来越长，日志也会增长的越来越多，将他们输出到同一个文件并非一个好办法。RollingFileAppender用于切分文件日志： 其中重要的是rollingPolicy的定义，上例中logback.%d{yyyy-MM-dd}.log定义了日志的切分方式——把每一天的日志归档到一个文件中，30表示只保留最近30天的日志，以防止日志填满整个磁盘空间。同理，可以使用%d{yyyy-MM-dd_HH-mm}来定义精确到分的日志切分方式。1GB用来指定日志文件的上限大小，例如设置为1GB的话，那么到了这个值，就会删除旧的日志。 子节点二root节点是必选节点，用来指定最基础的日志输出级别，只有一个level属性。 level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，不能设置为INHERITED或者同义词NULL，默认是DEBUG。 可以包含零个或多个元素，标识这个appender将会添加到这个loger。 子节点三用来设置某一个包或者具体的某一个类的日志打印级别、以及指定。仅有一个name属性，一个可选的level和一个可选的addtivity属性。 name:用来指定受此loger约束的某一个包或者具体的某一个类。 level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，还有一个特俗值INHERITED或者同义词NULL，代表强制执行上级的级别。如果未设置此属性，那么当前loger将会继承上级的级别。 addtivity:是否向上级loger传递打印信息。默认是true。 loger在实际使用的时候有两种情况，先来看一看代码中如何使用： 这是一个登录的判断的方法，我们引入日志，并且打印不同级别的日志，然后根据logback-spring.xml中的配置来看看打印了哪几种级别日志。 第一种：带有loger的配置，不指定级别，不指定appender将控制controller包下的所有类的日志的打印，但是并没用设置打印级别，所以继承他的上级的日志级别“info”； 没有设置addtivity，默认为true，将此loger的打印信息向上级传递； 没有设置appender，此loger本身不打印任何信息。 将root的打印级别设置为“info”，指定了名字为“console”的appender。 当执行com.dudu.controller.LearnController类的login方法时，LearnController 在包com.dudu.controller中，所以首先执行，将级别为“info”及大于“info”的日志信息传递给root，本身并不打印； root接到下级传递的信息，交给已经配置好的名为“console”的appender处理，“console”appender将信息打印到控制台； 打印结果如下： 第二种：带有多个loger的配置，指定级别，指定appender 控制com.dudu.controller.LearnController类的日志打印，打印级别为“WARN” additivity属性为false，表示此loger的打印信息不再向上级传递 指定了名字为“console”的appender 这时候执行com.dudu.controller.LearnController类的login方法时，先执行, 将级别为“WARN”及大于“WARN”的日志信息交给此loger指定的名为“console”的appender处理，在控制台中打出日志，不再向上级root传递打印信息。 打印结果如下： 当然如果你把additivity=”false”改成additivity=”true”的话，就会打印两次，因为打印信息向上级传递，logger本身打印一次，root接到后又打印一次。 多环境日志输出据不同环境（prod:生产环境，test:测试环境，dev:开发环境）来定义不同的日志输出，在 logback-spring.xml中使用 springProfile 节点来定义，方法如下： 文件名称不是logback.xml，想使用spring扩展profile支持，要以logback-spring.xml命名 可以启动服务的时候指定 profile （如不指定使用默认），如指定prod 的方式为： java -jar xxx.jar –spring.profiles.active=prod 关于多环境配置可以参考 Spring Boot干货系列：（二）配置文件解析 总结到此为止终于介绍完日志框架了，平时使用的时候推荐用自定义logback-spring.xml来配置，代码中使用日志也很简单，类里面添加private Logger logger = LoggerFactory.getLogger(this.getClass());即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!-- 日志级别从低到高分为TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL，如果设置为WARN，则低于WARN的信息都不会输出 --&gt;&lt;!-- scan:当此属性设置为true时，配置文档如果发生改变，将会被重新加载，默认值为true --&gt;&lt;!-- scanPeriod:设置监测配置文档是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。 当scan为true时，此属性生效。默认的时间间隔为1分钟。 --&gt;&lt;!-- debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 --&gt;&lt;configuration scan=\"true\" scanPeriod=\"10 seconds\"&gt; &lt;contextName&gt;logback&lt;/contextName&gt; &lt;!-- name的值是变量的名称，value的值时变量定义的值。通过定义的值会被插入到logger上下文中。定义后，可以使“${}”来使用变量。 --&gt; &lt;property name=\"log.path\" value=\"G:/logs/pmp\" /&gt; &lt;!--0. 日志格式和颜色渲染 --&gt; &lt;!-- 彩色日志依赖的渲染类 --&gt; &lt;conversionRule conversionWord=\"clr\" converterClass=\"org.springframework.boot.logging.logback.ColorConverter\" /&gt; &lt;conversionRule conversionWord=\"wex\" converterClass=\"org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter\" /&gt; &lt;conversionRule conversionWord=\"wEx\" converterClass=\"org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter\" /&gt; &lt;!-- 彩色日志格式 --&gt; &lt;property name=\"CONSOLE_LOG_PATTERN\" value=\"${CONSOLE_LOG_PATTERN:-%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}\"/&gt; &lt;!--1. 输出到控制台--&gt; &lt;appender name=\"CONSOLE\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;!--此日志appender是为开发使用，只配置最底级别，控制台输出的日志级别是大于或等于此级别的日志信息--&gt; &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt; &lt;level&gt;info&lt;/level&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;Pattern&gt;${CONSOLE_LOG_PATTERN}&lt;/Pattern&gt; &lt;!-- 设置字符集 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--2. 输出到文档--&gt; &lt;!-- 2.1 level为 DEBUG 日志，时间滚动输出 --&gt; &lt;appender name=\"DEBUG_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!-- 正在记录的日志文档的路径及文档名 --&gt; &lt;file&gt;${log.path}/web_debug.log&lt;/file&gt; &lt;!--日志文档输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 设置字符集 --&gt; &lt;/encoder&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;!-- 日志归档 --&gt; &lt;fileNamePattern&gt;${log.path}/web-debug-%d{yyyy-MM-dd}.%i.log&lt;/fileNamePattern&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\"&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;!--日志文档保留天数--&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 此日志文档只记录debug级别的 --&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;level&gt;debug&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 2.2 level为 INFO 日志，时间滚动输出 --&gt; &lt;appender name=\"INFO_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!-- 正在记录的日志文档的路径及文档名 --&gt; &lt;file&gt;${log.path}/web_info.log&lt;/file&gt; &lt;!--日志文档输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;!-- 每天日志归档路径以及格式 --&gt; &lt;fileNamePattern&gt;${log.path}/web-info-%d{yyyy-MM-dd}.%i.log&lt;/fileNamePattern&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\"&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;!--日志文档保留天数--&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 此日志文档只记录info级别的 --&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;level&gt;info&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 2.3 level为 WARN 日志，时间滚动输出 --&gt; &lt;appender name=\"WARN_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!-- 正在记录的日志文档的路径及文档名 --&gt; &lt;file&gt;${log.path}/web_warn.log&lt;/file&gt; &lt;!--日志文档输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 此处设置字符集 --&gt; &lt;/encoder&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;${log.path}/web-warn-%d{yyyy-MM-dd}.%i.log&lt;/fileNamePattern&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\"&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;!--日志文档保留天数--&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 此日志文档只记录warn级别的 --&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;level&gt;warn&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 2.4 level为 ERROR 日志，时间滚动输出 --&gt; &lt;appender name=\"ERROR_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!-- 正在记录的日志文档的路径及文档名 --&gt; &lt;file&gt;${log.path}/web_error.log&lt;/file&gt; &lt;!--日志文档输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 此处设置字符集 --&gt; &lt;/encoder&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;${log.path}/web-error-%d{yyyy-MM-dd}.%i.log&lt;/fileNamePattern&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\"&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;!--日志文档保留天数--&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 此日志文档只记录ERROR级别的 --&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- &lt;logger&gt;用来设置某一个包或者具体的某一个类的日志打印级别、 以及指定&lt;appender&gt;。&lt;logger&gt;仅有一个name属性， 一个可选的level和一个可选的addtivity属性。 name:用来指定受此logger约束的某一个包或者具体的某一个类。 level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF， 还有一个特俗值INHERITED或者同义词NULL，代表强制执行上级的级别。 如果未设置此属性，那么当前logger将会继承上级的级别。 addtivity:是否向上级logger传递打印信息。默认是true。 &lt;logger name=\"org.springframework.web\" level=\"info\"/&gt; &lt;logger name=\"org.springframework.scheduling.annotation.ScheduledAnnotationBeanPostProcessor\" level=\"INFO\"/&gt; --&gt; &lt;!-- 使用mybatis的时候，sql语句是debug下才会打印，而这里我们只配置了info，所以想要查看sql语句的话，有以下两种操作： 第一种把&lt;root level=\"info\"&gt;改成&lt;root level=\"DEBUG\"&gt;这样就会打印sql，不过这样日志那边会出现很多其他消息 第二种就是单独给dao下目录配置debug模式，代码如下，这样配置sql语句会打印，其他还是正常info级别： 【logging.level.org.mybatis=debug logging.level.dao=debug】 --&gt; &lt;!-- root节点是必选节点，用来指定最基础的日志输出级别，只有一个level属性 level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF， 不能设置为INHERITED或者同义词NULL。默认是DEBUG 可以包含零个或多个元素，标识这个appender将会添加到这个logger。 --&gt; &lt;!-- 4. 最终的策略 --&gt; &lt;!-- 4.1 开发环境:打印控制台--&gt; &lt;springProfile name=\"dev\"&gt; &lt;logger name=\"com.sdcm.pmp\" level=\"debug\"/&gt; &lt;/springProfile&gt; &lt;root level=\"info\"&gt; &lt;appender-ref ref=\"CONSOLE\" /&gt; &lt;appender-ref ref=\"DEBUG_FILE\" /&gt; &lt;appender-ref ref=\"INFO_FILE\" /&gt; &lt;appender-ref ref=\"WARN_FILE\" /&gt; &lt;appender-ref ref=\"ERROR_FILE\" /&gt; &lt;/root&gt; &lt;!-- 4.2 生产环境:输出到文档 &lt;springProfile name=\"pro\"&gt; &lt;root level=\"info\"&gt; &lt;appender-ref ref=\"CONSOLE\" /&gt; &lt;appender-ref ref=\"DEBUG_FILE\" /&gt; &lt;appender-ref ref=\"INFO_FILE\" /&gt; &lt;appender-ref ref=\"ERROR_FILE\" /&gt; &lt;appender-ref ref=\"WARN_FILE\" /&gt; &lt;/root&gt; &lt;/springProfile&gt; --&gt; &lt;/configuration&gt;","link":"/2019/09/19/springboot-日志/"},{"title":"springboot Quartz","text":"SpringBoot整合Quartz作为调度中心完整实用例子 因为想要做一个类似于调度中心的东西，定时执行一些Job(通常是一些自定义程序或者可执行的jar包)，搭了一个例子，总结了前辈们的相关经验和自己的一些理解，如有雷同或不当之处，望各位大佬见谅和帮忙指正。由于之前有许多小伙伴问过我如何写个定时任务，里面写上逻辑自己的逻辑,我另做了一个SpringBoot完全整合Quartz的简单例子可以作为Quartz的入门参考 : https://github.com/EalenXie/springboot-quartz-simple 本例子是整合Quartz作为调度中心使用的。1.首先新建项目SpringBoot-Quartz ,选用的技术栈为 SpringBoot + Quartz + Spring Data Jpa。完整代码可见(本文已非最新代码,最新见Github) : https://github.com/EalenXie/SpringBoot-Quartzpom.xml依赖 : 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;name.ealenxie&lt;/groupId&gt; &lt;artifactId&gt;SpringBoot-Quartz&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;!--quartz依赖--&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz-jobs&lt;/artifactId&gt; &lt;version&gt;2.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2.你需要在数据库中建立Quartz的相关系统表，所以你需要在数据库中执行如下来自Quartz官方的脚本。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172-- in your Quartz properties file, you'll need to set org.quartz.jobStore.driverDelegateClass = org.quartz.impl.jdbcjobstore.StdJDBCDelegate-- 你需要在你的quartz.properties文件中设置org.quartz.jobStore.driverDelegateClass = org.quartz.impl.jdbcjobstore.StdJDBCDelegate-- StdJDBCDelegate说明支持集群，所有的任务信息都会保存到数据库中，可以控制事物，还有就是如果应用服务器关闭或者重启，任务信息都不会丢失，并且可以恢复因服务器关闭或者重启而导致执行失败的任务-- This is the script from Quartz to create the tables in a MySQL database, modified to use INNODB instead of MYISAM-- 这是来自quartz的脚本，在MySQL数据库中创建以下的表，修改为使用INNODB而不是MYISAM-- 你需要在数据库中执行以下的sql脚本DROP TABLE IF EXISTS QRTZ_FIRED_TRIGGERS;DROP TABLE IF EXISTS QRTZ_PAUSED_TRIGGER_GRPS;DROP TABLE IF EXISTS QRTZ_SCHEDULER_STATE;DROP TABLE IF EXISTS QRTZ_LOCKS;DROP TABLE IF EXISTS QRTZ_SIMPLE_TRIGGERS;DROP TABLE IF EXISTS QRTZ_SIMPROP_TRIGGERS;DROP TABLE IF EXISTS QRTZ_CRON_TRIGGERS;DROP TABLE IF EXISTS QRTZ_BLOB_TRIGGERS;DROP TABLE IF EXISTS QRTZ_TRIGGERS;DROP TABLE IF EXISTS QRTZ_JOB_DETAILS;DROP TABLE IF EXISTS QRTZ_CALENDARS;-- 存储每一个已配置的Job的详细信息CREATE TABLE QRTZ_JOB_DETAILS(SCHED_NAME VARCHAR(120) NOT NULL,JOB_NAME VARCHAR(200) NOT NULL,JOB_GROUP VARCHAR(200) NOT NULL,DESCRIPTION VARCHAR(250) NULL,JOB_CLASS_NAME VARCHAR(250) NOT NULL,IS_DURABLE VARCHAR(1) NOT NULL,IS_NONCONCURRENT VARCHAR(1) NOT NULL,IS_UPDATE_DATA VARCHAR(1) NOT NULL,REQUESTS_RECOVERY VARCHAR(1) NOT NULL,JOB_DATA BLOB NULL,PRIMARY KEY (SCHED_NAME,JOB_NAME,JOB_GROUP))ENGINE=InnoDB;-- 存储已配置的Trigger的信息CREATE TABLE QRTZ_TRIGGERS (SCHED_NAME VARCHAR(120) NOT NULL,TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,JOB_NAME VARCHAR(200) NOT NULL,JOB_GROUP VARCHAR(200) NOT NULL,DESCRIPTION VARCHAR(250) NULL,NEXT_FIRE_TIME BIGINT(13) NULL,PREV_FIRE_TIME BIGINT(13) NULL,PRIORITY INTEGER NULL,TRIGGER_STATE VARCHAR(16) NOT NULL,TRIGGER_TYPE VARCHAR(8) NOT NULL,START_TIME BIGINT(13) NOT NULL,END_TIME BIGINT(13) NULL,CALENDAR_NAME VARCHAR(200) NULL,MISFIRE_INSTR SMALLINT(2) NULL,JOB_DATA BLOB NULL,PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),FOREIGN KEY (SCHED_NAME,JOB_NAME,JOB_GROUP)REFERENCES QRTZ_JOB_DETAILS(SCHED_NAME,JOB_NAME,JOB_GROUP))ENGINE=InnoDB;-- 存储已配置的Simple Trigger的信息CREATE TABLE QRTZ_SIMPLE_TRIGGERS (SCHED_NAME VARCHAR(120) NOT NULL,TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,REPEAT_COUNT BIGINT(7) NOT NULL,REPEAT_INTERVAL BIGINT(12) NOT NULL,TIMES_TRIGGERED BIGINT(10) NOT NULL,PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),FOREIGN KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP)REFERENCES QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP))ENGINE=InnoDB;-- 存储Cron Trigger，包括Cron表达式和时区信息CREATE TABLE QRTZ_CRON_TRIGGERS (SCHED_NAME VARCHAR(120) NOT NULL,TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,CRON_EXPRESSION VARCHAR(120) NOT NULL,TIME_ZONE_ID VARCHAR(80),PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),FOREIGN KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP)REFERENCES QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_SIMPROP_TRIGGERS ( SCHED_NAME VARCHAR(120) NOT NULL, TRIGGER_NAME VARCHAR(200) NOT NULL, TRIGGER_GROUP VARCHAR(200) NOT NULL, STR_PROP_1 VARCHAR(512) NULL, STR_PROP_2 VARCHAR(512) NULL, STR_PROP_3 VARCHAR(512) NULL, INT_PROP_1 INT NULL, INT_PROP_2 INT NULL, LONG_PROP_1 BIGINT NULL, LONG_PROP_2 BIGINT NULL, DEC_PROP_1 NUMERIC(13,4) NULL, DEC_PROP_2 NUMERIC(13,4) NULL, BOOL_PROP_1 VARCHAR(1) NULL, BOOL_PROP_2 VARCHAR(1) NULL, PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP), FOREIGN KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP) REFERENCES QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP))ENGINE=InnoDB;-- Trigger作为Blob类型存储(用于Quartz用户用JDBC创建他们自己定制的Trigger类型，JobStore并不知道如何存储实例的时候)CREATE TABLE QRTZ_BLOB_TRIGGERS (SCHED_NAME VARCHAR(120) NOT NULL,TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,BLOB_DATA BLOB NULL,PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),INDEX (SCHED_NAME,TRIGGER_NAME, TRIGGER_GROUP),FOREIGN KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP)REFERENCES QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP))ENGINE=InnoDB;-- 以Blob类型存储Quartz的Calendar日历信息,quartz可配置一个日历来指定一个时间范围CREATE TABLE QRTZ_CALENDARS (SCHED_NAME VARCHAR(120) NOT NULL,CALENDAR_NAME VARCHAR(200) NOT NULL,CALENDAR BLOB NOT NULL,PRIMARY KEY (SCHED_NAME,CALENDAR_NAME))ENGINE=InnoDB;-- 存储已暂停的Trigger组的信息CREATE TABLE QRTZ_PAUSED_TRIGGER_GRPS (SCHED_NAME VARCHAR(120) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,PRIMARY KEY (SCHED_NAME,TRIGGER_GROUP))ENGINE=InnoDB;-- 存储与已触发的Trigger相关的状态信息，以及相联Job的执行信息CREATE TABLE QRTZ_FIRED_TRIGGERS (SCHED_NAME VARCHAR(120) NOT NULL,ENTRY_ID VARCHAR(95) NOT NULL,TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,INSTANCE_NAME VARCHAR(200) NOT NULL,FIRED_TIME BIGINT(13) NOT NULL,SCHED_TIME BIGINT(13) NOT NULL,PRIORITY INTEGER NOT NULL,STATE VARCHAR(16) NOT NULL,JOB_NAME VARCHAR(200) NULL,JOB_GROUP VARCHAR(200) NULL,IS_NONCONCURRENT VARCHAR(1) NULL,REQUESTS_RECOVERY VARCHAR(1) NULL,PRIMARY KEY (SCHED_NAME,ENTRY_ID))ENGINE=InnoDB;-- 存储少量的有关 Scheduler的状态信息，和别的 Scheduler 实例(假如是用于一个集群中)CREATE TABLE QRTZ_SCHEDULER_STATE (SCHED_NAME VARCHAR(120) NOT NULL,INSTANCE_NAME VARCHAR(200) NOT NULL,LAST_CHECKIN_TIME BIGINT(13) NOT NULL,CHECKIN_INTERVAL BIGINT(13) NOT NULL,PRIMARY KEY (SCHED_NAME,INSTANCE_NAME))ENGINE=InnoDB;-- 存储程序的非观锁的信息(假如使用了悲观锁)CREATE TABLE QRTZ_LOCKS (SCHED_NAME VARCHAR(120) NOT NULL,LOCK_NAME VARCHAR(40) NOT NULL,PRIMARY KEY (SCHED_NAME,LOCK_NAME))ENGINE=InnoDB;CREATE INDEX IDX_QRTZ_J_REQ_RECOVERY ON QRTZ_JOB_DETAILS(SCHED_NAME,REQUESTS_RECOVERY);CREATE INDEX IDX_QRTZ_J_GRP ON QRTZ_JOB_DETAILS(SCHED_NAME,JOB_GROUP);CREATE INDEX IDX_QRTZ_T_J ON QRTZ_TRIGGERS(SCHED_NAME,JOB_NAME,JOB_GROUP);CREATE INDEX IDX_QRTZ_T_JG ON QRTZ_TRIGGERS(SCHED_NAME,JOB_GROUP);CREATE INDEX IDX_QRTZ_T_C ON QRTZ_TRIGGERS(SCHED_NAME,CALENDAR_NAME);CREATE INDEX IDX_QRTZ_T_G ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_GROUP);CREATE INDEX IDX_QRTZ_T_STATE ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_STATE);CREATE INDEX IDX_QRTZ_T_N_STATE ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP,TRIGGER_STATE);CREATE INDEX IDX_QRTZ_T_N_G_STATE ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_GROUP,TRIGGER_STATE);CREATE INDEX IDX_QRTZ_T_NEXT_FIRE_TIME ON QRTZ_TRIGGERS(SCHED_NAME,NEXT_FIRE_TIME);CREATE INDEX IDX_QRTZ_T_NFT_ST ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_STATE,NEXT_FIRE_TIME);CREATE INDEX IDX_QRTZ_T_NFT_MISFIRE ON QRTZ_TRIGGERS(SCHED_NAME,MISFIRE_INSTR,NEXT_FIRE_TIME);CREATE INDEX IDX_QRTZ_T_NFT_ST_MISFIRE ON QRTZ_TRIGGERS(SCHED_NAME,MISFIRE_INSTR,NEXT_FIRE_TIME,TRIGGER_STATE);CREATE INDEX IDX_QRTZ_T_NFT_ST_MISFIRE_GRP ON QRTZ_TRIGGERS(SCHED_NAME,MISFIRE_INSTR,NEXT_FIRE_TIME,TRIGGER_GROUP,TRIGGER_STATE);CREATE INDEX IDX_QRTZ_FT_TRIG_INST_NAME ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,INSTANCE_NAME);CREATE INDEX IDX_QRTZ_FT_INST_JOB_REQ_RCVRY ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,INSTANCE_NAME,REQUESTS_RECOVERY);CREATE INDEX IDX_QRTZ_FT_J_G ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,JOB_NAME,JOB_GROUP);CREATE INDEX IDX_QRTZ_FT_JG ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,JOB_GROUP);CREATE INDEX IDX_QRTZ_FT_T_G ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP);CREATE INDEX IDX_QRTZ_FT_TG ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,TRIGGER_GROUP);commit; 3.新建完Quartz系统表之后,你需要做SpringBoot和Quartz的相关配置application.yml 123456789101112131415161718quartz: enabled: trueserver: port: 9090spring: datasource: url: jdbc:mysql://localhost:3306/spring_quartz username: yourname password: yourpassword tomcat: initialSize: 20 maxActive: 100 maxIdle: 100 minIdle: 20 maxWait: 10000 testWhileIdle: true testOnBorrow: false testOnReturn: false 4.需要调度数据库中的Job实例定义。JobEntity.class 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156package com.ealen.entity;import javax.persistence.*;import java.io.Serializable;/** * Created by EalenXie on 2018/6/4 14:09 * 这里个人示例,可自定义相关属性 */@Entity@Table(name = \"JOB_ENTITY\")public class JobEntity implements Serializable { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; private String name; //job名称 private String group; //job组名 private String cron; //执行的cron private String parameter; //job的参数 private String description; //job描述信息 @Column(name = \"vm_param\") private String vmParam; //vm参数 @Column(name = \"jar_path\") private String jarPath; //job的jar路径,在这里我选择的是定时执行一些可执行的jar包 private String status; //job的执行状态,这里我设置为OPEN/CLOSE且只有该值为OPEN才会执行该Job public JobEntity() { } public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getGroup() { return group; } public void setGroup(String group) { this.group = group; } public String getCron() { return cron; } public void setCron(String cron) { this.cron = cron; } public String getParameter() { return parameter; } public void setParameter(String parameter) { this.parameter = parameter; } public String getDescription() { return description; } public void setDescription(String description) { this.description = description; } public String getVmParam() { return vmParam; } public void setVmParam(String vmParam) { this.vmParam = vmParam; } public String getJarPath() { return jarPath; } public void setJarPath(String jarPath) { this.jarPath = jarPath; } public String getStatus() { return status; } public void setStatus(String status) { this.status = status; } @Override public String toString() { return \"JobEntity{\" + \"id=\" + id + \", name='\" + name + '\\'' + \", group='\" + group + '\\'' + \", cron='\" + cron + '\\'' + \", parameter='\" + parameter + '\\'' + \", description='\" + description + '\\'' + \", vmParam='\" + vmParam + '\\'' + \", jarPath='\" + jarPath + '\\'' + \", status='\" + status + '\\'' + '}'; } //新增Builder模式,可选,选择设置任意属性初始化对象 public JobEntity(Builder builder) { id = builder.id; name = builder.name; group = builder.group; cron = builder.cron; parameter = builder.parameter; description = builder.description; vmParam = builder.vmParam; jarPath = builder.jarPath; status = builder.status; } public static class Builder { private Integer id; private String name = \"\"; //job名称 private String group = \"\"; //job组名 private String cron = \"\"; //执行的cron private String parameter = \"\"; //job的参数 private String description = \"\"; //job描述信息 private String vmParam = \"\"; //vm参数 private String jarPath = \"\"; //job的jar路径 private String status = \"\"; //job的执行状态,只有该值为OPEN才会执行该Job public Builder withId(Integer i) { id = i; return this; } public Builder withName(String n) { name = n; return this; } public Builder withGroup(String g) { group = g; return this; } public Builder withCron(String c) { cron = c; return this; } public Builder withParameter(String p) { parameter = p; return this; } public Builder withDescription(String d) { description = d; return this; } public Builder withVMParameter(String vm) { vmParam = vm; return this; } public Builder withJarPath(String jar) { jarPath = jar; return this; } public Builder withStatus(String s) { status = s; return this; } public JobEntity newJobEntity() { return new JobEntity(this); } }} 为了方便测试,设计好表之后先插入几条记录,job_entity.sql,相关sql语句如下: 12345678910111213141516171819SET FOREIGN_KEY_CHECKS=0;DROP TABLE IF EXISTS `job_entity`;CREATE TABLE `job_entity` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, `group` varchar(255) DEFAULT NULL, `cron` varchar(255) DEFAULT NULL, `parameter` varchar(255) NOT NULL, `description` varchar(255) DEFAULT NULL, `vm_param` varchar(255) DEFAULT NULL, `jar_path` varchar(255) DEFAULT NULL, `status` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8;INSERT INTO `job_entity` VALUES ('1', 'first', 'helloworld', '0/2 * * * * ? ', '1', '第一个', '', null, 'OPEN');INSERT INTO `job_entity` VALUES ('2', 'second', 'helloworld', '0/5 * * * * ? ', '2', '第二个', null, null, 'OPEN');INSERT INTO `job_entity` VALUES ('3', 'third', 'helloworld', '0/15 * * * * ? ', '3', '第三个', null, null, 'OPEN');INSERT INTO `job_entity` VALUES ('4', 'four', 'helloworld', '0 0/1 * * * ? *', '4', '第四个', null, null, 'CLOSE');INSERT INTO `job_entity` VALUES ('5', 'OLAY Job', 'Nomal', '0 0/2 * * * ?', '5', '第五个', null, 'C:\\\\EalenXie\\\\Download\\\\JDE-Order-1.0-SNAPSHOT.jar', 'CLOSE'); Quartz的核心配置类 : ConfigureQuartz.class 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.ealen.config;import org.quartz.spi.JobFactory;import org.quartz.spi.TriggerFiredBundle;import org.springframework.beans.factory.config.AutowireCapableBeanFactory;import org.springframework.beans.factory.config.PropertiesFactoryBean;import org.springframework.context.ApplicationContext;import org.springframework.context.ApplicationContextAware;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.core.io.ClassPathResource;import org.springframework.scheduling.quartz.SchedulerFactoryBean;import org.springframework.scheduling.quartz.SpringBeanJobFactory;import javax.sql.DataSource;import java.io.IOException;import java.util.Properties;/** * Created by EalenXie on 2018/6/4 11:02 * Quartz的核心配置类 */@Configurationpublic class ConfigureQuartz { //配置JobFactory @Bean public JobFactory jobFactory(ApplicationContext applicationContext) { AutowiringSpringBeanJobFactory jobFactory = new AutowiringSpringBeanJobFactory(); jobFactory.setApplicationContext(applicationContext); return jobFactory; } /** * SchedulerFactoryBean这个类的真正作用提供了对org.quartz.Scheduler的创建与配置，并且会管理它的生命周期与Spring同步。 * org.quartz.Scheduler: 调度器。所有的调度都是由它控制。 * @param dataSource 为SchedulerFactory配置数据源 * @param jobFactory 为SchedulerFactory配置JobFactory */ @Bean public SchedulerFactoryBean schedulerFactoryBean(DataSource dataSource, JobFactory jobFactory) throws IOException { SchedulerFactoryBean factory = new SchedulerFactoryBean(); //可选,QuartzScheduler启动时更新己存在的Job,这样就不用每次修改targetObject后删除qrtz_job_details表对应记录 factory.setOverwriteExistingJobs(true); factory.setAutoStartup(true); //设置自行启动 factory.setDataSource(dataSource); factory.setJobFactory(jobFactory); factory.setQuartzProperties(quartzProperties()); return factory; } //从quartz.properties文件中读取Quartz配置属性 @Bean public Properties quartzProperties() throws IOException { PropertiesFactoryBean propertiesFactoryBean = new PropertiesFactoryBean(); propertiesFactoryBean.setLocation(new ClassPathResource(\"/quartz.properties\")); propertiesFactoryBean.afterPropertiesSet(); return propertiesFactoryBean.getObject(); } //配置JobFactory,为quartz作业添加自动连接支持 public final class AutowiringSpringBeanJobFactory extends SpringBeanJobFactory implements ApplicationContextAware { private transient AutowireCapableBeanFactory beanFactory; @Override public void setApplicationContext(final ApplicationContext context) { beanFactory = context.getAutowireCapableBeanFactory(); } @Override protected Object createJobInstance(final TriggerFiredBundle bundle) throws Exception { final Object job = super.createJobInstance(bundle); beanFactory.autowireBean(job); return job; } }} 写一个dao访问数据库，JobEntityRepository.class 123456789package com.ealen.dao;import com.ealen.entity.JobEntity;import org.springframework.data.repository.CrudRepository;/** * Created by EalenXie on 2018/6/4 14:27 */public interface JobEntityRepository extends CrudRepository&lt;JobEntity, Long&gt; { JobEntity getById(Integer id);} 5.定义调度中心执行逻辑的Job,这个Job的作用就是按照数据库中的Job的逻辑,规则去执行数据库中的Job。这里使用了一个自定义的枚举工具类StringUtils , StringUtils.class : 12345678910111213141516171819202122232425262728293031323334package com.ealen.util;import java.util.List;import java.util.Map;/** * Created by EalenXie on 2018/6/4 14:20 * 自定义枚举单例对象 StringUtil */public enum StringUtils { getStringUtil; //是否为空 public boolean isEmpty(String str) { return (str == null) || (str.length() == 0) || (str.equals(\"\")); } //去空格 public String trim(String str) { return str == null ? null : str.trim(); } //获取Map参数值 public String getMapString(Map&lt;String, String&gt; map) { String result = \"\"; for (Map.Entry entry : map.entrySet()) { result += entry.getValue() + \" \"; } return result; } //获取List参数值 public String getListString(List&lt;String&gt; list) { String result = \"\"; for (String s : list) { result += s + \" \"; } return result; }} 调度中心执行逻辑的Job。DynamicJob.class : 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package com.ealen.job;import com.ealen.util.StringUtils;import org.quartz.*;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Component;import java.io.*;import java.util.ArrayList;import java.util.List;/** * Created by EalenXie on 2018/6/4 14:29 * :@DisallowConcurrentExecution : 此标记用在实现Job的类上面,意思是不允许并发执行. * :注意org.quartz.threadPool.threadCount线程池中线程的数量至少要多个,否则@DisallowConcurrentExecution不生效 * :假如Job的设置时间间隔为3秒,但Job执行时间是5秒,设置@DisallowConcurrentExecution以后程序会等任务执行完毕以后再去执行,否则会在3秒时再启用新的线程执行 */@DisallowConcurrentExecution@Componentpublic class DynamicJob implements Job { private Logger logger = LoggerFactory.getLogger(DynamicJob.class); /** * 核心方法,Quartz Job真正的执行逻辑. * @param executorContext executorContext JobExecutionContext中封装有Quartz运行所需要的所有信息 * @throws JobExecutionException execute()方法只允许抛出JobExecutionException异常 */ @Override public void execute(JobExecutionContext executorContext) throws JobExecutionException { //JobDetail中的JobDataMap是共用的,从getMergedJobDataMap获取的JobDataMap是全新的对象 JobDataMap map = executorContext.getMergedJobDataMap(); String jarPath = map.getString(\"jarPath\"); String parameter = map.getString(\"parameter\"); String vmParam = map.getString(\"vmParam\"); logger.info(\"Running Job name : {} \", map.getString(\"name\")); logger.info(\"Running Job description : \" + map.getString(\"JobDescription\")); logger.info(\"Running Job group: {} \", map.getString(\"group\")); logger.info(\"Running Job cron : \" + map.getString(\"cronExpression\")); logger.info(\"Running Job jar path : {} \", jarPath); logger.info(\"Running Job parameter : {} \", parameter); logger.info(\"Running Job vmParam : {} \", vmParam); long startTime = System.currentTimeMillis(); if (!StringUtils.getStringUtil.isEmpty(jarPath)) { File jar = new File(jarPath); if (jar.exists()) { ProcessBuilder processBuilder = new ProcessBuilder(); processBuilder.directory(jar.getParentFile()); List&lt;String&gt; commands = new ArrayList&lt;&gt;(); commands.add(\"java\"); if (!StringUtils.getStringUtil.isEmpty(vmParam)) commands.add(vmParam); commands.add(\"-jar\"); commands.add(jarPath); if (!StringUtils.getStringUtil.isEmpty(parameter)) commands.add(parameter); processBuilder.command(commands); logger.info(\"Running Job details as follows &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;: \"); logger.info(\"Running Job commands : {} \", StringUtils.getStringUtil.getListString(commands)); try { Process process = processBuilder.start(); logProcess(process.getInputStream(), process.getErrorStream()); } catch (IOException e) { throw new JobExecutionException(e); } } else throw new JobExecutionException(\"Job Jar not found &gt;&gt; \" + jarPath); } long endTime = System.currentTimeMillis(); logger.info(\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Running Job has been completed , cost time : \" + (endTime - startTime) + \"ms\\n\"); } //打印Job执行内容的日志 private void logProcess(InputStream inputStream, InputStream errorStream) throws IOException { String inputLine; String errorLine; BufferedReader inputReader = new BufferedReader(new InputStreamReader(inputStream)); BufferedReader errorReader = new BufferedReader(new InputStreamReader(errorStream)); while ((inputLine = inputReader.readLine()) != null) logger.info(inputLine); while ((errorLine = errorReader.readLine()) != null) logger.error(errorLine); }} 6.为了方便控制Job的运行，为调度中心添加相关的业务逻辑 :DynamicJobService.class : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.ealen.service;import com.ealen.dao.JobEntityRepository;import com.ealen.entity.JobEntity;import com.ealen.job.DynamicJob;import org.quartz.*;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.ArrayList;import java.util.List;/** * Created by EalenXie on 2018/6/4 14:25 */@Servicepublic class DynamicJobService { @Autowired private JobEntityRepository repository; //通过Id获取Job public JobEntity getJobEntityById(Integer id) { return repository.getById(id); } //从数据库中加载获取到所有Job public List&lt;JobEntity&gt; loadJobs() { List&lt;JobEntity&gt; list = new ArrayList&lt;&gt;(); repository.findAll().forEach(list::add); return list; } //获取JobDataMap.(Job参数对象) public JobDataMap getJobDataMap(JobEntity job) { JobDataMap map = new JobDataMap(); map.put(\"name\", job.getName()); map.put(\"group\", job.getGroup()); map.put(\"cronExpression\", job.getCron()); map.put(\"parameter\", job.getParameter()); map.put(\"JobDescription\", job.getDescription()); map.put(\"vmParam\", job.getVmParam()); map.put(\"jarPath\", job.getJarPath()); map.put(\"status\", job.getStatus()); return map; } //获取JobDetail,JobDetail是任务的定义,而Job是任务的执行逻辑,JobDetail里会引用一个Job Class来定义 public JobDetail geJobDetail(JobKey jobKey, String description, JobDataMap map) { return JobBuilder.newJob(DynamicJob.class) .withIdentity(jobKey) .withDescription(description) .setJobData(map) .storeDurably() .build(); } //获取Trigger (Job的触发器,执行规则) public Trigger getTrigger(JobEntity job) { return TriggerBuilder.newTrigger() .withIdentity(job.getName(), job.getGroup()) .withSchedule(CronScheduleBuilder.cronSchedule(job.getCron())) .build(); } //获取JobKey,包含Name和Group public JobKey getJobKey(JobEntity job) { return JobKey.jobKey(job.getName(), job.getGroup()); }} JobController.class : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104package com.ealen.web;import com.ealen.entity.JobEntity;import com.ealen.service.DynamicJobService;import org.quartz.*;import org.quartz.impl.matchers.GroupMatcher;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.scheduling.quartz.SchedulerFactoryBean;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import javax.annotation.PostConstruct;import java.util.Set;/** * Created by EalenXie on 2018/6/4 16:12 */@RestControllerpublic class JobController { private static final Logger logger = LoggerFactory.getLogger(JobController.class); @Autowired private SchedulerFactoryBean schedulerFactoryBean; @Autowired private DynamicJobService jobService; //初始化启动所有的Job @PostConstruct public void initialize() { try { reStartAllJobs(); logger.info(\"INIT SUCCESS\"); } catch (SchedulerException e) { logger.info(\"INIT EXCEPTION : \" + e.getMessage()); e.printStackTrace(); } } //根据ID重启某个Job @RequestMapping(\"/refresh/{id}\") public String refresh(@PathVariable Integer id) throws SchedulerException { String result; JobEntity entity = jobService.getJobEntityById(id); if (entity == null) return \"error: id is not exist \"; synchronized (logger) { JobKey jobKey = jobService.getJobKey(entity); Scheduler scheduler = schedulerFactoryBean.getScheduler(); scheduler.pauseJob(jobKey); scheduler.unscheduleJob(TriggerKey.triggerKey(jobKey.getName(), jobKey.getGroup())); scheduler.deleteJob(jobKey); JobDataMap map = jobService.getJobDataMap(entity); JobDetail jobDetail = jobService.geJobDetail(jobKey, entity.getDescription(), map); if (entity.getStatus().equals(\"OPEN\")) { scheduler.scheduleJob(jobDetail, jobService.getTrigger(entity)); result = \"Refresh Job : \" + entity.getName() + \"\\t jarPath: \" + entity.getJarPath() + \" success !\"; } else { result = \"Refresh Job : \" + entity.getName() + \"\\t jarPath: \" + entity.getJarPath() + \" failed ! , \" + \"Because the Job status is \" + entity.getStatus(); } } return result; } //重启数据库中所有的Job @RequestMapping(\"/refresh/all\") public String refreshAll() { String result; try { reStartAllJobs(); result = \"SUCCESS\"; } catch (SchedulerException e) { result = \"EXCEPTION : \" + e.getMessage(); } return \"refresh all jobs : \" + result; } /** * 重新启动所有的job */ private void reStartAllJobs() throws SchedulerException { synchronized (logger) { //只允许一个线程进入操作 Scheduler scheduler = schedulerFactoryBean.getScheduler(); Set&lt;JobKey&gt; set = scheduler.getJobKeys(GroupMatcher.anyGroup()); scheduler.pauseJobs(GroupMatcher.anyGroup()); //暂停所有JOB for (JobKey jobKey : set) { //删除从数据库中注册的所有JOB scheduler.unscheduleJob(TriggerKey.triggerKey(jobKey.getName(), jobKey.getGroup())); scheduler.deleteJob(jobKey); } for (JobEntity job : jobService.loadJobs()) { //从数据库中注册的所有JOB logger.info(\"Job register name : {} , group : {} , cron : {}\", job.getName(), job.getGroup(), job.getCron()); JobDataMap map = jobService.getJobDataMap(job); JobKey jobKey = jobService.getJobKey(job); JobDetail jobDetail = jobService.geJobDetail(jobKey, job.getDescription(), map); if (job.getStatus().equals(\"OPEN\")) scheduler.scheduleJob(jobDetail, jobService.getTrigger(job)); else logger.info(\"Job jump name : {} , Because {} status is {}\", job.getName(), job.getName(), job.getStatus()); } } }} 7.现在已经完事具备,只欠东风了,写一个启动类,跑一下看看效果。 123456789101112package com.ealen;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;/** * Created by EalenXie on 2018/6/4 11:00 */@SpringBootApplicationpublic class QuartzApplication { public static void main(String[] args) { SpringApplication.run(QuartzApplication.class, args); }} 运行效果如下 : 看到这里，那么恭喜你，Quartz集群已经搭建成功了。如果部署该项目应用到多个服务器上面，Job会在多个服务器上面执行，但同一个Job只会在某个服务器上面执行，即如果服务器A在某个时间执行了某个Job,则其他服务器如B,C,D在此时间均不会执行此Job。即不会造成该Job被多次执行。 这里可以看到数据库中的Job已经在Quartz注册并初始化成功了。 这里可看到Scheduler已经在工作了，Job也已经按照cron在定时执行了。执行一个包含jar的Job，看一下效果，图下已经看到JOB在正常运行了 :看到Job执行完成了。 此时如果在数据库中手动修改某个Job的执行cron，并不会马上生效，则可以调用上面写到的业务方法，/refresh/all，则可刷新所有的Job，或/refresh/{id},刷新某个Job。","link":"/2019/07/17/springboot-quarz/"},{"title":"springboot session","text":"SpringBoot+SpringSession+Redis实现session共享及单点登录最近在学习springboot，session这个点一直困扰了我好久，今天把这些天踩的坑分享出来吧，希望能帮助更多的人。 一、pom.xml配置 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 二、application.properties的redis配置 1234567891011#redisspring.redis.host=127.0.0.1spring.redis.port=6379spring.redis.password=123456spring.redis.pool.max-idle=8spring.redis.pool.min-idle=0spring.redis.pool.max-active=8spring.redis.pool.max-wait=-1#超时一定要大于0spring.redis.timeout=3000spring.session.store-type=redis 在配置redis时需要确保redis安装正确，并且配置notify-keyspace-events Egx，spring.redis.timeout设置为大于0，我当时这里配置为0时springboot时启不起来。 三、编写登录状态拦截器RedisSessionInterceptor 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960//拦截登录失效的请求public class RedisSessionInterceptor implements HandlerInterceptor{ @Autowired private StringRedisTemplate redisTemplate; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //无论访问的地址是不是正确的，都进行登录验证，登录成功后的访问再进行分发，404的访问自然会进入到错误控制器中 HttpSession session = request.getSession(); if (session.getAttribute(\"loginUserId\") != null) { try { //验证当前请求的session是否是已登录的session String loginSessionId = redisTemplate.opsForValue().get(\"loginUser:\" + (long) session.getAttribute(\"loginUserId\")); if (loginSessionId != null &amp;&amp; loginSessionId.equals(session.getId())) { return true; } } catch (Exception e) { e.printStackTrace(); } } response401(response); return false; } private void response401(HttpServletResponse response) { response.setCharacterEncoding(\"UTF-8\"); response.setContentType(\"application/json; charset=utf-8\"); try { response.getWriter().print(JSON.toJSONString(new ReturnData(StatusCode.NEED_LOGIN, \"\", \"用户未登录！\"))); } catch (IOException e) { e.printStackTrace(); } } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { }} 四、配置拦截器 123456789101112131415161718@Configurationpublic class WebSecurityConfig extends WebMvcConfigurerAdapter{ @Bean public RedisSessionInterceptor getSessionInterceptor() { return new RedisSessionInterceptor(); } @Override public void addInterceptors(InterceptorRegistry registry) { //所有已api开头的访问都要进入RedisSessionInterceptor拦截器进行登录验证，并排除login接口(全路径)。必须写成链式，分别设置的话会创建多个拦截器。 //必须写成getSessionInterceptor()，否则SessionInterceptor中的@Autowired会无效 registry.addInterceptor(getSessionInterceptor()).addPathPatterns(\"/api/**\").excludePathPatterns(\"/api/user/login\"); super.addInterceptors(registry); }} 五、登录控制器 12345678910111213141516171819202122232425262728293031323334353637383940414243@RestController@RequestMapping(value = \"/api/user\")public class LoginController{ @Autowired private UserService userService; @Autowired private StringRedisTemplate redisTemplate; @RequestMapping(\"/login\") public ReturnData login(HttpServletRequest request, String account, String password) { User user = userService.findUserByAccountAndPassword(account, password); if (user != null) { HttpSession session = request.getSession(); session.setAttribute(\"loginUserId\", user.getUserId()); redisTemplate.opsForValue().set(\"loginUser:\" + user.getUserId(), session.getId()); return new ReturnData(StatusCode.REQUEST_SUCCESS, user, \"登录成功！\"); } else { throw new MyException(StatusCode.ACCOUNT_OR_PASSWORD_ERROR, \"账户名或密码错误！\"); } } @RequestMapping(value = \"/getUserInfo\") public ReturnData get(long userId) { User user = userService.findUserByUserId(userId); if (user != null) { return new ReturnData(StatusCode.REQUEST_SUCCESS, user, \"查询成功！\"); } else { throw new MyException(StatusCode.USER_NOT_EXIST, \"用户不存在！\"); } }} 六、效果 我在浏览器上登录，然后获取用户信息，再在postman上登录相同的账号，浏览器再获取用户信息，就会提示401错误了，浏览器需要重新登录才能获取得到用户信息，同样，postman上登录的账号就失效了。 浏览器： postman： 七、核心原理详解分布式session需要解决两个难点：1、正确配置redis让springboot把session托管到redis服务器。2、单点登录。 1、redis：redis需要能正确启动到出现如下效果才证明redis正常配置并启动 同时还要保证配置正确 123456789101112@EnableCaching@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 30)//session过期时间(秒)@Configurationpublic class RedisSessionConfig{ @Bean public static ConfigureRedisAction configureRedisAction() { //让springSession不再执行config命令 return ConfigureRedisAction.NO_OP; }} springboot启动后能在redis上查到缓存的session才能说明整个redis+springboot配置成功！ 2、单点登录：这里的实现我搜过大量资料，基本是基于第三方框架实现的，我在想，既然已经使用redis做集中缓存了那干嘛不用redis来做单点登录呢？ 1、用户登录时，在redis中记录该userId对应的sessionId，并将userId保存到session中。 HttpSession session = request.getSession();session.setAttribute(“loginUserId”, user.getUserId());redisTemplate.opsForValue().set(“loginUser:” + user.getUserId(), session.getId());2、访问接口时，会在RedisSessionInterceptor拦截器中的preHandle()中捕获，然后根据该请求发起者的session中保存的userId去redis查当前已登录的sessionId，若查到的sessionId与访问者的sessionId相等，那么说明请求合法，放行。否则抛出401异常给全局异常捕获器去返回给客户端401状态。 单点登录经过我的验证后满足需求，暂时没有出现问题，也希望大家能看看有没有问题，有的话给我点好的建议！","link":"/2019/10/15/springboot-session/"},{"title":"springboot拦截器配置fastjson配置","text":"为什么要集成fastjson而不是用springmvc内置的? 因为用fastjson 后，返回字符串时会在外面强制添加双引号例如:如果返回的字符串为 哈哈, 最终返回得到的数据是 “哈哈”返回的字符串中包含双引号，例如{“name”:”ethan”}，则 fastjson处理后，最终返回的结果是 “{&quot;name&quot;:&quot;ethan&quot;}” 要想得到原始字符串,解决办法:添加配置字符串转换器StringHttpMessageConverter 我们只需要加一个配置文件：WebMvcConfig.java: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.chinaedu.back.conf;import java.util.List;import org.springframework.context.annotation.Configuration;import org.springframework.http.converter.HttpMessageConverter;import org.springframework.web.servlet.config.annotation.InterceptorRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;import com.alibaba.fastjson.serializer.SerializerFeature;import com.alibaba.fastjson.support.config.FastJsonConfig;import com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter;import com.chinaedu.back.interceptor.LoginInterceptor;@Configurationpublic class WebMvcConfig extends WebMvcConfigurerAdapter { /** * 设置拦截器 */ @Override public void addInterceptors(InterceptorRegistry registry) { // 多个拦截器组成一个拦截器链 // addPathPatterns 用于添加拦截规则 excludePathPatterns 用户排除拦截 // registry.addInterceptor(new LoginInterceptor()).addPathPatterns(\"/**\").excludePathPatterns(\"/user/**\"); registry.addInterceptor(new LoginInterceptor()).addPathPatterns(\"/**\").excludePathPatterns(\"/toLogin\",\"/login\",\"/js/**\",\"/css/**\",\"/images/**\"); super.addInterceptors(registry); } /** * 配置fastjson */ @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) { super.configureMessageConverters(converters); // 初始化转换器 FastJsonHttpMessageConverter fastConvert = new FastJsonHttpMessageConverter(); // 初始化一个转换器配置 FastJsonConfig fastJsonConfig = new FastJsonConfig(); fastJsonConfig.setDateFormat(\"yyyy-MM-dd HH:mm:ss\"); fastJsonConfig.setSerializerFeatures(SerializerFeature.PrettyFormat); // 将配置设置给转换器并添加到HttpMessageConverter转换器列表中 fastConvert.setFastJsonConfig(fastJsonConfig); converters.add(fastConvert); }} https://blog.csdn.net/weixin_34240520/article/details/91877728","link":"/2019/09/20/springboot拦截器配置fastjson/"},{"title":"springboot中的json、gson、fastjson如何使用与日期格式转换","text":"关于如何引用json、gson、fastjsonsrpngboot中默认用的是json格式，如果需要使用gson和fastjson其中一种格式的话，首先需要在pom文件中排除对json格式的依赖，再去引入你想要gson或者fastjson当中的一种。代码如下：下面这种是引入fastjson 12345678910111213141516171819202122232425262728&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;!--排除对json格式的依赖--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-json&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!--引入gson格式的依赖--&gt; &lt;!-- &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;/dependency&gt;--&gt; &lt;!--引入fastjson格式的依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.49&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependency&gt; json格式日期转换默认不处理传给前台的json格式形式如下： 第一种通过在实体类需要转换的日期属性上加上@JsonFormat(pattern = “yyyy-MM-dd”)。缺点就是如果有多个实体类都有日期属性都需要日期转换，那么都需要加。第二种就是自定义一个WebMvcConfig类，类中加上自定义的bean。那么整个项目的json格式日期都会按照这个格式来转换。如果就是有多个类中都有日期需要转换，但是已经在全局配置中定义转换格式，但是某个类中日期转换又不想用全局的日期转换格式，此时可以在这个类上加上@JsonFormat(pattern = “yyyy-MM-dd”)指明需要格式即可。转换后如下： 1234567891011@Configurationpublic class WebMvcConfig { @Bean MappingJackson2HttpMessageConverter mappingJackson2HttpMessageConverter() { MappingJackson2HttpMessageConverter converter = new MappingJackson2HttpMessageConverter(); ObjectMapper om = new ObjectMapper(); om.setDateFormat(new SimpleDateFormat(\"yyyy/MM/dd\")); converter.setObjectMapper(om); return converter; } } gson下：需要排除json依赖，引入gson依赖 12345678910111213@Configurationpublic class WebMvcConfig { @Bean GsonHttpMessageConverter gsonHttpMessageConverter() { GsonHttpMessageConverter converter = new GsonHttpMessageConverter(); converter.setGson(new GsonBuilder().setDateFormat(\"yyyy/MM/dd\").create()); return converter; } @Bean Gson gson() { return new GsonBuilder().setDateFormat(\"yyyy/MM/dd\").create(); }} fastjson下：需要排除json依赖，引入fastjson下 1234567891011@Configurationpublic class WebMvcConfig { @Bean FastJsonHttpMessageConverter fastJsonHttpMessageConverter() { FastJsonHttpMessageConverter converter = new FastJsonHttpMessageConverter(); FastJsonConfig config = new FastJsonConfig(); config.setDateFormat(\"yyyy-MM-dd\"); converter.setFastJsonConfig(config); return converter; }}","link":"/2019/09/20/springboot中的json、gson、fastjson如何使用与日期格式转换/"},{"title":"springboot整合mybatis逆向工程","text":"1.添加依赖 12345678910111213141516171819&lt;!--mysql依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- SpringBoot - MyBatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- SpringBoot - MyBatis 逆向工程 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt;&lt;/dependency&gt; 2.添加插件 1234567891011121314151617181920&lt;!-- MyBatis 逆向工程 插件 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;configuration&gt; &lt;!--允许移动生成的文件 --&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;!-- 是否覆盖 --&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt; mysql&lt;/groupId&gt; &lt;artifactId&gt; mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.30&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/plugin&gt; 3.application配置mysql连接 123456spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/wechat username: root password: root 4.generatorConfig.xml配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\" \"http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\"&gt;&lt;generatorConfiguration&gt; &lt;context id=\"DB2Tables\" targetRuntime=\"MyBatis3\"&gt; &lt;!--去掉注释--&gt; &lt;commentGenerator&gt; &lt;property name=\"suppressAllComments\" value=\"true\"/&gt; &lt;/commentGenerator&gt; &lt;!--需要配置数据库连接--&gt; &lt;jdbcConnection driverClass=\"com.mysql.jdbc.Driver\" connectionURL=\"jdbc:mysql://****:3306/competition?characterEncoding=utf-8&amp;amp;useSSL=false\" userId=\"****\" password=\"****\"&gt; &lt;/jdbcConnection&gt; &lt;javaTypeResolver &gt; &lt;property name=\"forceBigDecimals\" value=\"false\" /&gt; &lt;/javaTypeResolver&gt; &lt;!--指定entity生成的位置--&gt; &lt;javaModelGenerator targetPackage=\"com.example.competition.dao.entity\" targetProject=\"./src/main/java\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\" /&gt; &lt;property name=\"trimStrings\" value=\"true\" /&gt; &lt;/javaModelGenerator&gt; &lt;!--sql映射文件生成的位置 mapper.xml--&gt; &lt;sqlMapGenerator targetPackage=\"mapper\" targetProject=\"./src/main/resources\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\" /&gt; &lt;/sqlMapGenerator&gt; &lt;!--指定DaoMapper生成的位置 mapper.java--&gt; &lt;javaClientGenerator type=\"XMLMAPPER\" targetPackage=\"com.example.competition.dao.mapper\" targetProject=\"./src/main/java\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\" /&gt; &lt;/javaClientGenerator&gt; &lt;!--table是指定每个表的生成策略--&gt; &lt;!--&lt;table tableName=\"department\" domainObjectName=\"Department\"&gt;&lt;/table&gt;--&gt; &lt;!--&lt;table tableName=\"group_teacher_rel\" domainObjectName=\"Group_teacher_rel\"&gt;&lt;/table&gt;--&gt; &lt;!--&lt;table tableName=\"groups\" domainObjectName=\"Groups\"&gt;&lt;/table&gt;--&gt; &lt;!--&lt;table tableName=\"specialty\" domainObjectName=\"Specialty\"&gt;&lt;/table&gt;--&gt; &lt;!--&lt;table tableName=\"student\" domainObjectName=\"Student\"&gt;&lt;/table&gt;--&gt; &lt;!--&lt;table tableName=\"teacher\" domainObjectName=\"Teacher\"&gt;&lt;/table&gt;--&gt; &lt;table tableName=\"test\" domainObjectName=\"Test\"&gt;&lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 5.启动类Generator 123456789101112131415161718192021222324252627282930313233public class Generator { public static void main(String[] args){ List&lt;String&gt; warnings = new ArrayList&lt;&gt;(); boolean overwrite = true; String genCfg = \"/generatorConfig.xml\"; File configFile = new File(Generator.class.getResource(genCfg).getFile()); ConfigurationParser cp = new ConfigurationParser(warnings); Configuration config = null; try { config = cp.parseConfiguration(configFile); } catch (IOException e) { e.printStackTrace(); } catch (XMLParserException e) { e.printStackTrace(); } DefaultShellCallback callback = new DefaultShellCallback(overwrite); MyBatisGenerator myBatisGenerator = null; try { myBatisGenerator = new MyBatisGenerator(config, callback, warnings); } catch (InvalidConfigurationException e) { e.printStackTrace(); } try { myBatisGenerator.generate(null); } catch (SQLException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } }}","link":"/2019/09/12/springboot整合mybatis逆向工程/"},{"title":"SpringBoot整合Redis","text":"SpringBoot整合Redis 首先做好准备工作，在本地安装一个redis，然后启动redis。出现下图页面就启动成功了。然后新建项目，加入redis依赖，pom文件如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.dalaoyang&lt;/groupId&gt; &lt;artifactId&gt;springboot_redis&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;springboot_redis&lt;/name&gt; &lt;description&gt;springboot_redis&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 然后在application.properties加入redis配置： 123456789101112131415161718192021##端口号server.port=8888# Redis数据库索引（默认为0）spring.redis.database=0 # Redis服务器地址spring.redis.host=localhost# Redis服务器连接端口spring.redis.port=6379 # Redis服务器连接密码（默认为空）spring.redis.password=#连接池最大连接数（使用负值表示没有限制）spring.redis.pool.max-active=8 # 连接池最大阻塞等待时间（使用负值表示没有限制）spring.redis.pool.max-wait=-1 # 连接池中的最大空闲连接spring.redis.pool.max-idle=8 # 连接池中的最小空闲连接spring.redis.pool.min-idle=0 # 连接超时时间（毫秒）spring.redis.timeout=0 RedisConfig配置类，其中@EnableCaching开启注解 123456789101112131415161718192021222324252627282930313233343536package com.dalaoyang.config;import org.springframework.cache.CacheManager;import org.springframework.cache.annotation.CachingConfigurerSupport;import org.springframework.cache.annotation.EnableCaching;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.cache.RedisCacheManager;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.core.RedisTemplate;/** * @author dalaoyang * @Description * @project springboot_learn * @package com.dalaoyang.config * @email yangyang@dalaoyang.cn * @date 2018/4/18 */@Configuration@EnableCaching//开启缓存public class RedisConfig extends CachingConfigurerSupport { @Bean public CacheManager cacheManager(RedisTemplate&lt;?,?&gt; redisTemplate) { CacheManager cacheManager = new RedisCacheManager(redisTemplate); return cacheManager; } @Bean public RedisTemplate&lt;String, String&gt; redisTemplate(RedisConnectionFactory factory) { RedisTemplate&lt;String, String&gt; redisTemplate = new RedisTemplate&lt;String, String&gt;(); redisTemplate.setConnectionFactory(factory); return redisTemplate; }} 由于只是简单整合，我只创建了一个RedisService来用来存取缓存数据，实际项目中可以根据需求创建interface，impl等等，代码如下： 1234567891011121314151617181920212223242526272829303132333435package com.dalaoyang.service;import javax.annotation.Resource;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.core.ValueOperations;import org.springframework.data.redis.serializer.RedisSerializer;import org.springframework.data.redis.serializer.StringRedisSerializer;import org.springframework.stereotype.Service;/** * @author dalaoyang * @Description * @project springboot_learn * @package com.dalaoyang.service * @email yangyang@dalaoyang.cn * @date 2018/4/18 */@Servicepublic class RedisService { @Resource private RedisTemplate&lt;String,Object&gt; redisTemplate; public void set(String key, Object value) { //更改在redis里面查看key编码问题 RedisSerializer redisSerializer =new StringRedisSerializer(); redisTemplate.setKeySerializer(redisSerializer); ValueOperations&lt;String,Object&gt; vo = redisTemplate.opsForValue(); vo.set(key, value); } public Object get(String key) { ValueOperations&lt;String,Object&gt; vo = redisTemplate.opsForValue(); return vo.get(key); }} 实体类City： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.dalaoyang.entity;import java.io.Serializable;/** * @author dalaoyang * @Description * @project springboot_learn * @package com.dalaoyang.Entity * @email 397600342@qq.com * @date 2018/4/7 */public class City implements Serializable { private int cityId; private String cityName; private String cityIntroduce; public City(int cityId, String cityName, String cityIntroduce) { this.cityId = cityId; this.cityName = cityName; this.cityIntroduce = cityIntroduce; } public City(String cityName, String cityIntroduce) { this.cityName = cityName; this.cityIntroduce = cityIntroduce; } public City() { } public int getCityId() { return cityId; } public void setCityId(int cityId) { this.cityId = cityId; } public String getCityName() { return cityName; } public void setCityName(String cityName) { this.cityName = cityName; } public String getCityIntroduce() { return cityIntroduce; } public void setCityIntroduce(String cityIntroduce) { this.cityIntroduce = cityIntroduce; }} 测试类CityController 12345678910111213141516171819202122232425262728293031323334353637383940package com.dalaoyang.controller;import com.dalaoyang.entity.City;import com.dalaoyang.service.RedisService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;/** * @author dalaoyang * @Description * @project springboot_learn * @package com.dalaoyang.controller * @email 397600342@qq.com * @date 2018/4/7 */@RestControllerpublic class CityController { @Autowired private RedisService redisService; //http://localhost:8888/saveCity?cityName=北京&amp;cityIntroduce=中国首都&amp;cityId=1 @GetMapping(value = \"saveCity\") public String saveCity(int cityId,String cityName,String cityIntroduce){ City city = new City(cityId,cityName,cityIntroduce); redisService.set(cityId+\"\",city); return \"success\"; } //http://localhost:8888/getCityById?cityId=1 @GetMapping(value = \"getCityById\") public City getCity(int cityId){ City city = (City) redisService.get(cityId+\"\"); return city; }} 到这里配置基本上都完成了，然后启动项目访问http://localhost:8888/saveCity?cityName=北京&amp;cityIntroduce=中国首都&amp;cityId=18","link":"/2019/08/10/springboot整合redis/"},{"title":"registry center compare Eureka or ZooKeeper","text":"Eureka的工作原理以及它与ZooKeeper的区别1、Eureka 简介： Eureka 是 Netflix 出品的用于实现服务注册和发现的工具。 Spring Cloud 集成了 Eureka，并提供了开箱即用的支持。其中， Eureka 又可细分为 Eureka Server 和 Eureka Client。 1.基本原理上图是来自eureka的官方架构图，这是基于集群配置的eureka； 处于不同节点的eureka通过Replicate进行数据同步 Application Service为服务提供者 Application Client为服务消费者 Make Remote Call完成一次服务调用 服务启动后向Eureka注册，Eureka Server会将注册信息向其他Eureka Server进行同步，当服务消费者要调用服务提供者，则向服务注册中心获取服务提供者地址，然后会将服务提供者地址缓存在本地，下次再调用时，则直接从本地缓存中取，完成一次调用。 当服务注册中心Eureka Server检测到服务提供者因为宕机、网络原因不可用时，则在服务注册中心将服务置为DOWN状态，并把当前服务提供者状态向订阅者发布，订阅过的服务消费者更新本地缓存。 服务提供者在启动后，周期性（默认30秒）向Eureka Server发送心跳，以证明当前服务是可用状态。Eureka Server在一定的时间（默认90秒）未收到客户端的心跳，则认为服务宕机，注销该实例。 2.Eureka的自我保护机制在默认配置中，Eureka Server在默认90s没有得到客户端的心跳，则注销该实例，但是往往因为微服务跨进程调用，网络通信往往会面临着各种问题，比如微服务状态正常，但是因为网络分区故障时，Eureka Server注销服务实例则会让大部分微服务不可用，这很危险，因为服务明明没有问题。 为了解决这个问题，Eureka 有自我保护机制，通过在Eureka Server配置如下参数，可启动保护机制 eureka.server.enable-self-preservation=true它的原理是，当Eureka Server节点在短时间内丢失过多的客户端时（可能发送了网络故障），那么这个节点将进入自我保护模式，不再注销任何微服务，当网络故障回复后，该节点会自动退出自我保护模式。 自我保护模式的架构哲学是宁可放过一个，决不可错杀一千 作为服务注册中心，Eureka比Zookeeper好在哪里著名的CAP理论指出，一个分布式系统不可能同时满足C(一致性)、A(可用性)和P(分区容错性)。由于分区容错性在是分布式系统中必须要保证的，因此我们只能在A和C之间进行权衡。在此Zookeeper保证的是CP, 而Eureka则是AP。 3.1 Zookeeper保证CP当向注册中心查询服务列表时，我们可以容忍注册中心返回的是几分钟以前的注册信息，但不能接受服务直接down掉不可用。也就是说，服务注册功能对可用性的要求要高于一致性。但是zk会出现这样一种情况，当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30 ~ 120s, 且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪。在云部署的环境下，因网络问题使得zk集群失去master节点是较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。 3.2 Eureka保证APEureka看明白了这一点，因此在设计时就优先保证可用性。Eureka各个节点都是平等的，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka的客户端在向某个Eureka注册或时如果发现连接失败，则会自动切换至其它节点，只要有一台Eureka还在，就能保证注册服务可用(保证可用性)，只不过查到的信息可能不是最新的(不保证强一致性)。除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况： Eureka不再从注册列表中移除因为长时间没收到心跳而应该过期的服务 Eureka仍然能够接受新服务的注册和查询请求，但是不会被同步到其它节点上(即保证当前节点依然可用) 当网络稳定时，当前实例新的注册信息会被同步到其它节点中 因此， Eureka可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像zookeeper那样使整个注册服务瘫痪。 总结Eureka作为单纯的服务注册中心来说要比zookeeper更加“专业”，因为注册服务更重要的是可用性，我们可以接受短期内达不到一致性的状况。不过Eureka目前1.X版本的实现是基于servlet的Java web应用，它的极限性能肯定会受到影响。期待正在开发之中的2.X版本能够从servlet中独立出来成为单独可部署执行的服务。","link":"/2019/07/18/registry/"},{"title":"springboot过滤器与拦截器","text":"一、拦截器与过滤器 在讲Spring boot之前，我们先了解一下过滤器和拦截器。这两者在功能方面很类似，但是在具体技术实现方面，差距还是比较大的。在分析两者的区别之前，我们先理解一下AOP的概念，AOP不是一种具体的技术，而是一种编程思想。在面向对象编程的过程中，我们很容易通过继承、多态来解决纵向扩展。 但是对于横向的功能，比如，在所有的service方法中开启事务，或者统一记录日志等功能，面向对象的是无法解决的。所以AOP——面向切面编程其实是面向对象编程思想的一个补充。而我们今天讲的过滤器和拦截器都属于面向切面编程的具体实现。而两者的主要区别包括以下几个方面： 1、Filter是依赖于Servlet容器，属于Servlet规范的一部分，而拦截器则是独立存在的，可以在任何情况下使用。 2、Filter的执行由Servlet容器回调完成，而拦截器通常通过动态代理的方式来执行。 3、Filter的生命周期由Servlet容器管理，而拦截器则可以通过IoC容器来管理，因此可以通过注入等方式来获取其他Bean的实例，因此使用会更方便。 二、过滤器的配置 现在我们通过过滤器来实现记录请求执行时间的功能，其实现如下： 123456789101112131415161718 public class LogCostFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { long start = System.currentTimeMillis(); filterChain.doFilter(servletRequest,servletResponse); System.out.println(\"Execute cost=\"+(System.currentTimeMillis()-start)); } @Override public void destroy() { }} 这段代码的逻辑比较简单，就是在方法执行前先记录时间戳，然后通过过滤器链完成请求的执行，在返回结果之间计算执行的时间。这里需要主要，这个类必须继承Filter类，这个是Servlet的规范，这个跟以前的Web项目没区别。但是，有了过滤器类以后，以前的web项目可以在web.xml中进行配置，但是spring boot项目并没有web.xml这个文件，那怎么配置？在Spring boot中，我们需要FilterRegistrationBean来完成配置。其实现过程如下： 1234567891011121314@Configurationpublic class FilterConfig { @Bean public FilterRegistrationBean registFilter() { FilterRegistrationBean registration = new FilterRegistrationBean(); registration.setFilter(new LogCostFilter()); registration.addUrlPatterns(\"/*\"); registration.setName(\"LogCostFilter\"); registration.setOrder(1); return registration; } } 这样配置就完成了，需要配置的选项主要包括实例化Filter类，然后指定url的匹配模式，设置过滤器名称和执行顺序，这个过程和在web.xml中配置其实没什么区别，只是形式不同而已。现在我们可以启动服务器访问任意URL： 大家可以看到上面的配置已经生效了。除了通过 FilterRegistrationBean 来配置以外，还有一种更直接的办法，直接通过注解就可以完成了： 12345678910111213141516171819 @WebFilter(urlPatterns = \"/*\", filterName = \"logFilter2\")public class LogCostFilter2 implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { long start = System.currentTimeMillis(); filterChain.doFilter(servletRequest, servletResponse); System.out.println(\"LogFilter2 Execute cost=\" + (System.currentTimeMillis() - start)); } @Override public void destroy() { }} 这里直接用@WebFilter就可以进行配置，同样，可以设置url匹配模式，过滤器名称等。这里需要注意一点的是@WebFilter这个注解是Servlet3.0的规范，并不是Spring boot提供的。除了这个注解以外，我们还需在配置类中加另外一个注解：@ServletComponetScan，指定扫描的包。 12345678@SpringBootApplication@MapperScan(\"com.pandy.blog.dao\")@ServletComponentScan(\"com.pandy.blog.filters\")public class Application { public static void main(String[] args) throws Exception { SpringApplication.run(Application.class, args); }} 现在，我们再来访问一下任意URL： 可以看到，我们配置的两个过滤器都生效了。细心的读者会发现，第二个Filter我们并没有指定执行的顺序，但是却在第一个Filter之前执行。这里需要解释一下，@WebFilter这个注解并没有指定执行顺序的属性，其执行顺序依赖于Filter的名称，是根据Filter类名（注意不是配置的filter的名字）的字母顺序倒序排列，并且@WebFilter指定的过滤器优先级都高于FilterRegistrationBean配置的过滤器。有兴趣的朋友可以自己实验一下。 三、拦截器的配置 上面我们已经介绍了过滤器的配置方法，接下来我们再来看看如何配置一个拦截器。我们使用拦截器来实现上面同样的功能，记录请求的执行时间。首先我们实现拦截器类： 1234567891011121314151617 public class LogCostInterceptor implements HandlerInterceptor { long start = System.currentTimeMillis(); @Override public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception { start = System.currentTimeMillis(); return true; } @Override public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception { System.out.println(\"Interceptor cost=\"+(System.currentTimeMillis()-start)); } @Override public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception { }} 这里我们需要实现HandlerInterceptor这个接口，这个接口包括三个方法，preHandle是请求执行前执行的，postHandler是请求结束执行的，但只有preHandle方法返回true的时候才会执行，afterCompletion是视图渲染完成后才执行，同样需要preHandle返回true，该方法通常用于清理资源等工作。除了实现上面的接口外，我们还需对其进行配置： 123456789@Configurationpublic class InterceptorConfig extends WebMvcConfigurerAdapter { @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(new LogCostInterceptor()).addPathPatterns(\"/**\"); super.addInterceptors(registry); }} 这里我们继承了WebMVCConfigurerAdapter，看过前面的文章的朋友应该已经见过这个类了，在进行静态资源目录配置的时候我们用到过这个类。这里我们重写了addInterceptors这个方法，进行拦截器的配置，主要配置项就两个，一个是指定拦截器，第二个是指定拦截的URL。现在我们再启动系统访问任意一个URL： 可以看到，我们通过拦截器实现了同样的功能。不过这里还要说明一点的是，其实这个实现是有问题的，因为preHandle和postHandle是两个方法，所以我们这里不得不设置一个共享变量start来存储开始值，但是这样就会存在线程安全问题。当然，我们可以通过其他方法来解决，比如通过ThreadLocal就可以很好的解决这个问题，有兴趣的同学可以自己实现。不过通过这一点我们其实可以看到，虽然拦截器在很多场景下优于过滤器，但是在这种场景下，过滤器比拦截器实现起来更简单。 四、总结 本文主要对基于Spring boot对过滤器和拦截器的配置进行的讲解。无论是过滤器还是拦截器都属于AOP（面向切面编程）思想的具体实现。除了这两种实现我们还见过另一种更灵活的AOP实现技术，即Aspect，我们可以通过Aspect来完成更多更强大的功能。这个后续再给大家分享。","link":"/2019/09/05/springboot过滤器与拦截器/"},{"title":"springboot配置mvc和静态资源路径","text":"1234567891011121314151617spring: #配置控制台允许彩色输出 ANSI是一种字符代码，为使计算机支持更多语言，通常使用 0x00~0x7f 范围的1 个字节来表示 1 个英文字符。超出此范围的使用0x80~0xFFFF来编码，即扩展的ASCII编码。 output: ansi: enabled: ALWAYS jackson: time-zone: GMT+8 date-format: yyyy-MM-dd HH:mm:ss gson: date-format: yyyy-MM-dd HH:mm:ss pid: file: /var/run/ensbrain/server.pid mvc: static-path-pattern: /** #指定静态资源存放的地址放前端打包的东西 resources: static-locations: file:/usr/local/ensbrain/www/ 自定义日志等级高亮颜色ANSI 简单输出测试 123public static void main(String[] args) { System.out.println(\"\\033[32m test \\033[39m 时间\"); } ASNI 颜色编码说明 ANSIConstants ASNI 颜色 ESC_START \\033[ 编码开始标识 BLACK_FG 30 BLACK_FG RED_FG 31 RED_FG GREEN_FG 32 GREEN_FG YELLOW_FG 33 YELLOW_FG BLUE_FG 34 BLUE_FG MAGENTA_FG 35 MAGENTA_FG CYAN_FG 36 CYAN_FG WHITE_FG 37 WHITE_FG DEFAULT_FG 39 DEFAULT_FG ESC_END m 编码结束标识","link":"/2019/09/21/springboot配置mvc/"},{"title":"springboot配置数据库密码特殊字符","text":"springboot配置数据库密码特殊字符报错问题一般的springboot项目会有application.yml或者application.properties文件，开发中需要连接数据库时密码可能会有特殊字符，.properties文件不会报错，但是.yml文件会报错。 解决：yml中password对应的值用单引号引住（’!@test’）就可以了，如下 123456spring: datasource: password: '!@test' type: com.alibaba.druid.pool.DruidDataSource url: jdbc:mysql://localhost:3306/test?characterEncoding=utf-8&amp;serverTimezone=GMT%2B8 username: root","link":"/2019/09/16/springboot配置数据库密码特殊字符/"},{"title":"springboot集成druid","text":"Spring Boot教程(七)：Spring Boot集成druid连接池 一、项目准备直接使用上个章节的源码，Spring Boot教程(六)：Spring Boot集成mybatis 二、添加druid依赖 123456&lt;!-- druid --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.9&lt;/version&gt;&lt;/dependency&gt; 三、数据源配置在application.properties配置文件里添加druid的配置 123456789101112131415161718192021222324252627282930313233343536373839404142## 数据源配置#spring.datasource.url=jdbc:mysql://localhost:3306/springboot?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false#spring.datasource.username=root#spring.datasource.password=root#spring.datasource.driver-class-name=com.mysql.jdbc.Driver# 这4个参数key里不带druid也可以，即可以还用上面的这个4个参数spring.datasource.druid.url=jdbc:mysql://localhost:3306/springboot?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falsespring.datasource.druid.username=rootspring.datasource.druid.password=rootspring.datasource.druid.driver-class-name=com.mysql.jdbc.Driver# 初始化时建立物理连接的个数spring.datasource.druid.initial-size=5# 最大连接池数量spring.datasource.druid.max-active=30# 最小连接池数量spring.datasource.druid.min-idle=5# 获取连接时最大等待时间，单位毫秒spring.datasource.druid.max-wait=60000# 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒spring.datasource.druid.time-between-eviction-runs-millis=60000# 连接保持空闲而不被驱逐的最小时间spring.datasource.druid.min-evictable-idle-time-millis=300000# 用来检测连接是否有效的sql，要求是一个查询语句spring.datasource.druid.validation-query=SELECT 1 FROM DUAL# 建议配置为true，不影响性能，并且保证安全性。申请连接的时候检测，如果空闲时间大于timeBetweenEvictionRunsMillis，执行validationQuery检测连接是否有效。spring.datasource.druid.test-while-idle=true# 申请连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。spring.datasource.druid.test-on-borrow=false# 归还连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。spring.datasource.druid.test-on-return=false# 是否缓存preparedStatement，也就是PSCache。PSCache对支持游标的数据库性能提升巨大，比如说oracle。在mysql下建议关闭。spring.datasource.druid.pool-prepared-statements=true# 要启用PSCache，必须配置大于0，当大于0时，poolPreparedStatements自动触发修改为true。spring.datasource.druid.max-pool-prepared-statement-per-connection-size=50# 配置监控统计拦截的filters，去掉后监控界面sql无法统计spring.datasource.druid.filters=stat,wall# 通过connectProperties属性来打开mergeSql功能；慢SQL记录spring.datasource.druid.connection-properties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500# 合并多个DruidDataSource的监控数据spring.datasource.druid.use-global-data-source-stat=true 四、测试启动服务，浏览器输入http://localhost:8080/users ,界面如下： 浏览器输入http://localhost:8080/druid ，界面如下： 打开mysql客户端navicat的sql窗口，执行show full processlist，显示如下内容： 可以看到，启动项目后，直接创建5个数据连接，这是由application.properties配置文件中spring.datasource.druid.initial-size=5控制的。 五、druid监控在步骤四我们可以看到，浏览器输入http://localhost:8080/druid直接就能看到druid控制台界面，在这里面可以看到很多项目信息，如果任凭用户随意访问，非常危险。我们可以通过配置，设置只有通过登录认证才可以访问。 在application.properties配置文件中增加： 12345# druid连接池监控spring.datasource.druid.stat-view-servlet.login-username=adminspring.datasource.druid.stat-view-servlet.login-password=123# 排除一些静态资源，以提高效率spring.datasource.druid.web-stat-filter.exclusions=*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/* 只需要配置用户名和密码，重启服务器后再次访问就需要登录才能访问。浏览器输入http://localhost:8080/druid ，界面如下 输入刚才配置文件里配置的用户名admin和密码123，登录之后便可以正常访问了。","link":"/2019/09/06/springboot集成druid/"},{"title":"springcloudstream-rabbitmq","text":"[TOCM] [TOC] 依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;&lt;/dependency&gt; 同样，创建使用者项目，但只能创建spring-cloud-starter-stream-rabbit依赖项。 1.建立生产者如前所述，消息从发布者传递到交换并传递到队列的整个过程是通过通道完成的。因此，让我们创建一个HelloBinding包含我们MessageChannel称为“ greetingChannel” 的接口： 1234interface HelloBinding { @Output(\"greetingChannel\") MessageChannel greeting();} 由于这将发布消息，因此我们使用了@Output注释。方法名称可以是我们想要的任何名称，当然，在一个接口中可以有多个通道。 现在，让我们创建一个REST端点，将消息推送到此通道： 1234567891011121314151617@RestControllerpublic class ProducerController { private MessageChannel greet; public ProducerController(HelloBinding binding) { greet = binding.greeting(); } @GetMapping(\"/greet/{name}\") public void publish(@PathVariable String name) { String greeting = \"Hello, \" + name + \"!\"; Message&lt;String&gt; msg = MessageBuilder.withPayload(greeting) .build(); this.greet.send(msg); }} 上面，我们创建了一个ProducerController具有greet 属性的类MessageChannel。这是通过我们之前声明的方法在构造函数中初始化的。 注意：我们也可以以紧凑的方式进行相同的操作，但是我们使用不同的名称来使您更清楚地了解事物之间的联系。 然后我们有一个简单的REST的映射，一个需要从PathVariable 获取name，并创建一个Message类型的String使用MessageBuilder。最后，我们使用.send()方法通过MessageChannel来发布消息。 现在，我们必须向Spring讲述我们的HelloBinding，我们将在我们的主类中使用@EnableBinding注释： 1234567@EnableBinding(HelloBinding.class)@SpringBootApplicationpublic class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); }} 配置文件最后，我们必须告诉Spring如何连接RabbitMQ（通过前面的“ AMQP URL”），并找到一种方法将“ greetingChannel”连接到可能的使用者。 这两个都在application.properties内定义： 12345spring.rabbitmq.addresses=&lt;amqp url&gt;spring.cloud.stream.bindings.greetingChannel.destination = greetingsserver.port=8080 2.建立消费者现在，我们需要收听先前创建的频道，即“ greetingChannel”。让我们为其创建一个绑定： 12345public interface HelloBinding {String GREETING = \"greetingChannel\"; @Input(GREETING) SubscribableChannel greeting();} 与生产者绑定的两个区别应该很明显。由于我们正在使用消息，因此我们使用SubscribableChannel和@Input注释连接到将推送数据的“ greetingChannel”。 现在，让我们创建在其中将实际处理数据的方法： 1234567@EnableBinding(HelloBinding.class)public class HelloListener { @StreamListener(target = HelloBinding.GREETING) public void processHelloChannelGreeting(String msg) { System.out.println(msg); }} 在这里，我们创建了一个类HelloListener，该类的方法带有@StreamListener以表示的“ greetingChannel”。此方法需要a String作为参数，我们刚刚在控制台中登录了该参数。我们还在班级顶部HelloBinding使用启用了此处@EnableBinding。 再一次，我们使用了在这儿使用了@EnableBinding 而不是主main类中，目的是告诉您，如何组织名称，声明等取决于您或您的团队，这取决于您。 让我们也看看我们的主类，我们没有改变： 1234567@SpringBootApplicationpublic class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); }} 在application.properties我们需要定义我们做了生产者同样的事情，不同的是，这将在不同端口上运行：","link":"/2019/10/24/springcloudstream-rabbitmq/"},{"title":"springcloud stream mq","text":"安装RabbitMQwindow下安装:(1)：下载erlang，原因在于RabbitMQ服务端代码是使用并发式语言erlang编写的，下载地址：http://www.erlang.org/downloads，双击.exe文件进行安装就好，安装完成之后创建一个名为ERLANG_HOME的环境变量，其值指向erlang的安装目录，同时将%ERLANG_HOME%\\bin加入到Path中，最后打开命令行，输入erl，如果出现erlang的版本信息就表示erlang语言环境安装成功； (2)：下载RabbitMQ，下载地址：http://www.rabbitmq.com/，同样双击.exe进行安装就好。然后下载RabbitMQ 管理插件，可以更好的可视化方式查看Rabbit MQ 服务器实例的状态。1.使用管理员打开命令窗口，进入安装目录sbin：输入命令：rabbitmq-plugins.bat enable rabbitmq_management 2.安装成功后，重启服务器 输入命令：net stop RabbitMQ &amp;&amp; net start RabbitMQ 3.用户及权限管理 使用rabbitmqctl控制台命令来创建用户，密码，绑定权限等。 查看已有用户及用户的角色：rabbitmqctl.bat list_users 默认会存在一个来宾账号 guest 新增一个用户：rabbitmqctl.bat add_user username password 新增成功后，可以看见新增的角色为[]，guest的角色是administor。 rabbitmq用户角色可分为五类：超级管理员, 监控者, 策略制定者, 普通管理者以及其他 (1) 超级管理员(administrator)可登陆管理控制台(启用management plugin的情况下)，可查看所有的信息，并且可以对用户，策略(policy)进行操作。(2) 监控者(monitoring)可登陆管理控制台(启用management plugin的情况下)，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等)(3) 策略制定者(policymaker)可登陆管理控制台(启用management plugin的情况下), 同时可以对policy进行管理。(4) 普通管理者(management)仅可登陆管理控制台(启用management plugin的情况下)，无法看到节点信息，也无法对策略进行管理。(5) 其他的无法登陆管理控制台，通常就是普通的生产者和消费者 下面给新增的用户来增加administrator角色 rabbitmqctl.bat set_user_tags username administrator 4.消息队列的管理 使用浏览器打开 http://localhost:15672 访问Rabbit Mq的管理控制台，使用刚才创建的账号登陆系统： RibbitMQ的具体运用结构原理： spring cloud streamSpring Cloud Stream 是一个构建消息驱动微服务的框架. 应用程序通过 inputs 或者 outputs 来与 Spring Cloud Stream 中binder 交互，通过我们配置来 binding ，而 Spring Cloud Stream 的 binder 负责与中间件交互。 Binder 是 Spring Cloud Stream 的一个抽象概念，是应用与消息中间件之间的粘合剂。 通过 binder ，可以很方便的连接中间件，可以动态的改变消息的destinations（对应于 Kafka 的topic，Rabbit MQ 的 exchanges） ，这些都可以通过外部配置项来做到。 新建一个stream项目，主要有3部分，消息产生者类(provider)，消息消费者类(receive)，stream input/output通道定义类(source) 由于是微服务框架，这里我把stream的有关定义都放到了这个项目集中定义，其他用到stream的项目直接引入这个项目的jar包就可以使用其中的类： 消息提供者配置： 1234567891011public interface MessageProviderSource { // exchange名称 public static final String EXCHANGE_OUT = \"exporttv_exchange_out\"; // 绑定exchange @Output(MessageProviderSource.EXCHANGE_OUT) public MessageChannel messageOutput(); } 123456789101112131415@EnableBinding(MessageProviderSource.class)public class MessageProvider { @Autowired private MessageProviderSource messageSource; public void sendApplicationLoadMessage(HashMap&lt;String, Integer&gt; map) { // 创建并发送消息 messageSource.messageOutput().send(message(map)); } private static final &lt;T&gt; Message&lt;T&gt; message(T val) { return MessageBuilder.withPayload(val).build(); }} 消息消费者配置： 12345678public interface MessageReceiveSource { // exchange名称 public static final String EXCHANGE_IN = \"exporttv_exchange_in\"; // 绑定通道 @Input(MessageReceiveSource.EXCHANGE_IN) public SubscribableChannel messageIutput();} 12345678@EnableBinding(MessageReceiveSource.class) public class MessageReceive { @StreamListener(MessageReceiveSource.EXCHANGE_IN) public void ApplicationLoadMessage(Message&lt;HashMap&lt;String,Integer&gt;&gt; message) { }} 然后其他项目引入这个项目后，还要在yml中配置一下绑定： 消息提供者yml 12345678910111213141516171819spring: cloud: stream: bindings: # 服务的整合处理 exporttv_exchange_out: destination: exporttv_exchange # 绑定exchange content-type: application/json # 设置消息类型 binder: exporttv-rabbitmq # 消息中间件 binders: exporttv-rabbitmq: type: rabbit environment: spring: rabbitmq: host: localhost port: 5672 username: guest password: guest virtual-host: / 消息消费者yml： 1234567891011121314151617181920spring: cloud: stream: bindings: # 服务的整合处理 exporttv_exchange_in: destination: exporttv_exchange # 绑定exchange content-type: application/json # 设置消息类型 group: exporttv-group # 进行操作的分组 binder: exporttv-rabbitmq # 消息中间件 binders: exporttv-rabbitmq: type: rabbit environment: spring: rabbitmq: host: localhost port: 5672 username: guest password: guest virtual-host: / 下面说说提供者和消费者怎么引用之前定义的类 消息提供者项目： 123456789101112131415161718192021222324@Servicepublic class SendApplicationMessage { @Autowired private MessageProvider messageProvider; public void SendApplicationLoadMessage() { try { // 业务功能省略 messageProvider.sendApplicationLoadMessage(); } catch (Exception e) { // 打印错误日志 LogUtil.printLog(e, Exception.class); // 抛出错误 throw new MyRuntimeException(ResultEnum.DBException); } }} 消息消费者子项目： 123456789101112131415161718@Componentpublic class ReceiveApplicationMessage extends MessageReceive{ @Autowired private ApplicationService applicationService; @Override public void ApplicationLoadMessage(Message&lt;HashMap&lt;String,Integer&gt;&gt; message) { Integer toalYear = message.getPayload().get(\"year\"); Integer toalMonth = message.getPayload().get(\"month\"); Integer toalWeek = message.getPayload().get(\"week\"); Integer toanId = message.getPayload().get(\"toanId\"); applicationService.updateApplicationLoad(toalYear, toalMonth, toalWeek, toanId); } }","link":"/2019/10/24/stream mq/"},{"title":"Java代码里拼接SQL语句到mybatis的xml","text":"Java代码里拼接SQL语句到mybatis的xml关键语句： StringBuilder whereSql = new StringBuilder(); whereSql.append(“SQL语句”); //需要注意sql注入的问题 实现类： 12345678910111213141516171819202122public List getList(Map&lt;String, Object&gt; map) { List&lt;Map&lt;String, Object&gt;&gt; rs = new ArrayList&lt;Map&lt;String, Object&gt;&gt;(); try { StringBuilder whereSql = new StringBuilder(); if (map.get(\"userName\").toString().length()&gt;0) { whereSql.append(\" AND a.userName in ('\" + map.get(\"userName\").toString().replaceAll(\",\", \"\\',\\'\") + \"')\");//不为空时加入查询条件 } if (map.get(\"CURRENTPAGE\").toString().length()&gt;0 &amp;&amp; map.get(\"PAGESIZE\").toString().length()&gt;0) {//前端有传分页参数时就添加分页查询条件 int currenpage = Integer.parseInt(map.get(\"CURRENTPAGE\").toString()); int pagesize = Integer.parseInt(map.get(\"PAGESIZE\").toString()); currenpage = ((currenpage - 1) * pagesize); whereSql.append(\" limit \" + currenpage + \",\" + pagesize); } rs = wmTblWorkorderMapper.getList(whereSql.toString()); return rs; } catch (Exception e) { e.printStackTrace(); } return null;} mapper： 1List&lt;Map&lt;String,Object&gt;&gt; getList(@Param(\"whereSql\") String whereSql); mapper对应的xml： 123&lt;select id=\"getList\" resultType=\"HashMap\"&gt; SELECT * FROM user a WHERE 1=1 ${whereSql}&lt;/select&gt;","link":"/2019/09/24/sql/"},{"title":"subList用法","text":"java 中subList的区间为左闭右开 比如集合中的内容为1,2,3,4,5list.sublist(2,4)就返回一个子集合：它的内容包括从下标为2到下标为4，而且这是左闭右开的就是说是从大于等于2到小于4那子集内容就是3,4(集合的下标都是从0开始) 12345678public static void main(String[] args) { List list = new ArrayList(); for (int i = 1; i &lt;= 5; i++) { list.add(i); } System.out.println(list); System.out.println(list.subList(2,4)); } 输出：[1, 2, 3, 4, 5] [3, 4]","link":"/2019/11/29/subList用法/"},{"title":"swagger2","text":"配置1234567891011121314151617181920212223242526272829303132333435363738import io.swagger.annotations.ApiOperation;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import springfox.documentation.builders.ApiInfoBuilder;import springfox.documentation.builders.PathSelectors;import springfox.documentation.builders.RequestHandlerSelectors;import springfox.documentation.service.ApiInfo;import springfox.documentation.service.Contact;import springfox.documentation.spi.DocumentationType;import springfox.documentation.spring.web.plugins.Docket;import springfox.documentation.swagger2.annotations.EnableSwagger2;@Configuration@EnableSwagger2public class SwaggerConfig { @Value(\"${spring.application.version}\") private String version; @Bean public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2).apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.withMethodAnnotation(ApiOperation.class)) .paths(PathSelectors.any()) .build(); } private ApiInfo apiInfo() { return new ApiInfoBuilder().title(\"安枢\") .description(\"安枢控制器Restful API\") .termsOfServiceUrl(\"\") .contact(new Contact(\"江苏易安联网络技术有限公司\", \"http://www.enlink.com.cn/\", \"develop@enlink.cn\")) .version(version) .build(); }} 运用1234@ApiOperation(value = \"日志查询\")@ApiImplicitParam(name = \"dto\", value = \"搜索参数\", required = true, dataType = \"SearchSQLDto\")@PostMapping(\"/search\")public AjaxResult log(@RequestBody SearchSQLDto dto){ 访问http://ip:port/swagger-ui.html","link":"/2019/10/26/swagger/"},{"title":"tk.mybatis","text":"1234mapper: mappers: cn.enlink.ensbrain.core.basedao.BaseDao not-empty: false identity: MYSQL UUID：设置生成UUID的方法，需要用OGNL方式配置，不限制返回值，但是必须和字段类型匹配IDENTITY：取回主键的方式，可以配置的内容看下一篇如何使用中的介绍ORDER：中的order属性，可选值为BEFORE和AFTERcatalog：数据库的catalog，如果设置该值，查询的时候表名会带catalog设置的前缀schema：同catalog，catalog优先级高于schemaseqFormat：序列的获取规则,使用{num}格式化参数，默认值为{0}.nextval，针对Oracle，可选参数一共4个，对应0,1,2,3分别为SequenceName，ColumnName, PropertyName，TableNamenotEmpty：insert和update中，是否判断字符串类型!=’’，少数方法会用到style：实体和表转换时的规则，默认驼峰转下划线，可选值为normal用实体名和字段名;camelhump是默认值，驼峰转下划线;uppercase转换为大写;lowercase转换为小写enableMethodAnnotation：可以控制是否支持方法上的JPA注解，默认false。","link":"/2019/10/24/tk.mybatis/"},{"title":"tupdump抓包","text":"tcpdump 抓包写文件抓端口8080的包 12tcpdump tcp port 8080 -n tcpdump tcp port 8080 -w /tmp/xxxx.cap 抓119.29.121.116的80端口的包tcpdump tcp port 8080 and host 10.104.102.228 -n tcpdump tcp port 8080 and host 10.104.102.228 -w /tmp/xxxx.cap *tcpdump 的抓包保存到文件的命令参数是-w xxx.cap * 抓eth0的包 tcpdump -i eth0 -w /tmp/xxx.cap 抓端口8080的包 tcpdump tcp port 8080 -w /tmp/xxxx.cap 抓 119.29.121.116的包 tcpdump -i eth0 host 119.29.121.116 -w /tmp/xxx.cap 抓119.29.121.116的80端口的包 tcpdump -i eth0 host 119.29.121.116 and port 80 -w /tmp/xxx.cap 抓119.29.121.116的icmp的包 tcpdump -i eth0 host 119.29.121.116 and icmp -w /tmp/xxx.cap 抓119.29.121.116的80端口和110和25以外的其他端口的包 tcpdump -i eth0 host 119.29.121.116 and ! port 80 and ! port 25 and ! port 110 -w /tmp/xxx.cap 抓vlan 1的包 tcpdump -i eth0 port 80 and vlan 1 -w /tmp/xxx.cap 抓pppoe的密码 tcpdump -i eth0 pppoes -w /tmp/xxx.cap 以100m大小分割保存文件， 超过100m另开一个文件 -C 100m 抓10000个包后退出 -c 10000 后台抓包， 控制台退出也不会影响： nohup tcpdump -i eth0 port 110 -w /tmp/xxx.cap &amp; 抓下来的文件可以直接用ethereal 或者wireshark打开。","link":"/2019/09/01/tupdump抓包/"},{"title":"unset","text":"Shell删除数组元素（也可以删除整个数组）&lt; Shell数组拼接Shell关联数组 &gt;《Linux就该这么学》是一本基于最新Linux系统编写的入门必读书籍，内容面向零基础读者，由浅入深渐进式教学，销量保持国内第一，年销售量预期超过10万本。点此免费在线阅读。在 Shell 中，使用 unset 关键字来删除数组元素，具体格式如下：unset array_name[index]其中，array_name 表示数组名，index 表示数组下标。 如果不写下标，而是写成下面的形式：unset array_name那么就是删除整个数组，所有元素都会消失。 下面我们通过具体的代码来演示：纯文本复制 123456#!/bin/basharr=(23 56 99 \"http://c.biancheng.net/shell/\")unset arr[1]echo ${arr[@]}unset arrecho ${arr[*]} 运行结果：23 99 http://c.biancheng.net/shell/ 注意最后的空行，它表示什么也没输出，因为数组被删除了，所以输出为空。","link":"/2019/06/27/unset/"},{"title":"utc转化本地时间","text":"UTC就是世界标准时间，与北京时间相差八个时区（相关文章）。所以只要将UTC时间转化成一定格式的时间，再在此基础上加上8个小时就得到北京时间了。 12345678910111213141516171819202122232425262728import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.*;/** * Created by LiChao on 2017/11/23 */public class RegexTest { public static void main(String args[]) throws ParseException { UTCToCST(\"2017-11-27T03:16:03.944Z\", \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"); } public static void UTCToCST(String UTCStr, String format) throws ParseException { Date date = null; SimpleDateFormat sdf = new SimpleDateFormat(format); date = sdf.parse(UTCStr); System.out.println(\"UTC时间: \" + date); Calendar calendar = Calendar.getInstance(); calendar.setTime(date); calendar.set(Calendar.HOUR, calendar.get(Calendar.HOUR) + 8); //calendar.getTime() 返回的是Date类型，也可以使用calendar.getTimeInMillis()获取时间戳 System.out.println(\"北京时间: \" + calendar.getTime()); }} 123456789101112131415161718192021222324/** * toString * &lt;p&gt;将时间字符串转化成 yyyy-MM-dd HH:mm:ss&lt;/&gt; * @param dateStr * @author: tongq * @return */ public static String getPatternTime(String dateStr) { SimpleDateFormat dateFormat = new SimpleDateFormat(DATE_TIME_PATTERN, Locale.SIMPLIFIED_CHINESE); String time = null; try { LOGGER.info(\"UTC: \"+dateStr); Date date = dateFormat.parse(dateStr.replace(\"T\",\" \").replace(\"Z\",\"\")); Calendar calendar = Calendar.getInstance(); calendar.setTime(date); calendar.set(Calendar.HOUR, calendar.get(Calendar.HOUR) + 8); time = dateFormat.format(calendar.getTime()); LOGGER.info(\"local: \" + time); } catch (ParseException e) { LOGGER.error(\"时间转换出错\"); e.printStackTrace(); } return time; }","link":"/2019/10/31/utc转化本地时间/"},{"title":"vim常用的操作","text":"删除当前行dd 删除光标后的三行 3dd删除光标后的所有行 dG光标到首行 gg复制 yyp修改 i下一行插入 o设置文件格式:set ff=unix保存:wq","link":"/2020/05/10/vim常用的操作/"},{"title":"@AutoConfigureBefore","text":"1.了解自动配置的bean 查看(脱掉)Spring的代码(衣服),auto-configuration 就是一个实现了Configuration接口的类。使用@Conditional注解来限制何时让auto-configuration 生效，通常auto-configuration 使用ConditionalOnClass和ConditionalOnMissingBean注解，这两注解的确保只有当我们拥有相关类的时候使得@Configuration注解生效。 2.auto-configuration的目录结构Spring Boot 会检查所有jar包下的META-INF/spring.factories文件，这个文件中EnableAutoConfiguration 的KEY下面罗列了需要自动配置的类，例如： 123org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\com.mycorp.libx.autoconfigure.LibXAutoConfiguration,\\com.mycorp.libx.autoconfigure.LibXWebAutoConfiguration 当我们需要对配置类的加载顺序排序的时候，可以使用@AutoConfigureAfter或者@AutoConfigureBefore注解。例如，如果我们提供了一个特殊的web configuration,需要在WebMvcAutoConfiguration之后才对我们注解的类进行加载。如果想要自动排序，可以使用@AutoconfigureOrder注解，这个注解类似于@Order，但是它是专门给auto-configuration使用。 1Auto-configuration 只有通过这种方式加载，确保 他们定义在一个特定的包空间下能够被扫描。 3.Condition注解我们在auto-configuration中看到不止一个使用了@Condition注解的类，比如 @ConditionalOnMissingBean，那么下面就介绍一下auto-configuration中常用的注解 Class Conditions@ConditionalOnClass和@ConditionalOnMissingClass 注解允许拥有或缺失指定的类进行配置，另外使用了ASM技术 来解析注解，我们可以使用value属性来引用出真实的类，即使这个类不会出现在正在运行的程序类路径中，如果希望指定类名，可以使用name属性 bean ConditionsConditionalOnBean和ConditionalOnMissingBean注解允许用于或确实指定的bean来进行配置，我们可以使用value属性配置一个特别的类型。或者用name配置特殊的名字，search属性允许限制搜索ApplicationContext中的层次结构。 123@Configuration public class MyAutoConfiguration { @Bean @ConditionalOnMissingBean public MyService myService（）{...}} 在这个方法中，myService如果还没有存在Spring容器中，那么它将会在这个方法中得到创建。 这里特别需要注意bean的启动顺序，因为在做一个共享库的时候影响十分深远，所以，我们应当尽量使用@ConditionalOnBean 和@ConditionalOnMissingBena注解在auto-configuration的过程中。保证用户在添加了自已定义的bean后能够正常的加载系统。 1@ConditionalOnBean和@ConditionalOnMissingBean不会阻止被@Configuration注解的类加载，所以这些条件应当去标记具体包含的每个方法 Property Condition@ConditionalOnProperty注解允许基于Spring的环境属性进行配置，使用prefix和name参数来检查指定的属性值，任何存在且不等于属性false都将被匹配，更高级的检查可以使用havingValue和matchIfMissing属性。 Resource Condition@ConditionalOnResource注解去判断指定的资源是否存在，可以使用常规的Spring约束来指定资源，例如file://home/usr/test.dat。 Web Application Condition@ConditionalOnWebApplication和@ConditionalOnNotWebApplication注解允许根据应用是否是一个“web应用程序”被包括配置。 SPEL表达式该@ConditionalOnExpression注释允许基于一个的结果被包括配置使用SpEL表达。 创建自定义的starter一个完整的Spring Boot starter应该包含下面这些组件： autoconfigure 模块包含了自动配置的代码starter模块提供了一个autoconfigure的模块和其他额外的依赖。 命名的问题需要确保为我们的starter提供了一个合适名字，不要让模块名字叫spring-boot等一系列不知所云的名称，即使你用的不同的Maven GroupId，我们应当让模块名称更规范，容易理解。 例如我们正在创建一个acme的start模块，命名推荐叫做acme-spring-boot-autoconfigure和acme-spring-boot-starter， finalstarter其实是一个空的模块，它的唯一目的其实就是提供一个必要的依赖关系，","link":"/2019/09/04/utoConfigureBefore/"},{"title":"文档标题","text":"","link":"/2019/10/24/vue问题/"},{"title":"yaml","text":"yaml 是什么？ 在 github 一些开源项目里经常可以看到 .travis.yml 文件，后来接触持续集成这个概念时发现很多文件都是 .yml 后缀的文件。我在 阮一峰-YAML 语言教程 里了解了它的作用以及基本写法，他的那篇文章描述的略微有点啰嗦，实际看到的大多数配置文件还是及其简单的，而我们要做的是能读懂它们，就像读懂 .json 一样。 yaml 语言（或者说是一种规范吧）可以编写 .yml 文件，和 json 一样是配置文件。也许是有人认为 json 的写法不爽，于是乎发明了这玩意，通过下面的例子，可以看到 yaml 写的配置文件确实要比 json 方便很多。 编写规则 大小写敏感json 里也是大小写敏感的，这点二者一样。 使用缩进表示层级关系json 中使用 {} 的嵌套表示层级，而 yaml 使用缩进，后者更方便一些。 表示注释json 文件中不允许写注释，对于很长配置文件全靠字面意思猜挺痛苦的，yaml 可以写注释 数据结构 配置文件理应十分简洁，与 json 相比，不用频繁的写 {} 和 []，毕竟换行和 - 符号更加简洁，字符串也不需要频繁的加引号（无论是单引号还是双引号）。 对象 conf.ymlanimal: petshash: { name: Steve, foo: bar }转换为 json 为： 1234{ { \"animal\": \"pets\" }, { \"hash\": { \"name\": \"Steve\", \"foo\": \"bar\" } } } 数组 conf.yml1234Animal: - Cat - Dog - Goldfish 转换为 json 为： { “Animal”: [ “Cat”, “Dog”, “Goldfish” ] }字符串 conf.yml正常情况下字符串不用写引号str: 这是一行字符串 字符串内有空格或者特殊字符时需要加引号str: ‘内容： 字符串’null conf.ymlparent: ~ .yml 中 ~ 表示 null，转换为 json 为： { “parent”: null }","link":"/2019/07/13/yaml/"},{"title":"WebSocket","text":"WebSocket一个web版群聊程序https://blog.csdn.net/zzti_erlie/article/details/101060892 qq聊天室https://blog.csdn.net/qq_40315080/article/details/83052689","link":"/2019/10/24/一个web版群聊程序/"},{"title":"中间件rabbitMQ与activeMQ","text":"AMQPjms springboot 默认集成rabbitMQ高并发性能强 因为用了erlang语言编写","link":"/2019/09/26/中间件rabbitMQ/"},{"title":"Transaction事务传播行为种类PROPAGATION_REQUIRED","text":"事务传播行为种类 Spring在TransactionDefinition接口中规定了7种类型的事务传播行为，它们规定了事务方法和事务方法发生嵌套调用时事务如何进行传播： 事务传播行为类型 说明 PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。 PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 例子： 12345678910@Transactional(propagation = Propagation.REQUIRED) public void delete(Object id) throws ServiceException { try{ getDao().delete(id); }catch (DaoException e){ throw new ServiceException(\"ERROR:\",e); }catch (Exception e){ throw new ServiceException(\"ERROR:\",e); } } 当使用PROPAGATION_NESTED时，底层的数据源必须基于JDBC 3.0，并且实现者需要支持保存点事务机制。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136&lt;!--Hibernate事务管理器--&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.orm.hibernate3.HibernateTransactionManager\"&gt; &lt;property name=\"sessionFactory\"&gt; &lt;ref bean=\"sessionFactory\" /&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 定义事务拦截器bean--&gt;&lt;bean id=\"transactionInterceptor\" class=\"org.springframework.transaction.interceptor.TransactionInterceptor\"&gt; &lt;!-- 事务拦截器bean需要依赖注入一个事务管理器--&gt; &lt;property name=\"transactionManager\" ref=\"transactionManager\" /&gt; &lt;property name=\"transactionAttributes\"&gt; &lt;!-- 下面定义事务传播属性--&gt; &lt;props&gt; &lt;prop key=\"save*\"&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;prop key=\"find*\"&gt;PROPAGATION_REQUIRED,readOnly&lt;/prop&gt; &lt;prop key=\"delete*\"&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;prop key=\"update*\"&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;prop key=\"*\"&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"managerTemplate\" abstract=\"true\" lazy-init=\"true\"&gt; &lt;property name=\"teamDao\"&gt; &lt;ref bean=\"teamDao\" /&gt; &lt;/property&gt; &lt;property name=\"studentDao\"&gt; &lt;ref bean=\"studentDao\" /&gt; &lt;/property&gt; &lt;/bean&gt;&lt;bean id =\"manager\" class=\"com.zd.service.impl.Manager\" parent=\"managerTemplate\" /&gt;&lt;!-- 定义BeanNameAutoProxyCreator--&gt; &lt;bean class=\"org.springframework.aop.framework.autoproxy.BeanNameAutoProxyCreator\"&gt; &lt;!-- 指定对满足哪些bean name的bean自动生成业务代理 --&gt; &lt;property name=\"beanNames\"&gt; &lt;!-- 下面是所有需要自动创建事务代理的bean--&gt; &lt;list&gt; &lt;value&gt;manager&lt;/value&gt; &lt;/list&gt; &lt;!-- 此处可增加其他需要自动创建事务代理的bean--&gt; &lt;/property&gt; &lt;!-- 下面定义BeanNameAutoProxyCreator所需的事务拦截器--&gt; &lt;property name=\"interceptorNames\"&gt; &lt;list&gt; &lt;!-- 此处可增加其他新的Interceptor --&gt; &lt;value&gt;transactionInterceptor&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 基本数据库操作 --&gt; &lt;bean id=\"baseDao\" class=\"com.zd.service.impl.BaseDao\"&gt; &lt;property name=\"hibernateTemplate\"&gt; &lt;ref bean=\"hibernateTemplate\"/&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 班级 --&gt; &lt;bean id=\"teamDao\" class=\"com.zd.service.impl.TeamDao\"&gt; &lt;property name=\"baseDao\"&gt; &lt;ref bean=\"baseDao\" /&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 学生 --&gt; &lt;bean id=\"studentDao\" class=\"com.zd.service.impl.StudentDao\"&gt; &lt;property name=\"baseDao\"&gt; &lt;ref bean=\"baseDao\" /&gt; &lt;/property&gt; &lt;/bean&gt; 12345678910111213141516171819202122232425public void testSaveTeam() { Team team = new Team(); team.setTeamId(DBKeyCreator.getRandomKey(12)); team.setTeamName(\"Class CCC\"); IManager manager = (IManager) SpringContextUtil.getContext().getBean(\"manager\"); Student student = new Student(); student.setStudentId(DBKeyCreator.getRandomKey(13)); student.setSex(Student.SEX_FEMALE); student.setStudentName(\"Tom\"); student.setTeamId(\"60FHDXDIG5JQ\"); manager.saveTeamAndStu(team, student); System.out.println(\"Save Team and Student Success\");","link":"/2019/09/25/事务传播行为种类/"},{"title":"分布式eureka","text":"Netflix Eureka 简介 1、Eureka 是 Netflix 公司开发的服务发现框架，Spring Cloud 对它提供了支持，将它集成在了自己的 spring-cloud-netflix 子项目中。 2、Netflix 公司在 Github 上开源了很多项目，Eureka 只是其中一个，Netflix 开源主页：https://github.com/Netflix 3、Netflix Eureka GitHub 开源地址：https://github.com/Netflix/eureka。 AWS Service registry for resilient mid-tier load balancing and failover.（Eureka 是用于弹性中间层负载平衡和故障转移的AWS服务注册中心） 1.简介EureKa在Spring Cloud全家桶中担任着服务的注册与发现的落地实现。Netflix在设计EureKa时遵循着AP原则，它基于REST的服务，用于定位服务，以实现云端中间层服务发现和故障转移，功能类似于Dubbo的注册中心Zookeeper。 2.实现原理 EureKa采用CS的设计架构，即包括了Eureka Server（服务端），EureKa client（客户端）。1.EureKa Server 提供服务注册，各个节点启动后，在EureKa server中进行注册； 2 EureKa Client 是一个Java客户端，用于和服务端进行交互，同时客户端也是一个内置的默认使用轮询负载均衡算法的负载均衡器。在应用启动后，会向Eueka Server发送心跳（默认30秒）。如果Eureka Server在多个心跳周期内没有接受到某个节点的心跳，EureKa Server将会从服务注册表中将这个服务移出（默认90秒）。 3.SpringCloud Eureka的使用步骤 3.1Eureka Service端(服务端) 3.1.1.POM.XML导入相依的依赖 1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; 3.1.2.application.yml配置文件的配置demo 12345678910111213141516eureka: instance: #单机hostname: localhost hostname: eureka7002.com #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: #单机设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址（单机）。 #defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/ #Eureka高复用时设置其他的Eureka之间通信 #defaultZone: http://eureka7003.com:7003/eureka/,http://eureka7004.com:7004/eureka/ defaultZone: http://eureka7003.com:7003/eureka/ #server: #enable-self-preservation: false #Eureka服务端关闭心跳连接测试 3.1.3.主程序类添加注解@EnableEurekaServerdemo : 1234567891011121314package com.mark.eureka;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@EnableEurekaServer@SpringBootApplicationpublic class EurekaServerApplication { public static void main(String[] args) { SpringApplication.run(EurekaServerApplication.class, args); }} 启动后访问设置的端口eg http://127.0.0.1:7002/ 3.2Eureka Clinet(客户端) 3.2.1 pom.XMl 12345 &lt;!-- 将微服务provider侧注册进eureka --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt; 3.2.2 application.yml server: port: 8001 eureka: client: #客户端注册进eureka服务列表内 service-url: #单机defaultZone: http://localhost:7002/eureka #集群是 #defaultZone: http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/,http://eureka7004.com:7004/eureka/ defaultZone: http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/ instance: instance-id: microservicecloud-dept8001 prefer-ip-address: true #访问路径可以显示IP地址 info: #在Eure页面访问info返回的信息的配置 app.name: atguigu-microservicecloud company.name: www.mark.com build.artifactId: $project.artifactId$ build.version: $project.version$ 3.2.3主程序类 添加注解：（注意和服务端区分开）@EnableEurekaClient@EnableDiscoveryClient 1234@SpringBootApplication@EnableEurekaClient@EnableDiscoveryClientpublic class Deptprovider8003_App { 3.3补充如果是Eureka Client的消费者，如果获取注册中心中的微服务，那么还需要在配置类中注入一个组件RestTemplatedemo 1234567891011121314151617181920package com.mark.springcloud.config;import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.client.RestTemplate;/** * Created by Choisaaaa on 2018/7/7. * 自定义配置类 */@Configurationpublic class MyConfig { @Bean @LoadBalanced//使用负载均衡 public RestTemplate restTemplate(){ return new RestTemplate(); }} 调用的demo 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.mark.springcloud.controller;import com.mark.springcloud.entities.Dept;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.client.RestTemplate;import java.util.List;/** * Created by Choisaaaa on 2018/7/7. * */@RestController@RequestMapping(\"/consumer/dept\")public class DeptControler_Consumer { //private static final String REST_URL_PREFIX = \"http://localhost:8001\"; //MICROSERVICECLOUD-DEP：为Eureka Server中心的微服务实例名称 private static final String REST_URL_PREFIX = \"http://MICROSERVICECLOUD-DEPT\"; @Autowired private RestTemplate restTemplate; @RequestMapping(value = \"/add\") public boolean add(Dept dept) { return restTemplate.getForObject(REST_URL_PREFIX+\"/dept/add\",Boolean.class,dept); } @RequestMapping(value = \"get/{id}\") public Dept get(@PathVariable(\"id\") Long id) { return restTemplate.getForObject(REST_URL_PREFIX+\"/dept/get/\"+id,Dept.class); } @RequestMapping(value = \"/list\") public List&lt;Dept&gt; list(){ return restTemplate.getForObject(REST_URL_PREFIX+\"/dept/list\",List.class); } //消费者调用服务发现 @RequestMapping(\"/discovery\") public Object discovery() { return restTemplate.getForObject(REST_URL_PREFIX+\"/dept/discovery\",Object.class); }} 参考：https://www.cnblogs.com/knowledgesea/p/11208000.html熔断https://blog.csdn.net/qq_36763236/article/details/82024039https://www.cnblogs.com/guagua-join-1/p/9638767.html","link":"/2019/10/12/分布式eureka/"},{"title":"Spring中用jointpoint访问目标方法的参数","text":"概念:访问目标方法即用jointpoint(@around用poceedingjointpoint)1.获取他们的目标对象信息,如test.component@80387a这种,2.还有获取带参方法的参数,如java.lang.Object;@183cfe9(想当然我们也可以用对象的arrays.toString()方法将其还原) 3.另外还有获取被增强的方法相关信息 如String test.component.test1(String)这种 总结:访问目标方法参数,有三种方法(实际有四种,先说三种) 1.joinpoint.getargs():获取带参方法的参数 2.joinpoint.getTarget():.获取他们的目标对象信息 3..joinpoint.getSignature():(signature是信号,标识的意思):获取被增强的方法相关信息 看不懂,请看代码! 我以@afterreturning为例解说 我们先看3 Signature方法 1234567891011@AfterReturning(value=\"execution(* test.*.*(..))\",returning=\"name2\") private void test1(JoinPoint jp ,String name2) { System.out.println(jp.getSignature()); //其实getSignature()方法还有有用的方法,如:getDeclaringTypeName和getname(); 前者是一个返回方法所在的包名和类名后者是返回方法名 System.out.println(jp.getSignature().getName()); System.out.println(jp.getSignature().getDeclaringTypeName());} 组件类 12345678910@Componentpublic class component { public void test() { } public void test1(String name2) { }} Signature方法下的输出结果 你看,getSignature());是获取到这样的信息 修饰符 + 包名 + 组件名(类名) + 方法的名字 getSignature().getName()); 方法名 getSignature().getDeclaringTypeName()); 包名 + 组件名(类名)我们再看1 getargs() System.out.println(jp.getArgs()); System.out.println(Arrays.toString(jp.getArgs()));结果输出 jp.getArgs() 我们会得到一个 看不懂一组数组对象,但是我们知道这是一个对象,参数对象 Arrays.toString(jp.getArgs()) 但是我们可以用Arrays类中数组转字符串方法:arrays.tostring(返回指定数组内容以字符串表示出来) 就可以得到具体数字了 3.getTarget()方法:该方法返回被织入增强处理的目标对象.(这个和getthis()方法很相似 但是的出来的值不相等) System.out.println(jp.getTarget()); 输出结果 是一个实实在在的对象,既不是参数也不是相关信息 总结: 以上就是访问目标方法的参数的所有方法(除了getthis()没有说其实也是返回一个对象生成的代理对象 得出的对象与gettarget()方法很相似几乎可以说是一模一样的,但是我用过.eqauls和==都不相等)这些方法,让我想起了 document(dom)编程的获取元素docuemnt.getelementbyid(); 获取这些元素可以进行修改或者观察自己代码是否有异样. 以上就是访问目标方法的参数的所有方法(除了getthis()没有说其实也是返回一个对象生成的代理对象 得出的对象与gettarget()方法很相似几乎可以说是一模一样的,但是我用过.eqauls和==都不相等)这些方法,让我想起了 document(dom)编程的获取元素docuemnt.getelementbyid(); 获取这些元素可以进行修改或者观察自己代码是否有异样。 1234567891011121314151617public static String getControllerMethodOperateType(JoinPoint joinPoint) throws Exception { String targetName = joinPoint.getTarget().getClass().getName(); String methodName = joinPoint.getSignature().getName(); Object[] arguments = joinPoint.getArgs(); Class targetClass = Class.forName(targetName); Method[] methods = targetClass.getMethods(); String operate = \"\"; for (Method method : methods) { if (method.getName().equals(methodName)) { Class[] clazzs = method.getParameterTypes(); if (clazzs.length == arguments.length) { operate = method.getAnnotation(OperaControllerLog.class).operateType(); break; } } } return operate;","link":"/2019/09/29/切入点获取方法参数/"},{"title":"博客/网站内嵌网易云音乐插件教程","text":"步骤 1.进入官网，登录 http://music.163.com/# 2.选择一张歌单 如图，点击生成外链播放器。 3.选择合适插件，设置尺寸，是否自动播放 4.复制代码到想添加的地方再来一张仙剑系列配乐歌单其实想试试，CSDN支不支持flash插件，结果不支持。只支持iframe插件哦！非自动播放，点击播放！ 1&lt;iframe border=\"0\" width=\"330\" height=\"450\" src=\"//music.163.com/outchain/player?type=0&amp;id=823888692&amp;auto=0&amp;height=430\"&gt;&lt;/iframe&gt; 后记 插件里的歌顺序等，在网易云那边修改会同步哦！","link":"/2019/09/18/博客-网站内嵌网易云音乐插件教程/"},{"title":"多数据源","text":"Spring Boot + MyBatis + Druid + PageHelper 实现多数据源并分页https://mp.weixin.qq.com/s/6gcyfWnb3zvs5QuEjYxh-w前言 本篇文章主要讲述的是 Spring Boo t整合Mybatis、Druid和PageHelper 并实现多数据源和分页。其中Spring Boot整合Mybatis这块，在之前的的一篇文章中已经讲述了，这里就不过多说明了。重点是讲述在多数据源下的如何配置使用Druid和PageHelperDruid介绍和使用在使用Druid之前，先来简单的了解下Druid。Druid是一个数据库连接池。Druid可以说是目前最好的数据库连接池！因其优秀的功能、性能和扩展性方面，深受开发人员的青睐。Druid已经在阿里巴巴部署了超过600个应用，经过一年多生产环境大规模部署的严苛考验。Druid是阿里巴巴开发的号称为监控而生的数据库连接池！同时Druid不仅仅是一个数据库连接池，Druid 核心主要包括三部分：基于Filter－Chain模式的插件体系。DruidDataSource 高效可管理的数据库连接池。SQLParserDruid的主要功能如下:是一个高效、功能强大、可扩展性好的数据库连接池。可以监控数据库访问性能。数据库密码加密获得SQL执行日志扩展JDBC介绍方面这块就不再多说，具体的可以看官方文档。那么开始介绍Druid如何使用。首先是Maven依赖，只需要添加druid这一个jar就行了。 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.8&lt;/version&gt;&lt;/dependency&gt; Tips：可以关注微信公众号：Java后端，获取Maven教程和每日技术博文推送。配置方面，主要的只需要在application.properties或application.yml添加如下就可以了。 说明：因为这里我是用了两个数据源，所以稍微有些不同而已。Druid 配置的说明在下面中已经说的很详细了，这里我就不在说明了。 1234567891011121314151617181920212223242526272829303132333435## 默认的数据源master.datasource.url=jdbc:mysql://localhost:3306/springBoot?useUnicode=true&amp;characterEncoding=utf8&amp;allowMultiQueries=truemaster.datasource.username=rootmaster.datasource.password=123456master.datasource.driverClassName=com.mysql.jdbc.Driver## 另一个的数据源cluster.datasource.url=jdbc:mysql://localhost:3306/springBoot_test?useUnicode=true&amp;characterEncoding=utf8cluster.datasource.username=rootcluster.datasource.password=123456cluster.datasource.driverClassName=com.mysql.jdbc.Driver# 连接池的配置信息# 初始化大小，最小，最大spring.datasource.type=com.alibaba.druid.pool.DruidDataSourcespring.datasource.initialSize=5spring.datasource.minIdle=5spring.datasource.maxActive=20# 配置获取连接等待超时的时间spring.datasource.maxWait=60000# 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒spring.datasource.timeBetweenEvictionRunsMillis=60000# 配置一个连接在池中最小生存的时间，单位是毫秒spring.datasource.minEvictableIdleTimeMillis=300000spring.datasource.validationQuery=SELECT 1 FROM DUALspring.datasource.testWhileIdle=truespring.datasource.testOnBorrow=falsespring.datasource.testOnReturn=false# 打开PSCache，并且指定每个连接上PSCache的大小spring.datasource.poolPreparedStatements=truespring.datasource.maxPoolPreparedStatementPerConnectionSize=20# 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙spring.datasource.filters=stat,wall,log4j# 通过connectProperties属性来打开mergeSql功能；慢SQL记录spring.datasource.connectionProperties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000 成功添加了配置文件之后，我们再来编写Druid相关的类。首先是MasterDataSourceConfig.java这个类，这个是默认的数据源配置类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110@Configuration@MapperScan(basePackages = MasterDataSourceConfig.PACKAGE, sqlSessionFactoryRef = \"masterSqlSessionFactory\")public class MasterDataSourceConfig { static final String PACKAGE = \"com.pancm.dao.master\"; static final String MAPPER_LOCATION = \"classpath:mapper/master/*.xml\"; @Value(\"${master.datasource.url}\") private String url; @Value(\"${master.datasource.username}\") private String username; @Value(\"${master.datasource.password}\") private String password; @Value(\"${master.datasource.driverClassName}\") private String driverClassName; @Value(\"${spring.datasource.initialSize}\") private int initialSize; @Value(\"${spring.datasource.minIdle}\") private int minIdle; @Value(\"${spring.datasource.maxActive}\") private int maxActive; @Value(\"${spring.datasource.maxWait}\") private int maxWait; @Value(\"${spring.datasource.timeBetweenEvictionRunsMillis}\") private int timeBetweenEvictionRunsMillis; @Value(\"${spring.datasource.minEvictableIdleTimeMillis}\") private int minEvictableIdleTimeMillis; @Value(\"${spring.datasource.validationQuery}\") private String validationQuery; @Value(\"${spring.datasource.testWhileIdle}\") private boolean testWhileIdle; @Value(\"${spring.datasource.testOnBorrow}\") private boolean testOnBorrow; @Value(\"${spring.datasource.testOnReturn}\") private boolean testOnReturn; @Value(\"${spring.datasource.poolPreparedStatements}\") private boolean poolPreparedStatements; @Value(\"${spring.datasource.maxPoolPreparedStatementPerConnectionSize}\") private int maxPoolPreparedStatementPerConnectionSize; @Value(\"${spring.datasource.filters}\") private String filters; @Value(\"{spring.datasource.connectionProperties}\") private String connectionProperties; @Bean(name = \"masterDataSource\") @Primary public DataSource masterDataSource() { DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(url); dataSource.setUsername(username); dataSource.setPassword(password); dataSource.setDriverClassName(driverClassName); //具体配置 dataSource.setInitialSize(initialSize); dataSource.setMinIdle(minIdle); dataSource.setMaxActive(maxActive); dataSource.setMaxWait(maxWait); dataSource.setTimeBetweenEvictionRunsMillis(timeBetweenEvictionRunsMillis); dataSource.setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis); dataSource.setValidationQuery(validationQuery); dataSource.setTestWhileIdle(testWhileIdle); dataSource.setTestOnBorrow(testOnBorrow); dataSource.setTestOnReturn(testOnReturn); dataSource.setPoolPreparedStatements(poolPreparedStatements); dataSource.setMaxPoolPreparedStatementPerConnectionSize(maxPoolPreparedStatementPerConnectionSize); try { dataSource.setFilters(filters); } catch (SQLException e) { e.printStackTrace(); } dataSource.setConnectionProperties(connectionProperties); return dataSource; } @Bean(name = \"masterTransactionManager\") @Primary public DataSourceTransactionManager masterTransactionManager() { return new DataSourceTransactionManager(masterDataSource()); } @Bean(name = \"masterSqlSessionFactory\") @Primary public SqlSessionFactory masterSqlSessionFactory(@Qualifier(\"masterDataSource\") DataSource masterDataSource) throws Exception { final SqlSessionFactoryBean sessionFactory = new SqlSessionFactoryBean(); sessionFactory.setDataSource(masterDataSource); sessionFactory.setMapperLocations(new PathMatchingResourcePatternResolver() .getResources(MasterDataSourceConfig.MAPPER_LOCATION)); return sessionFactory.getObject(); }} 其中这两个注解说明下:@Primary ：标志这个 Bean 如果在多个同类 Bean 候选时，该 Bean优先被考虑。多数据源配置的时候注意，必须要有一个主数据源，用 @Primary 标志该 Bean。@MapperScan： 扫描 Mapper 接口并容器管理。 需要注意的是sqlSessionFactoryRef 表示定义一个唯一 SqlSessionFactory 实例。 上面的配置完之后，就可以将Druid作为连接池使用了。但是Druid并不简简单单的是个连接池，它也可以说是一个监控应用，它自带了web监控界面，可以很清晰的看到SQL相关信息。在SpringBoot中运用Druid的监控作用，只需要编写StatViewServlet和WebStatFilter类，实现注册服务和过滤规则。这里我们可以将这两个写在一起，使用@Configuration和@Bean。为了方便理解，相关的配置说明也写在代码中了，这里就不再过多赘述了。代码如下: 12345678910111213141516171819202122232425262728293031323334@Configurationpublic class DruidConfiguration { @Bean public ServletRegistrationBean druidStatViewServle() { //注册服务 ServletRegistrationBean servletRegistrationBean = new ServletRegistrationBean( new StatViewServlet(), \"/druid/*\"); // 白名单(为空表示,所有的都可以访问,多个IP的时候用逗号隔开) servletRegistrationBean.addInitParameter(\"allow\", \"127.0.0.1\"); // IP黑名单 (存在共同时，deny优先于allow) servletRegistrationBean.addInitParameter(\"deny\", \"127.0.0.2\"); // 设置登录的用户名和密码 servletRegistrationBean.addInitParameter(\"loginUsername\", \"pancm\"); servletRegistrationBean.addInitParameter(\"loginPassword\", \"123456\"); // 是否能够重置数据. servletRegistrationBean.addInitParameter(\"resetEnable\", \"false\"); return servletRegistrationBean; } @Bean public FilterRegistrationBean druidStatFilter() { FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean( new WebStatFilter()); // 添加过滤规则 filterRegistrationBean.addUrlPatterns(\"/*\"); // 添加不需要忽略的格式信息 filterRegistrationBean.addInitParameter(\"exclusions\", \"*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*\"); System.out.println(\"druid初始化成功!\"); return filterRegistrationBean; }} 编写完之后，启动程序，在浏览器输入:http://127.0.0.1:8084/druid/index.html ，然后输入设置的用户名和密码，便可以访问Web界面了。 多数据源配置在进行多数据源配置之前，先分别在springBoot和springBoot_test的mysql数据库中执行如下脚本。 12345678910111213141516-- springBoot库的脚本CREATE TABLE `t_user` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '自增id', `name` varchar(10) DEFAULT NULL COMMENT '姓名', `age` int(2) DEFAULT NULL COMMENT '年龄', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=utf8-- springBoot_test库的脚本CREATE TABLE `t_student` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(16) DEFAULT NULL, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 注:为了偷懒，将两张表的结构弄成一样了！不过不影响测试! 在application.properties中已经配置这两个数据源的信息，上面已经贴出了一次配置，这里就不再贴了。这里重点说下 第二个数据源的配置。和上面的MasterDataSourceConfig.java差不多，区别在与没有使用@Primary 注解和名称不同而已。需要注意的是MasterDataSourceConfig.java对package和mapper的扫描是精确到目录的，这里的第二个数据源也是如此。那么代码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Configuration@MapperScan(basePackages = ClusterDataSourceConfig.PACKAGE, sqlSessionFactoryRef = \"clusterSqlSessionFactory\")public class ClusterDataSourceConfig { static final String PACKAGE = \"com.pancm.dao.cluster\"; static final String MAPPER_LOCATION = \"classpath:mapper/cluster/*.xml\"; @Value(\"${cluster.datasource.url}\") private String url; @Value(\"${cluster.datasource.username}\") private String username; @Value(\"${cluster.datasource.password}\") private String password; @Value(\"${cluster.datasource.driverClassName}\") private String driverClass; // 和MasterDataSourceConfig一样，这里略 @Bean(name = \"clusterDataSource\") public DataSource clusterDataSource() { DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(url); dataSource.setUsername(username); dataSource.setPassword(password); dataSource.setDriverClassName(driverClass); // 和MasterDataSourceConfig一样，这里略 ... return dataSource; } @Bean(name = \"clusterTransactionManager\") public DataSourceTransactionManager clusterTransactionManager() { return new DataSourceTransactionManager(clusterDataSource()); } @Bean(name = \"clusterSqlSessionFactory\") public SqlSessionFactory clusterSqlSessionFactory(@Qualifier(\"clusterDataSource\") DataSource clusterDataSource) throws Exception { final SqlSessionFactoryBean sessionFactory = new SqlSessionFactoryBean(); sessionFactory.setDataSource(clusterDataSource); sessionFactory.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(ClusterDataSourceConfig.MAPPER_LOCATION)); return sessionFactory.getObject(); }} 成功写完配置之后，启动程序，进行测试。分别在springBoot和springBoot_test库中使用接口进行添加数据。 PageHelper 分页实现PageHelper是Mybatis的一个分页插件，非常的好用！这里强烈推荐！！！PageHelper的使用很简单，只需要在Maven中添加pagehelper这个依赖就可以了。Maven的依赖如下: 12345&lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt;&lt;/dependency&gt; 注：这里我是用springBoot版的！也可以使用其它版本的。 添加依赖之后，只需要添加如下配置或代码就可以了。 第一种，在application.properties或application.yml添加 12345pagehelper: helperDialect: mysql offsetAsPageNum: true rowBoundsWithCount: true reasonable: false 第二种，在mybatis.xml配置中添加 1234567891011121314151617181920&lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;!-- 扫描mapping.xml文件 --&gt; &lt;property name=\"mapperLocations\" value=\"classpath:mapper/*.xml\"&gt;&lt;/property&gt; &lt;!-- 配置分页插件 --&gt; &lt;property name=\"plugins\"&gt; &lt;array&gt; &lt;bean class=\"com.github.pagehelper.PageHelper\"&gt; &lt;property name=\"properties\"&gt; &lt;value&gt; helperDialect=mysql offsetAsPageNum=true rowBoundsWithCount=true reasonable=false &lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/array&gt; &lt;/property&gt; &lt;/bean&gt; 第三种，在代码中添加，使用@Bean注解在启动程序的时候初始化。 1234567891011121314@Bean public PageHelper pageHelper(){ PageHelper pageHelper = new PageHelper(); Properties properties = new Properties(); //数据库 properties.setProperty(\"helperDialect\", \"mysql\"); //是否将参数offset作为PageNum使用 properties.setProperty(\"offsetAsPageNum\", \"true\"); //是否进行count查询 properties.setProperty(\"rowBoundsWithCount\", \"true\"); //是否分页合理化 properties.setProperty(\"reasonable\", \"false\"); pageHelper.setProperties(properties); } 因为这里我们使用的是多数据源，所以这里的配置稍微有些不同。我们需要在sessionFactory这里配置。这里就对MasterDataSourceConfig.java进行相应的修改。在masterSqlSessionFactory方法中，添加如下代码。 123456789101112131415161718192021222324@Bean(name = \"masterSqlSessionFactory\") @Primary public SqlSessionFactory masterSqlSessionFactory(@Qualifier(\"masterDataSource\") DataSource masterDataSource) throws Exception { final SqlSessionFactoryBean sessionFactory = new SqlSessionFactoryBean(); sessionFactory.setDataSource(masterDataSource); sessionFactory.setMapperLocations(new PathMatchingResourcePatternResolver() .getResources(MasterDataSourceConfig.MAPPER_LOCATION)); //分页插件 Interceptor interceptor = new PageInterceptor(); Properties properties = new Properties(); //数据库 properties.setProperty(\"helperDialect\", \"mysql\"); //是否将参数offset作为PageNum使用 properties.setProperty(\"offsetAsPageNum\", \"true\"); //是否进行count查询 properties.setProperty(\"rowBoundsWithCount\", \"true\"); //是否分页合理化 properties.setProperty(\"reasonable\", \"false\"); interceptor.setProperties(properties); sessionFactory.setPlugins(new Interceptor[] {interceptor}); return sessionFactory.getObject(); } 注：其它的数据源也想进行分页的时候，参照上面的代码即可。 这里需要注意的是reasonable参数，表示分页合理化，默认值为false。如果该参数设置为 true 时，pageNum&lt;=0 时会查询第一页，pageNum&gt;pages（超过总数时），会查询最后一页。默认false 时，直接根据参数进行查询。设置完PageHelper 之后，使用的话，只需要在查询的sql前面添加PageHelper.startPage(pageNum,pageSize);，如果是想知道总数的话，在查询的sql语句后买呢添加 page.getTotal()就可以了。代码示例: 1234567891011public List&lt;T&gt; findByListEntity(T entity) { List&lt;T&gt; list = null; try { Page&lt;?&gt; page =PageHelper.startPage(1,2); System.out.println(getClassName(entity)+\"设置第一页两条数据!\"); list = getMapper().findByListEntity(entity); System.out.println(\"总共有:\"+page.getTotal()+\"条数据,实际返回:\"+list.size()+\"两条数据!\"); } catch (Exception e) { logger.error(\"查询\"+getClassName(entity)+\"失败!原因是:\",e); } return list; 代码编写完毕之后，开始进行最后的测试。查询t_user表的所有的数据，并进行分页。请求:GET http://localhost:8084/api/user","link":"/2019/10/24/多数据源/"},{"title":"多种方式实现Spring的Bean注入","text":"多种方式实现Spring的Bean注入2019.03.24 15:54 2093浏览Spring的核心是控制反转（IoC）和面向切面（AOP）。 Spring就是一个大工厂（容器），可以将所有对象创建和依赖关系维护，交给Spring管理 。 Spring工厂是用于生成Bean，对Bean进行管理。 在Spring中，所有Bean的生命周期都交给Ioc容器管理。 Spring中，Spring可以通过Xml形式或注解的形式来管理Bean 。 下面基于注解的形式，采用多种方式实现Spring的Bean注入。具体如下： 一、通过方法注入Bean 通过构造方法注入Bean 实例代码： 123456789101112131415161718192021@Component(\"anotherBean1\")public class AnotherBean {}@Componentpublic class MyBean1 { private AnotherBean anotherBean1; public MyBean1(AnotherBean anotherBean1) { this.anotherBean1 = anotherBean1; } @Override public String toString() { return \"MyBean1{\" + \"anotherBean1=\" + anotherBean1 + '}'; }} 2.通过Set方法注入Bean 实例代码： 1234567891011121314151617@Componentpublic class MyBean2 { private AnotherBean anotherBean2; @Autowired public void setAnotherBean2(AnotherBean anotherBean2) { this.anotherBean2 = anotherBean2; } @Override public String toString() { return \"MyBean2{\" + \"anotherBean2=\" + anotherBean2 + '}'; }} 二、通过属性注入Bean 12345678910111213@Componentpublic class MyBean3 { @Autowired private AnotherBean anotherBean3; @Override public String toString() { return \"MyBean3{\" + \"anotherBean3=\" + anotherBean3 + '}'; }} 三、通过集合类型注入Bean 直接注入集合实例 List集合注入Bean 12345678910111213141516171819202122232425262728293031323334@Componentpublic class MyBean4 { private List&lt;String&gt; stringList; public List&lt;String&gt; getStringList() { return stringList; } @Autowired public void setStringList(List&lt;String&gt; stringList) { this.stringList = stringList; } private List&lt;String&gt; stringList1; public List&lt;String&gt; getStringList1() { return stringList1; } @Autowired @Qualifier(\"stringList1\") //使用@Qualifier注解指定bean的Id，此处的Id与BeanConfiguration类中的stringList1方法的Bean 的Id要一致 public void setStringList1(List&lt;String&gt; stringList1) { this.stringList1 = stringList1; } @Override public String toString() { return \"MyBean4{\" + \"stringList=\" + stringList + \", stringList1=\" + stringList1 + '}'; }} Map集合注入Bean 123456789101112131415161718192021@Componentpublic class MyBean5 { private Map&lt;String, Integer&gt; integerMap; public Map&lt;String, Integer&gt; getIntegerMap() { return integerMap; } @Autowired //加上@Autowired 注解是希望Spring帮我们完成注入 public void setIntegerMap(Map&lt;String, Integer&gt; integerMap) { this.integerMap = integerMap; } @Override public String toString() { return \"MyBean5{\" + \"integerMap=\" + integerMap + '}'; }} 将多个泛型的实例注入到集合 (1)将多个泛型的实例注入到List (2)控制泛型实例在List中的顺序 (3)将多个泛型的实例注入到Map 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@Configuration@ComponentScan(\"com.lhf.spring.bean\")public class BeanConfiguration { //方式一： List集合注入Bean @Bean //告知这个Bean将会交给Spring进行管理 public List&lt;String&gt; stringList(){ //List集合有序可重复 List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(\"111\"); list.add(\"222\"); list.add(\"333\"); return list; } @Bean(\"stringList1\") //告知这个Bean将会交给Spring进行管理, 指定Bean的Id public List&lt;String&gt; stringList1(){ //List集合有序可重复 List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(\"1111\"); list.add(\"2222\"); list.add(\"3333\"); return list; } //方式二：List集合注入Bean， 注意类型一定要与List集合的类型一致 @Bean @Order(100) //@Order注解指定顺序 public String string1(){ return \"444\"; } @Bean @Order(1) public String string2(){ return \"555\"; } @Bean @Order(50) public String string3(){ return \"666\"; } //----------------------------------------------------------------// //方式一：Map集合注入Bean @Bean public Map&lt;String, Integer&gt; integerMap(){ Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); map.put(\"aaa\", 1111); map.put(\"bbb\", 2222); map.put(\"ccc\", 3333); return map; } //方式二：Map集合注入Bean, 注意类型一定要与Map集合的类型一致 @Bean(\"int1\") public Integer integer1(){ return 10001; } @Bean(\"int2\") public Integer integer2(){ return 10002; }} 四、简单类型（String、Integer)直接注入Bean 12345678910111213141516171819202122232425262728293031323334@Componentpublic class MyBean6 { private String string; private Integer integer; public String getString() { return string; } @Value(\"没有了你，万杯觥筹只不是是提醒寂寞罢了\") //使用@Value注解直接注入值 public void setString(String string) { this.string = string; } public Integer getInteger() { return integer; } @Value(\"1314\") public void setInteger(Integer integer) { this.integer = integer; } @Override public String toString() { return \"MyBean6{\" + \"string='\" + string + '\\'' + \", integer=\" + integer + '}'; }} 五、SpringIoc容器内置接口注入Bean 123456789101112131415161718192021@Componentpublic class MyBean7 { private ApplicationContext context; public ApplicationContext getContext() { return context; } @Autowired public void setContext(ApplicationContext context) { this.context = context; } @Override public String toString() { return \"Mybean7{\" + \"context=\" + context + '}'; }} 在这里，除了可以直接将ApplicationContext注入进来之外，还可以注入BeanFactory、Environment、ResourceLoader、ApplicationEventPublisher、MessageSource及其实现类。","link":"/2019/12/12/多种方式实现Spring的Bean注入/"},{"title":"","text":"可视化面板 [TOC] 需求描述统计模板化框架，现有的统计项，可以模板化显示。 需求分析图表，用户可以根据想看的数据配置成一个一个的图表，作为库存在 面板，用户可以按照功能等自主配置添加已有的图表，并按照自主配置的布局组成一个个功能面板 在想要可视化展现的地方选择展示的面板进行图表的展示，例如可以在首页添加上访问量统计面板等 实现方案一、在日志中心增加可视化菜单，用于配置面板和图表 二、图表管理提供增加和删除图表功能，并配置具体图表内容 三、面板管理提供新增删除面板功能，并配置包含哪些图表（包括图表排版） 四、面板内容配置 新增图表 删除图表 保存 五、图表内容配置 接口数据库","link":"/2020/05/30/可视化面板/"},{"title":"微服务开发指南","text":"微服务架构优势-[] 易于开发维护-[] 单个微服务启动较快-[] 局部修改容易部署-[] 技术栈不受限制-[] 按需伸缩 面临的挑战-[] 运维成本较高-[] 分布式固有的复杂性-[] 按接口调整成本高-[] 重复劳动 设计原则 gradle 和 maven 项目可以互相转换maven转gradlegradle init –type pom 使用微服务构建的是分布式系统，微服务之间通过网络进行通信我们使用服务提供者何服务消费者来描述","link":"/2019/09/02/微服务开发指南/"},{"title":"抖音分享商品链接","text":"抖音如何添加商品链接怎么拍摄视频速度越来越多的淘宝店家开始使用抖音引流，所以都很期待抖音加淘宝商品链接功能，其实也就是指“商品分享功能”，当然这个需要开通抖音购物车权限。 什么是抖音商品分享功能。 抖音商品分享功能是指大家在自己的抖音视频和抖音主页里添加商品链接的功能，开通此功能后，抖音的主抖音培训页会增加“商品橱窗”入口，你可以直接在橱窗里添加要分享的商品链接。 如果你在发布视频的时候添加了分享的商品链接，那么在视频的左则和视频评论区的顶部都会有“购物车”标识，对你分享的商品感兴趣的用户可以通过“购物车”标识来了解商品的详情并购买。 如何开通抖音商抖音教学品分享推广功能。 抖音商品分享推广功能可以说是开启了抖音达人的电商变现之路，抖音每次推出新功能，都会有很多申请条件，目前抖音开通淘宝链接的审核力度及其低，只要你满足粉丝量大于0名，发布视频大于10个，通过实名认证这三个条件就可以立即申请并拥有个人主页电商橱窗抖音上热门啦。 抖音如何添加商品链接。 1，大家在开通“商品分享功能”之后，点击进入个人主页“我的电商橱窗”。 2，接着点击页面右上角的“电商工具箱”中的“商品橱窗管理”功能区。 3，点击页面右上角“添加商品”中的“淘宝商品”并且粘贴店铺商品淘口令，抖音代运营淘宝商品链接就算是在抖音添加好了。 抖音短视频怎么设置拍摄视频速度。 1.打开抖音，查看下面导航栏目中间的那个加号，进入里面。 2.选择界面的一首歌，随便选择一首即可，每首都可以设置快的视频，选择点击开拍。 3.进入开拍的初始界面有五个选项，非别是极慢，慢，标准，抖音教程快，极快，初始是标准，我们选择快的那个选项。 4.设置好了之后我们就开始点击下面的粉色按钮进行拍摄。 5.拍摄好了之后我们可以预览一下拍摄之后的效果怎样，效果好的我们就可以直接选择发送。 6.效果可以的，我们在将封面设置一下就可以发布了，快视频就这样设置好了。��置一下就可以发布了，快视频就这样设置好了。","link":"/2019/10/21/抖音分享商品链接/"},{"title":"拖动改变Div大小","text":"拖动改变Div大小 转自 blog : http://wuxinxi007.cnblogs.com/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" /&gt; &lt;title&gt;jQuery 版“元素拖拽改变大小”原型 &lt;/title&gt; &lt;script src=\"./jquery-1.11.2.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt; $(function() { //绑定需要拖拽改变大小的元素对象 bindResize(document.getElementById('test')); }); function bindResize(el) { //初始化参数 var els = el.style, //鼠标的 X 和 Y 轴坐标 x = y = 0; //邪恶的食指 $(el).mousedown(function(e) { //按下元素后，计算当前鼠标与对象计算后的坐标 x = e.clientX - el.offsetWidth, y = e.clientY - el.offsetHeight; //在支持 setCapture 做些东东 el.setCapture ? ( //捕捉焦点 el.setCapture(), //设置事件 el.onmousemove = function(ev) { mouseMove(ev || event) }, el.onmouseup = mouseUp ) : ( //绑定事件 $(document).bind(\"mousemove\", mouseMove).bind(\"mouseup\", mouseUp) ) //防止默认事件发生 e.preventDefault() }); //移动事件 function mouseMove(e) { //宇宙超级无敌运算中... els.width = e.clientX - x + 'px', els.height = e.clientY - y + 'px' } //停止事件 function mouseUp() { //在支持 releaseCapture 做些东东 el.releaseCapture ? ( //释放焦点 el.releaseCapture(), //移除事件 el.onmousemove = el.onmouseup = null ) : ( //卸载事件 $(document).unbind(\"mousemove\", mouseMove).unbind(\"mouseup\", mouseUp) ) } } &lt;/script&gt; &lt;style type=\"text/css\"&gt; #test { position: absolute; top: 0; left: 0; width: 400px; height: 300px; background: #f1f1f1; text-align: center; line-height: 100px; border: 1px solid #CCC; cursor: se-resize; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"test\"&gt; 这是内容&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;","link":"/2019/11/15/拖动改变Div大小/"},{"title":"数据库表设计","text":"对于数据库表的设计，一般会有一对一，一对多，多对多，自关联四种情况。一对一对象的表设计:做一个身份证管理系统，里面的人和身份证就是一对一的关系，主从关系，人拥有身份证，身份证属于人。只需要给人和身份证分别设计一张表，后再加关系，idcard身份证的id列设为外键约束 设计人和身份证的表： 12345678910111213create table person( id int primary key, name varchar(40));create table idcard( id int primary key, city varchar(40), constraint id_FK foreign key(id) references person(id) ); 一对多或者多对一对象的表设计:假设做一个部门管理系统，从面向对象的角度考虑，要设计2个对象。department代表部门，员工对象employee。不管对象引用关系，只管基本属性，根据基本属性建表。在多的一方加外键列描述数据之间的关系。 123456789101112131415create table department( id int primary key, name varchar(40));create table employee( id int primary key, name varchar(40), salary decimal(8,2), department_id int, constraint department_id_FK foreign key(department_id) references department(id)); 多对多对象的表设计:一个老师可以有多个学生，一个学生可以有多个老师，同样地，只需要把老师teacher和学生student的相关属性用表描述出来，这个时候数据间的关系就要设计中间表teacher_student。两列作为联合主键。加上外键约束。 123456789101112131415161718192021create table teacher( id int primary key, name varchar(40), salary decimal(8,2));create table student( id int primary key, name varchar(40));create table teacher_student( teacher_id int, student_id int, primary key(teacher_id,student_id), constraint teacher_id_FK foreign key(teacher_id) references teacher(id), constraint student_id_FK foreign key(student_id) references student(id) ); 还有一种设计方案，有的人会觉得设计3张表到时候查询会很麻烦，就设计成一张表，只是设计成一张表会造成数据的冗余。自关联对象的表设计: 一个家庭里有多个人，家族成员之间的关系是自关联的。 自连接的表 1234567create table person( id int primary key, name varchar(40), parent_id int, constraint parent_id_FK foreign key(parent_id) references person(id));","link":"/2019/09/02/数据库表设计/"},{"title":"反射","text":"通过Java反射调用方法 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.lang.reflect.Method;import java.lang.reflect.InvocationTargetException;/*** Created by IntelliJ IDEA.* File: TestRef.java* User: leizhimin* Date: 2008-1-28 14:48:44*/public class TestRef { public staticvoid main(String args[]) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException { Foo foo = new Foo(\"这个一个Foo对象！\"); Class clazz = foo.getClass(); Method m1 = clazz.getDeclaredMethod(\"outInfo\"); Method m2 = clazz.getDeclaredMethod(\"setMsg\", String.class); Method m3 = clazz.getDeclaredMethod(\"getMsg\"); m1.invoke(foo); m2.invoke(foo, \"重新设置msg信息！\"); String msg = (String) m3.invoke(foo); System.out.println(msg); }}class Foo { private String msg; public Foo(String msg) { this.msg = msg; } public void setMsg(String msg) { this.msg = msg; } public String getMsg() { return msg; } public void outInfo() { System.out.println(\"这是测试Java反射的测试类\"); }} JAVA反射使用手记 本篇文章为在工作中使用JAVA反射的经验总结，也可以说是一些小技巧，以后学会新的小技巧，会不断更新。本文不准备讨论JAVA反射的机制，网上有很多，大家随便google一下就可以了。 在开始之前，我先定义一个测试类Student，代码如下：12345678910111213141516171819202122package chb.test.reflect; public class Student { private int age; private String name; public int getAge() { return age; } public void setAge(int age) { this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public static void hi(int age,String name){ System.out.println(\"大家好，我叫\"+name+\"，今年\"+age+\"岁\"); } } 一、JAVA反射的常规使用步骤 反射调用一般分为3个步骤： 得到要调用类的class得到要调用的类中的方法(Method)方法调用(invoke) 123Class cls = Class.forName(\"chb.test.reflect.Student\"); Method m = cls.getDeclaredMethod(\"hi\",new Class[]{int.class,String.class}); m.invoke(cls.newInstance(),20,\"chb\"); 二、方法调用中的参数类型 在方法调用中，参数类型必须正确，这里需要注意的是不能使用包装类替换基本类型，比如不能使用Integer.class代替int.class。如我要调用Student的setAge方法，下面的调用是正确的： 123Class cls = Class.forName(\"chb.test.reflect.Student\"); Method setMethod = cls.getDeclaredMethod(\"setAge\",int.class); setMethod.invoke(cls.newInstance(), 15); 而如果我们用Integer.class替代int.class就会出错，如： 123Class cls = Class.forName(\"chb.test.reflect.Student\"); Method setMethod = cls.getDeclaredMethod(\"setAge\",Integer.class); setMethod.invoke(cls.newInstance(), 15); jvm会报出如下异常： 123java.lang.NoSuchMethodException: chb.test.reflect.Student.setAge(java.lang.Integer) at java.lang.Class.getDeclaredMethod(Unknown Source) at chb.test.reflect.TestClass.testReflect(TestClass.java:23) 三、static方法的反射调用 static方法调用时，不必得到对象示例，如下： 1234Class cls = Class.forName(\"chb.test.reflect.Student\"); Method staticMethod = cls.getDeclaredMethod(\"hi\",int.class,String.class); staticMethod.invoke(cls,20,\"chb\");//这里不需要newInstance //staticMethod.invoke(cls.newInstance(),20,\"chb\"); 四、private的成员变量赋值 如果直接通过反射给类的private成员变量赋值，是不允许的，这时我们可以通过setAccessible方法解决。代码示例： 12345Class cls = Class.forName(\"chb.test.reflect.Student\"); Object student = cls.newInstance();//得到一个实例 Field field = cls.getDeclaredField(\"age\"); field.set(student, 10); System.out.println(field.get(student)); 运行如上代码，系统会报出如下异常： 123456java.lang.IllegalAccessException: Class chb.test.reflect.TestClass can not access a member of class chb.test.reflect.Student with modifiers \"private\" at sun.reflect.Reflection.ensureMemberAccess(Unknown Source) at java.lang.reflect.Field.doSecurityCheck(Unknown Source) at java.lang.reflect.Field.getFieldAccessor(Unknown Source) at java.lang.reflect.Field.set(Unknown Source) at chb.test.reflect.TestClass.testReflect(TestClass.java:20) 解决方法： 123456Class cls = Class.forName(\"chb.test.reflect.Student\"); Object student = cls.newInstance(); Field field = cls.getDeclaredField(\"age\"); field.setAccessible(true);//设置允许访问 field.set(student, 10); System.out.println(field.get(student)); 其实，在某些场合下(类中有get,set方法)，可以先反射调用set方法，再反射调用get方法达到如上效果，代码示例： 12345678Class cls = Class.forName(\"chb.test.reflect.Student\"); Object student = cls.newInstance(); Method setMethod = cls.getDeclaredMethod(\"setAge\",Integer.class); setMethod.invoke(student, 15);//调用set方法 Method getMethod = cls.getDeclaredMethod(\"getAge\"); System.out.println(getMethod.invoke(student));","link":"/2019/09/13/反射/"},{"title":"更换yum源rpmbuild包制作","text":"更换yum源163yum源：1）备份当前yum源防止出现意外还可以还原回来 cd /etc/yum.repos.d/cp /CentOS-Base.repo /CentOS-Base-repo.bak2）使用wget下载163yum源repo文件 wget http://mirrors.163.com/.help/CentOS7-Base-163.repo3) 清理旧包 yum clean all4）把下载下来163repo文件设置成为默认源 mv CentOS7-Base-163.repo CentOS-Base.repo5）生成163yum源缓存并更新yum源 yum makecacheyum update阿里云yum源：1）备份当前yum源防止出现意外还可以还原回来 cd /etc/yum.repos.d/cp /CentOS-Base.repo /CentOS-Base-repo.bak2）使用wget下载阿里yum源repo文件 wget http://mirrors.aliyun.com/repo/Centos-7.repo3) 清理旧包 yum clean all4）把下载下来阿里云repo文件设置成为默认源 mv Centos-7.repo CentOS-Base.repo5）生成阿里云yum源缓存并更新yum源 yum makecacheyum update 1,查找rpm-build，并安装 1）yum 安装 yum list |grep rpm-build 查找合适的rpm-build包yum install -y rpm-build.x86_64 2）非yum 安装如果没有yum源，可以先将rpm-build.rpm 下载到本地，下载rpm-build的时候，需要安装和操作系统版本一致的。否则会提示错误。比如我的系统如下： Linux sjs_78_213 2.6.32-220.17.1.el6.x86_64 #1 SMP Thu Apr 26 13:37:13 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux 对应的rpm包是 ：rpm-build-4.8.0-19.el6_2.1.x86_64.rpm 。查找rpm包可以到 http://rpm.pbone.net/ 下载rpm包 ：wget ftp://ftp.pbone.net/mirror/ftp.scientificlinux.org/linux/scientific/6.0/x86_64/updates/security/rpm-build-4.8.0-19.el6_2.1.x86_64.rpm 安装 rpm -ivh rpm-build-4.8.0-19.el6_2.1.x86_64.rpm 2,创建一个普通用户，以普通用户打包最好以普通用户打包，否则会有一些稀奇古怪的问题。adduser wangsu - wangmkdir -p /home/wang/rpmbuild/{BUILD,RPMS,SOURCES,SPECS,SRPMS} echo “%_topdir /home/wang/rpmbuild” &gt;~/.rpmmacros rpmbuild –showrc|grep _topdir cd /home/wang/rpmbuild/SPECSrpmbuild -ba dteworker-client.spec 一个完整的rpmbuild目录可以下载http://download.csdn.net/detail/wisgood/8384763，然后解压,打包即可。 rpmbuild用法利用rpmbuild打包，需要两类文件：1、源码，源码以tar归档进行压缩的源码包，以及一些.patch文件，存放于目录./SOURCES下；2、.spec文件，定义了打包的动作，以及依赖，是打包的最主要类容。 首先介绍SPEC文件： SPEC文件的一些语法： .spec中的条件判断语句有两种：1、if结构引用%if %{str}%else 动作%endif其中%{str}是条件，0为假，非0为真。2、?:结构引用%{?变量:动作1}动作2其中{}用于控制范围，而“？”号和“：”号是分割符，如果要判断条件是非的情况，可以在“？”号前加“！”号。此条件与前面的%if有点不同，其只判断变量是否定义，定义了就为真，否则就为假，即使变量定义为0，也为真，并运行后面的语句。 spec文件的一些定义： Name: #软件包的名称 Version: #软件包的版本号 Release: #发布的序列 Epoch: #发布的序列 Summary: #摘要 Group: #组描述 License: 发行许可证 Sources[0-n]: #打包的源码包 Patch0: *.patch #补丁文件 BuildRequires: #打包时依赖的软件 Requires: #安装此rpm包时依赖的软件包 BuildRoot: #安装此软件的虚拟根目录 以上是描述性的元素，其中Epoch:Version:Release表示了rpm包的新旧，优先级依次降低，打出的rpm包也是以${package}-${Version}-${Release}命名。 spec文件主体内容： spec文件中引用的一些宏变量主要定义在/usr/lib/rpm/macros中 主要有三个阶段： %pre #预处理阶段，解压缩软件包 %setup %setup 不加任何选项，仅将软件包打开。%setup -n newdir 将软件包解压在newdir目录。%setup -c 解压缩之前先产生目录。%setup -b num 将第num个source文件解压缩。%setup -T 不使用default的解压缩操作。%setup -T -b 0 将第0个源代码文件解压缩。%setup -c -n newdir 指定目录名称newdir，并在此目录产生rpm套件。%patch 最简单的补丁方式，自动指定patch level。%patch0 -p0 打第1个补丁，利用当前相对路径名称%pacth1 -p2 打第2个补丁，忽略补丁文件第一层目录%patch 0 使用第0个补丁文件，相当于%patch ?p 0。%patch -s 不显示打补丁时的信息。%patch -T 将所有打补丁时产生的输出文件删除。 %build 编译阶段 ./configure –prefix=$RPM_BUILD_ROOT/usrmakeor%configure #可以用rpm –eval ‘%configure’命令查看该宏make 在openstack项目中直接是:python setup.py build %install 将软件安装到虚拟根目录 常用命令： make DESTDIR=$RPM_BUILD_ROOT install install [options] src ${RPM_BUILD_ROOT}/${dst} #安装配置文件至指定目录,相当于cp 建立连接关系等。 在openstack 项目中： %{_python2} setup.py install -01 –skip-build –root %{buildroot} %clean 清理一些临时文件，或是生产中不需要的文件 %files [name] 文件和目录的归档，rpm包真正包含的内容，$name 与package name对应，一个package生成一个rpm包,名字${name}-￥{version}-${release}.rpm。若没有name，则即是spec Name项。 files是相对路径，应用宏或变量表示相对路径： 如果描述为目录，表示目录中除%exclude外的所有文件。%defattr (-,root,root) 指定包装文件的属性，分别是(mode,owner,group)，-表示默认值，对文本文件是0644，可执行文件是0755 %changelog 变更日志 一般会把git log记录输入，openstack文件中记录的日志： git rev-parse –abbrev-ref HEAD &gt;&gt; *.spec git log –pretty=oneline –abbrev-commit | head -n +1 &gt;&gt; *.spec #生成patch的命令 diff -Naur path/to/A_Project path/to/B_Project &gt; Project.patch （A是原始项目） 或者利用 git命令: #new 是有更改的分支，old是没有更改的分支git checkout newgit format-patch -M old 生成：000-*.patchgit打patch的命令： git am 000-*.patch #解析rpm包 rpm -qpl *.rpm #列出rpm包包含的内容 rpm2cpio *.rpm | cpio -div #解压缩出rpm包 yum-duilddep *.spec 安装spec文件中的所有编译依赖软件，BuidRequires。 rpmbuild –define “_topdir ${dir:-/home/rpmbuild}” -bb *.spec _topdir指定打包的目录，rpmbuild/{SURCES,BUILD,BUILDROOT,SPECS,RPMS,SRPMS}。 也可以向spec文件传入参数，也是利用–define","link":"/2019/12/27/更换yum源/"},{"title":"查询数据库转map","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.ResultSetMetaData; import java.sql.SQLException; import java.sql.Statement; import java.util.ArrayList; import java.util.HashMap; public class DBHelper { public static void main(String[] args) throws ClassNotFoundException, SQLException { Class.forName(\"oracle.jdbc.driver.OracleDriver\"); String url = \"jdbc:oracle:thin:@localhost:1521:orcl\"; String user = \"ssmy\"; String password = \"ssmy\"; Connection conn = DriverManager.getConnection(url, user, password); Statement stmt = conn.createStatement(); /*//造数据 * for(char letter='a';letter&lt;='z';letter++){ int id = letter-97; String * name = \"\"; String sex = (id&amp;1)!=0?\"男\":\"女\"; String state = \"Y\"; * * String sql = * \"insert into person (id,name, sex,state) values(\"+id+\",\"+ * name+\",\"+sex+\",\"+state+\")\"; ps.execute(sql); } */ ResultSet rs = stmt.executeQuery(\"select t.* from SSMY_SYS_USER t\"); ResultSetMetaData data = rs.getMetaData(); ArrayList&lt;HashMap&lt;String, String&gt;&gt; al = new ArrayList&lt;HashMap&lt;String, String&gt;&gt;(); while (rs.next()) { HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); for (int i = 1; i &lt;= data.getColumnCount(); i++) {// 数据库里从 1 开始 String c = data.getColumnName(i); String v = rs.getString(c); System.out.println(c + \":\" + v + \"\\t\"); map.put(c, v); } System.out.println(\"======================\"); al.add(map); } System.out.println(al); rs.close(); stmt.close(); conn.close(); } }","link":"/2019/09/06/查询数据库转map/"},{"title":"流程图和时序图","text":"12345678st=&gt;start: 开始op=&gt;operation: logincond=&gt;condition: login result yes/no?e=&gt;end: enterst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op 1234Alice -&gt; Bob: Click and drag to create a requestBob --&gt; Alice: drag to movenote over Bob,Double click to edit text: Click the ? icon for instructions and examplesBob-&gt;Double click to edit text: non-instantaneous message 12345classDiagramBaseClass &lt;|-- AveryLongClass : CoolBaseClass : size()BaseClass : int chimpBaseClass : int gorilla","link":"/2019/11/07/流程图和时序图/"},{"title":"淘宝上传视频","text":"淘宝主图视频怎么上传到淘宝视频中心，自己制作的主图视频，上传视频之前，需先上传至淘宝视频中心，待审核发布成功后，即可在宝贝的编辑页面直接添加即可，那么、淘宝主图视频是什么格式、有什么要求呢？淘宝主图视频基本支持常见的,AVI、FLV、MOV、ASF、NAVI、3GP、WMV、等格式、、淘宝主图视频是几秒呢！目前之多支持9秒时间。下面就来简单分享、淘宝宝贝主图视频上传方法供参考。 首先在浏览器打开把W主页，输入账号进行登录，然后点击进入【卖家中心】 进入卖家中心之后，在页面的左侧导航栏再点击进入【媒体中心】 进入媒体中心页面之后，在已订购的服务这里点击进入【淘宝视频服务】 进入淘宝视频中心之后，直接点击【自己上传】 此时我们在弹出的窗口中，在电脑找到已制作好的9秒视频，双击打开。淘宝主图视频基本支持常见的,AVI、FLV、MOV、ASF、NAVI、3GP、WMV、格式等、、 上传过程中切记不要关闭当前网页，当然您也可以再操作上传另一个视频。上传速度依个人网速而定。 视频上传完成之后，我们可以先填写【视频标题】选择【视频封面】视频封面可以单独上一张图片，或者选择视频中的一个画面作为封面。 接下来再填写相应的【标签】选择视频【分组】分组这里自己搜懂添加名称。完了直接点击【保存并发布】即可。 好啦上传成功之后、、、我们再进入我发布的视频查看，因为上传之后需要再转码与审核，这需要耐心等一等。如果状态这里显示为“发布中”说明还没有成功发布。 等到态这里显示为“ 发布成功 ”之后，视频就算是真正的发布成功了，并且可以应用到宝贝的主图视频中了。完了之后我们返回宝贝编辑页面，进行添加视频即可。","link":"/2019/10/21/淘宝上传视频/"},{"title":"","text":"特权用户访问策略控制[TOC] 需求描述1）配置管理员绑定IP功能，只允许特定IP地址的管理客户端访问安枢2）安全策略增加是否增加启用策略 需求分析1.特权用户访问策略默认开启2.配置管理员绑定IP功能 :#### 2.1在管理员列表里增加配置按钮​ 配置表里实现可配置可更改 #### 2.2绑定类型​ 每个用户可绑定三个ip,且只能有一种绑定类型 ​ 不绑定：不限制 ​ 单向绑定: 用户只允许绑定的ip登陆管理端，别的账户也可使用该ip ​ 双向绑定: 用户只允许绑定的ip登陆管理端，别的账户不可使用该ip登陆 3.只允许特定IP地址的管理客户端访问安枢：​ 绑定了IP的管理员需要登录校验IP 数据库表表: tb_apps_user_bind_addr id user_id bind_addr creator create_time update_time state 表: tb_apps_user_info 增加一个字段表示绑定类型 bind_type 0 逻辑1234567891011st=&gt;start: 开始op=&gt;operation: logincond=&gt;condition: 开启特权用户绑定了么？cond2=&gt;condition: 符合ip绑定规则么？op3=&gt;operation: 允许登录e=&gt;end: 结束st-&gt;op-&gt;condcond(yes)-&gt;cond2cond(no)-&gt;op3cond2(yes)-&gt;op3cond2(no)-&gt;op 接口1.根据userId 查询用绑定的类型和绑定的IPURL: http://ip:port/api/user/addr/{userId} Method: GET 2.新增用户绑定IP的接口URL: http://ip:port/api/user/addr/configuer Method: POST 12345{ \"userId\": \"\", \"bindType\": \"\", \"bindAddrs\": []} 3.更新用户绑定IP的接口URL: http://ip:port/api/user/addr/update Method: POST 12345{ \"userId\": \"\", \"bindType\": \"\", \"bindAddrs\": []} 4.登录时根据用户id校验绑定规则接口URL: http://ip:port/api/user/addr/check/{userId} Method: GET","link":"/2020/05/30/特权用户访问策略控制/"},{"title":"网络体系机构","text":"网络的体系结构分为哪五层，每层分别有哪些协议(阿里面试题)面试题：讲一下OSI七层模型，每层的作用，问了wifi属于哪一层(百度面试题)一、OSI七层模型 应用层是最靠近用户的OSI层。这一层为用户的应用程序（例如电子邮件、文件传输和终端仿真）提供网络服务。表示层表示提供各种用于应用层数据编码和转换功能，确保一个系统的应用层发送的数据能被另一个系统的应用层识别。如果必要该层可提供一种标准表示形式，用于将计算机内部的多种数据格式转换成通信中采用的标准表示形式。数据压缩和加密也是表示层可提供的转换功能之一。会话层会话层负载建立、管理和终止表示层实体之间的通信会话。该层的通信由不同设备中的应用程序之间的服务请求和响应组成。传输层传输层建立了主机端到端的链接，传输层的作用是为上层协议提供端到端的可靠和透明的数据传输服务，包括处理差错控制和流量控制等问题。该层向高层屏蔽了下层数据通信的细节，是高层用户看到的只是在两个传输实体建的一个主机到主机的、可由用户控制和设定、可靠的数据通路。通常说的TCP UDP就是在这层。端口号即是这里的“端”。网络层本层通过IP寻址来建立两点之间的连接，为源端的运输层来的分组，选择合适的路由和交换节点，正确无误地按照地址传送给目的端的运输层。就是通常说的ip层。这一层就是我们经常说的IP协议层。IP协议是Internet的基础。数据链路层将比特组合成字节，再将字节组成帧，使用链路层地址（以太网mac地址）来访问介质，并进行差错检测。数据链路层又分为2个子层：逻辑链路控制子层（LLC）和媒体访问控制子层（MAC）。物理层主要定义物理设备标准，如网线的接口类型、光纤的接口类型、各种传输介质的传输速率等。它的主要作用是传输比特流（就是由1、0转化为电流强弱来进行传输,到达目的地后在转化为1、0，也就是我们常说的数模转换与模数转换）。这一层的数据叫做比特。二、TCP/IP五层模型 三、Wifi和zigbeezigbee和wifi符合局域网标准，是工作在OSI数据链路层和物理层的。","link":"/2019/07/21/网络体系机构/"},{"title":"类加载器读取配置文件","text":"用类加载器的5种方式读取.properties文件 用类加载器的5中形式读取.properties文件（这个.properties文件一般放在src的下面） 用类加载器进行读取：这里采取先向大家讲读取类加载器的几种方法；然后写一个例子把几种方法融进去，让大家直观感受。最后分析原理。（主要是结合所牵涉的方法的源代码的角度进行分析） 这里先介绍用类加载器读取的几种方法: 1.任意类名.class.getResourceAsStream(“/文件所在的位置”);【文件所在的位置从包名开始写】 2.和.properties文件在同一个目录下的类名.class.getResourceAsStream(“文件所在的位置”);【文件所在的位置从包名开始写，注意这里和上面的相比较少了一个斜杠/】 当然你也可以写成跟1一样的形式即：任意类名.class.getResourceAsStream(“/文件所在的位置”); 3.任意类名.class.getClassLoader().getResourceAsStream(“文件所在的位置”);【文件所在的位置从包名开始写】 4.任意类名.class.getClassLoader().getResource(“文件所在的位置”).openStream();【文件所在的位置从包名开始写】 5.任意类名.class.getClassLoader().getResource(“文件所在的位置”)..openConnection().getInputStream();【文件所在的位置从包名开始写】 //一个例子，说明上述5中方法的用法。 上面图片中的各个红色矩形就是我要读取的properties文件。主要是两类。一类直接放在src下面。另一类是放在某个文件夹下面. //f.properties文件的内容如下图所示； //上述五种情况说明的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576 1 package com.qls.commonclass; 2 3 import java.io.IOException; 4 import java.io.InputStream; 5 import java.util.Properties; 6 7 import com.qls.counter.Ok; 8 9 /**10 * 分别用类加载器的5种方法读取f.properties文件。11 * @author 秦林森12 *13 */14 public class Test6 {15 16 public static void main(String[] args) throws IOException {17 // TODO Auto-generated method stub18 /**第一种情形获取输入流。19 * 任意类名.class.getResourceAsStream(\"/文件所在的位置\");【文件所在的位置从包名开始写】20 * @param args21 */22 //获取输入流23 InputStream in = Test.class.getResourceAsStream(\"/com/qls/counter/f.properties\");24 /**25 * 第二种情形获取输入流。26 * 和.properties文件在同一个目录下的类名.class.getResourceAsStream(\"文件所在的位置\");27 * 【文件所在的位置从包名开始写，注意这里和上面的相比较少了一个斜杠/】28 * 这里随便选择一个与：f.properties在同一个目录下的类比如Ok这个类吧！29 * 这里你自然也可以写成跟第一种情况一样的形式：30 * 即：31 * InputStream in2 = Ok.class.getResourceAsStream(\"/com/qls/counter/f.properties\");32 * 因为第一种情况是针对任意一个类而言的公式。33 */34 InputStream in2 = Ok.class.getResourceAsStream(\"f.properties\");35 /**36 * 第三种情形获取输入流：37 * 任意类名.class.getClassLoader().getResourceAsStream(\"文件所在的位置\");38 * 【文件所在的位置从包名开始写】39 */40 InputStream in3 = Test2.class.getClassLoader().getResourceAsStream(\"com/qls/counter/f.properties\");41 /**42 * 第四中情形获取输入流：43 * 任意类名.class.getClassLoader().getResource(\"文件所在的位置\").openStream();44 * 【文件所在的位置从包名开始写】45 */46 InputStream in4 = Test4.class.getClassLoader().getResource(\"com/qls/counter/f.properties\").openStream();47 /**48 * 第五种情形获取输入流：49 * .任意类名.class.getClassLoader().getResource(\"文件所在的位置\").openConnection().getInputStream();50 * 【文件所在的位置从包名开始写】51 */52 InputStream in5 = Test5.class.getClassLoader().getResource(\"com/qls/counter/f.properties\").openConnection().getInputStream();53 //创建Properties54 Properties prop=new Properties();55 //把输入流in加载到prop中56 /*57 * 验证上述5中输入流是否成立。只需带入prop.load(InputStream inputStream);验证即可。58 * 也就是：59 * prop.load(in);60 * prop.load(in2);61 * prop.load(in3);62 * prop.load(in4);63 * prop.load(in5);64 */65 prop.load(in5);66 System.out.println(\"sixi=\"+prop.getProperty(\"sixi\"));67 System.out.println(\"ouyangfeng=\"+prop.getProperty(\"ouyangfeng\"));68 System.out.println(\"rape=\"+prop.getProperty(\"farm\"));69 }70 71 }/*72 Output:73 sixi=river74 ouyangfeng=masses75 farm=flower76 **///:~ 上述5中方法的原理分析。 首先看看Class中的resolveName(String name)究竟是干什么的。源码如下所示： 1234567891011121314151617181920 1 private String resolveName(String name) { 2 if (name == null) { 3 return name; 4 } 5 if (!name.startsWith(\"/\")) { 6 Class&lt;?&gt; c = this; 7 while (c.isArray()) { 8 c = c.getComponentType(); 9 }10 String baseName = c.getName();11 int index = baseName.lastIndexOf('.');12 if (index != -1) {13 name = baseName.substring(0, index).replace('.', '/')14 +\"/\"+name;15 }16 } else {17 name = name.substring(1);18 }19 return name;20 } 下面我把这个源码讲的内容翻译人类语言： 给任意一个字符串name，如果该name是以/开始的，则该函数返回的是：去掉/这个字符的字符串。（如name=”/ouyangfeng” 则调用该函数之后得到的结果是：name=ouyangfeng）。如果该name这个字符串不是以/开始的，则该函数返回的结果是调用这个函数类所在的包名+name组成的字符串（例如假设Test5所在的包名是：com.qls.mount 。则：Test5.class.resolveName(“ouyangfeng”);返回结果是：com/qls/mount/ouyangfeng） 也就是帮助文档讲述的： If the name begins with a ‘/‘ (‘\\u002f’), then the absolute name of the resource is the portion of the name following the ‘/‘.Otherwise, the absolute name is of the following form:modified_package_name/nameWhere the modified_package_name is the package name of this object with ‘/‘ substituted for ‘.’ (‘\\u002e’). 上述英文我简要翻译一下：如果name是以一个/开头，则这个资源的绝对name就是：name中/之后的部分内容。否则：这这个绝对name就是如下形式。包名/name,把这里包名中的.用/代替掉。【如：com.qls.river把.用/代替掉就是：com/qls/river】然后我们再看看Class类中的getResourceAsStream(String name)的源码和ClassLoader中的getResourceAsStream(String name)中的源码。 Class类中的getResourceAsStream(String name)的源码如下： 1234567891 public InputStream getResourceAsStream(String name) {2 name = resolveName(name);//注意这里有一个resolveName(String name)方法，根据上述的分析，易知道这个源码的意思.3 ClassLoader cl = getClassLoader0();4 if (cl==null) {5 // A system class.6 return ClassLoader.getSystemResourceAsStream(name);7 }8 return cl.getResourceAsStream(name);9 } ClassLoader中的getResourceAsStream(String name)中的源码如下： 123456781 public InputStream getResourceAsStream(String name) {2 URL url = getResource(name);3 try {4 return url != null ? url.openStream() : null;//这句代码的意思是：如果url不是null时返回的是：url.openStream()，反之如果url为null则返回null.5 } catch (IOException e) {6 return null;7 }8 } 通过这Class.resolveName(String name)中的源代码和ClassLoader.getResourceAsStream(String name)中的源代码以及Class.getResourceAsStream（String name）中的原代码我们易知道上述五种情况是怎么来的。无需记忆。 只需学会数学推理即可。 大家在看源码是：发现Class.resolveName(String name)这个方法是private的，你用普通方法根本调用不了，下面我顺便提一下：如何调用这个方法。以便大家可以更好的理解这个方法所讲的意思。 1234567891011121314151617181920212223242526272829303132333435363738394041 1 package com.qls.commonclass; 2 3 import java.lang.reflect.InvocationTargetException; 4 import java.lang.reflect.Method; 5 6 /** 7 * 用反射调用ClassLoade中的 private String resolveName(String name) 8 * 验证这个方法所讲的意思。 9 * @author 秦林森10 *11 */12 public class Test7 {13 14 public static void main(String[] args) throws Exception{15 // TODO Auto-generated method stub16 //得到这个方法。17 Method method = Class.class.getDeclaredMethod(\"resolveName\", new Class[]{String.class});18 //由于这个方法是private,所以要获取这个方法的访问权限19 method.setAccessible(true);20 //写一个实例，以便调用这个方法。21 22 Object obj = Test7.class;//Test这个类所在的包是:com.qls.commonclass23 //调用这个方法24 String invoke = (String) method.invoke(obj, new Object[]{\"ouyangfeng\"});25 String invoke2 = (String) method.invoke(obj, new Object[]{\"/ouyangfeng\"});26 System.out.println(\"invoke=\"+invoke);27 System.out.println(\"invoke2=\"+invoke2);28 29 }30 31 }/*32 Output:33 invoke=com/qls/commonclass/ouyangfeng34 invoke2=ouyangfeng35 由此可以证明了:以/开头的字符串\"/ouyangfeng\"调用resolveName(String name)36 这个方法之后返回的结果是：ouyangfeng37 不以/开头的字符串：\"ouyangfeng\"返回的结果是：调用这个方法的包名/name38 【在本例中是Test7调用resolveName(String name),而Test7所在的包是：com.qls.commonclass39 所以返回结果是：com/qls/commonclass/ouyangfeng】40 41 *///:~","link":"/2019/07/15/类加载器读取配置文件/"},{"title":"MAVEN解决打包","text":"解决：target\\surefire-reports for the individual test results 错误： [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.10:test (default-test) on project web_nanchang: There are test failures. [ERROR] [ERROR] Please refer to E:\\maven\\web_nanchang\\target\\surefire-reports for the individual test results. 解决方法： 这是因为测试代码时遇到错误，它会停止编译。只需要在pom.xml的里添加以下配置，使得测试出错不影响项目的编译。 1234567891011&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;testFailureIgnore&gt;true&lt;/testFailureIgnore&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;","link":"/2019/09/10/解决打包/"},{"title":"设计模式","text":"https://www.runoob.com/design-pattern/observer-pattern.html [TOC] ##观察者模式 我的理解有个抽象类 抽象类中定义了一个通用的抽象更新方法 ，通用的对象作为子类的构造方法的入参，入参对象类 对象定义了一个或多个共用的变量 观察者对象集合用于存储所有观察者的子类，定义了一个添加子类对象到集合的方法，提供了通知方法更新所有观察者子类中的变量各种继承了观察者的抽象类 构造方法的入参是父类中定义的入参对象 构造方法里赋值这个对象等于入参对象同时调用入参对象的加入方法把自己（this）加入观察者的集合中 并且要重写父类中的更新方法 这样定义一个入参对象 将入参作为各个观察者的构造方法 构造出各个观察者 更改入参对象中的变量值 观察者模式当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知它的依赖对象。观察者模式属于行为型模式。 介绍意图：定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。 主要解决：一个对象状态改变给其他对象通知的问题，而且要考虑到易用和低耦合，保证高度的协作。 何时使用：一个对象（目标对象）的状态发生改变，所有的依赖对象（观察者对象）都将得到通知，进行广播通知。 如何解决：使用面向对象技术，可以将这种依赖关系弱化。 关键代码：在抽象类里有一个 ArrayList 存放观察者们。 应用实例： 1、拍卖的时候，拍卖师观察最高标价，然后通知给其他竞价者竞价。 2、西游记里面悟空请求菩萨降服红孩儿，菩萨洒了一地水招来一个老乌龟，这个乌龟就是观察者，他观察菩萨洒水这个动作。 优点： 1、观察者和被观察者是抽象耦合的。 2、建立一套触发机制。 缺点： 1、如果一个被观察者对象有很多的直接和间接的观察者的话，将所有的观察者都通知到会花费很多时间。 2、如果在观察者和观察目标之间有循环依赖的话，观察目标会触发它们之间进行循环调用，可能导致系统崩溃。 3、观察者模式没有相应的机制让观察者知道所观察的目标对象是怎么发生变化的，而仅仅只是知道观察目标发生了变化。 使用场景： 一个抽象模型有两个方面，其中一个方面依赖于另一个方面。将这些方面封装在独立的对象中使它们可以各自独立地改变和复用。一个对象的改变将导致其他一个或多个对象也发生改变，而不知道具体有多少对象将发生改变，可以降低对象之间的耦合度。一个对象必须通知其他对象，而并不知道这些对象是谁。需要在系统中创建一个触发链，A对象的行为将影响B对象，B对象的行为将影响C对象……，可以使用观察者模式创建一种链式触发机制。注意事项： 1、JAVA 中已经有了对观察者模式的支持类。 2、避免循环引用。 3、如果顺序执行，某一观察者错误会导致系统卡壳，一般采用异步方式。 实现观察者模式使用三个类 Subject、Observer 和 Client。Subject 对象带有绑定观察者到 Client 对象和从 Client 对象解绑观察者的方法。我们创建 Subject 类、Observer 抽象类和扩展了抽象类 Observer 的实体类。 ObserverPatternDemo，我们的演示类使用 Subject 和实体类对象来演示观察者模式。 图解 代码示例步骤 1subject.java 12345678910111213141516171819202122232425262728import java.util.ArrayList;import java.util.List; public class Subject { private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; public int getState() { return state; } public void setState(int state) { this.state = state; notifyAllObservers(); } public void attach(Observer observer){ observers.add(observer); } public void notifyAllObservers(){ for (Observer observer : observers) { observer.update(); } } } 步骤 2创建 Observer 类。 1234public abstract class Observer { protected Subject subject; public abstract void update();} 步骤 3创建实体观察者类。BinaryObserver.java 12345678910111213public class BinaryObserver extends Observer{ public BinaryObserver(Subject subject){ this.subject = subject; this.subject.attach(this); } @Override public void update() { System.out.println( \"Binary String: \" + Integer.toBinaryString( subject.getState() ) ); }} OctalObserver.java 12345678910111213public class OctalObserver extends Observer{ public OctalObserver(Subject subject){ this.subject = subject; this.subject.attach(this); } @Override public void update() { System.out.println( \"Octal String: \" + Integer.toOctalString( subject.getState() ) ); }} HexaObserver.java 12345678910111213public class HexaObserver extends Observer{ public HexaObserver(Subject subject){ this.subject = subject; this.subject.attach(this); } @Override public void update() { System.out.println( \"Hex String: \" + Integer.toHexString( subject.getState() ).toUpperCase() ); }} 步骤 4使用 Subject 和实体观察者对象。 1234567891011121314public class ObserverPatternDemo { public static void main(String[] args) { Subject subject = new Subject(); new HexaObserver(subject); new OctalObserver(subject); new BinaryObserver(subject); System.out.println(\"First state change: 15\"); subject.setState(15); System.out.println(\"Second state change: 10\"); subject.setState(10); }} 步骤 5执行程序，输出结果： First state change: 15Hex String: FOctal String: 17Binary String: 1111Second state change: 10Hex String: AOctal String: 12Binary String: 1010","link":"/2019/10/24/设计模式/"},{"title":"设计模式六大原则（3）：依赖倒置原则","text":"定义：高层模块不应该依赖低层模块，二者都应该依赖其抽象；抽象不应该依赖细节；细节应该依赖抽象。问题由来：类A直接依赖类B，假如要将类A改为依赖类C，则必须通过修改类A的代码来达成。这种场景下，类A一般是高层模块，负责复杂的业务逻辑；类B和类C是低层模块，负责基本的原子操作；假如修改类A，会给程序带来不必要的风险。 解决方案：将类A修改为依赖接口I，类B和类C各自实现接口I，类A通过接口I间接与类B或者类C发生联系，则会大大降低修改类A的几率。 依赖倒置原则基于这样一个事实：相对于细节的多变性，抽象的东西要稳定的多。以抽象为基础搭建起来的架构比以细节为基础搭建起来的架构要稳定的多。在java中，抽象指的是接口或者抽象类，细节就是具体的实现类，使用接口或者抽象类的目的是制定好规范和契约，而不去涉及任何具体的操作，把展现细节的任务交给他们的实现类去完成。 依赖倒置原则的核心思想是面向接口编程，我们依旧用一个例子来说明面向接口编程比相对于面向实现编程好在什么地方。场景是这样的，母亲给孩子讲故事，只要给她一本书，她就可以照着书给孩子讲故事了。代码如下：12345678910111213141516171819class Book{ public String getContent(){ return \"很久很久以前有一个阿拉伯的故事……\"; }} class Mother{ public void narrate(Book book){ System.out.println(\"妈妈开始讲故事\"); System.out.println(book.getContent()); }} public class Client{ public static void main(String[] args){ Mother mother = new Mother(); mother.narrate(new Book()); }} 运行结果： 妈妈开始讲故事很久很久以前有一个阿拉伯的故事…… 运行良好，假如有一天，需求变成这样：不是给书而是给一份报纸，让这位母亲讲一下报纸上的故事，报纸的代码如下： 12345class Newspaper{ public String getContent(){ return \"林书豪38+7领导尼克斯击败湖人……\"; }} 这位母亲却办不到，因为她居然不会读报纸上的故事，这太荒唐了，只是将书换成报纸，居然必须要修改Mother才能读。假如以后需求换成杂志呢？换成网页呢？还要不断地修改Mother，这显然不是好的设计。原因就是Mother与Book之间的耦合性太高了，必须降低他们之间的耦合度才行。 我们引入一个抽象的接口IReader。读物，只要是带字的都属于读物： 123interface IReader{ public String getContent();} Mother类与接口IReader发生依赖关系，而Book和Newspaper都属于读物的范畴，他们各自都去实现IReader接口，这样就符合依赖倒置原则了，代码修改为： 123456789101112131415161718192021222324252627class Newspaper implements IReader { public String getContent(){ return \"林书豪17+9助尼克斯击败老鹰……\"; }}class Book implements IReader{ public String getContent(){ return \"很久很久以前有一个阿拉伯的故事……\"; }} class Mother{ public void narrate(IReader reader){ System.out.println(\"妈妈开始讲故事\"); System.out.println(reader.getContent()); }} public class Client{ public static void main(String[] args){ Mother mother = new Mother(); mother.narrate(new Book()); mother.narrate(new Newspaper()); }} 运行结果： 妈妈开始讲故事很久很久以前有一个阿拉伯的故事……妈妈开始讲故事林书豪17+9助尼克斯击败老鹰…… 这样修改后，无论以后怎样扩展Client类，都不需要再修改Mother类了。这只是一个简单的例子，实际情况中，代表高层模块的Mother类将负责完成主要的业务逻辑，一旦需要对它进行修改，引入错误的风险极大。所以遵循依赖倒置原则可以降低类之间的耦合性，提高系统的稳定性，降低修改程序造成的风险。 采用依赖倒置原则给多人并行开发带来了极大的便利，比如上例中，原本Mother类与Book类直接耦合时，Mother类必须等Book类编码完成后才可以进行编码，因为Mother类依赖于Book类。修改后的程序则可以同时开工，互不影响，因为Mother与Book类一点关系也没有。参与协作开发的人越多、项目越庞大，采用依赖导致原则的意义就越重大。现在很流行的TDD开发模式就是依赖倒置原则最成功的应用。 传递依赖关系有三种方式，以上的例子中使用的方法是接口传递，另外还有两种传递方式：构造方法传递和setter方法传递，相信用过Spring框架的，对依赖的传递方式一定不会陌生。在实际编程中，我们一般需要做到如下3点： 低层模块尽量都要有抽象类或接口，或者两者都有。变量的声明类型尽量是抽象类或接口。使用继承时遵循里氏替换原则。 依赖倒置原则的核心就是要我们面向接口编程，理解了面向接口编程，也就理解了依赖倒置。","link":"/2019/09/29/设计模式依赖倒置原则/"},{"title":"返回多个值","text":"python返回多个参数返回多个值 函数可以返回多个值吗？答案是肯定的。 比如在游戏中经常需要从一个点移动到另一个点，给出坐标、位移和角度，就可以计算出新的新的坐标： import math def move(x, y, step, angle=0): nx = x + step * math.cos(angle) ny = y - step * math.sin(angle) return nx, ny import math语句表示导入math包，并允许后续代码引用math包里的sin、cos等函数。 然后，我们就可以同时获得返回值： x, y = move(100, 100, 60, math.pi / 6)print(x, y)151.96152422706632 70.0 但其实这只是一种假象，Python函数返回的仍然是单一值： r = move(100, 100, 60, math.pi / 6)print(r)(151.96152422706632, 70.0) 原来返回值是一个tuple！但是，在语法上，返回一个tuple可以省略括号，而多个变量可以同时接收一个tuple，按位置赋给对应的值，所以，Python的函数返回多值其实就是返回一个tuple，但写起来更方便。 再python的迭代语法中也可以看到这种返回tuple的语法 d = {‘a’: 1, ‘b’: 2, ‘c’: 3}for key in d:… print(key)…a cb 因为dict的存储不是按照list的方式顺序排列，所以，迭代出的结果顺序很可能不一样。 默认情况下，dict迭代的是key。如果要迭代value，可以用for value in d.values()，如果要同时迭代key和value，可以用for k, v in d.items()。","link":"/2019/06/28/返回多个值/"},{"title":"邮箱找回密码","text":"https://www.jb51.net/article/96492.htm 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183package com.soq.card.web.action;import java.sql.Timestamp;import java.util.List;import java.util.UUID;import org.hibernate.Criteria;import org.hibernate.Session;import org.hibernate.SessionFactory;import org.hibernate.criterion.Restrictions;import org.springframework.orm.hibernate3.HibernateTemplate;import com.soq.card.biz.UserHander;import com.soq.card.entity.Users;import com.soq.card.tools.DBhepler;import com.soq.card.tools.Mail;import com.soq.card.tools.Md5;import com.soq.card.web.base.BaseAction;/** * @author javen * @Email zyw205@gmail.com * */public class PassEmailAction extends BaseAction { private Users users; private UserHander userHander; private String email; private String sid; private String userName; public String sendmail() { try { HibernateTemplate ht = this.getUserHander().getUsersDAO().getHibernateTemplate(); SessionFactory factory = ht.getSessionFactory(); Session session = factory.openSession(); Criteria criteria = session.createCriteria(Users.class); criteria.add(Restrictions.eq(\"loginName\", email)); List&lt;Users&gt; list = criteria.list(); if (list.size() &gt; 0) { users=list.get(0); Mail mail = new Mail(); String secretKey = UUID.randomUUID().toString(); // 密钥 Timestamp outDate = new Timestamp(System.currentTimeMillis() + 30 * 60 * 1000);// 30分钟后过期 long date = outDate.getTime() / 1000 * 1000;// 忽略毫秒数 mySql 取出时间是忽略毫秒数的 DBhepler bhepler=new DBhepler(); String sql=\"update users set outDate=?,validataCode=? where loginName=?;\"; String str[] ={outDate+\"\",secretKey,users.getLoginName()}; bhepler.AddU(sql, str); //this.getUserHander().getUsersDAO().getHibernateTemplate().update(users); // 保存到数据库 System.out.println(\" UserName&gt;&gt;&gt;&gt; \"+users.getUserName()); String key =users.getUserName() + \"$\" + date + \"$\" + secretKey; System.out.println(\" key&gt;&gt;&gt;\"+key); String digitalSignature = Md5.md5(key);// 数字签名 String path = this.getRequest().getContextPath(); String basePath = this.getRequest().getScheme() + \"://\" + this.getRequest().getServerName() + \":\" + this.getRequest().getServerPort() + path + \"/\"; String resetPassHref = basePath + \"checkLink?sid=\" + digitalSignature +\"&amp;userName=\"+users.getUserName(); String emailContent = \"请勿回复本邮件.点击下面的链接,重设密码&lt;br/&gt;&lt;a href=\" + resetPassHref + \" target='_BLANK'&gt;\" + resetPassHref + \"&lt;/a&gt; 或者 &lt;a href=\" + resetPassHref + \" target='_BLANK'&gt;点击我重新设置密码&lt;/a&gt;\" + \"&lt;br/&gt;tips:本邮件超过30分钟,链接将会失效，需要重新申请&lt;a href=\"+basePath+\"&gt;找回密码&lt;/a&gt;\" + key + \"\\t\" + digitalSignature; mail.setTo(email); mail.setFrom(\"XX\");// 你的邮箱 mail.setHost(\"smtp.163.com\"); mail.setUsername(\"XXX@163.com\");// 用户 mail.setPassword(\"CXXX\");// 密码 mail.setSubject(\"[二维码名片]找回您的账户密码\"); mail.setContent(emailContent); if (mail.sendMail()) { System.out.println(\" 发送成功\"); this.getRequest().setAttribute(\"mesg\", \"重置密码邮件已经发送，请登陆邮箱进行重置！\"); return \"sendMail\"; } } else { this.getRequest().setAttribute(\"mesg\", \"用户名不存在,你不会忘记邮箱了吧?\"); return \"noUser\"; } } catch (Exception e) { // TODO: handle exception e.printStackTrace(); } return null; } public String checkResetLink() { System.out.println(\"sid&gt;&gt;&gt;\" + sid); if (sid.equals(\"\") || userName.equals(\"\")) { this.getRequest().setAttribute(\"mesg\", \"链接不完整,请重新生成\"); System.out.println(\"&gt;&gt;&gt;&gt;&gt; null\"); return \"error\"; } HibernateTemplate ht = this.getUserHander().getUsersDAO().getHibernateTemplate(); SessionFactory factory = ht.getSessionFactory(); Session session = factory.openSession(); Criteria criteria = session.createCriteria(Users.class); criteria.add(Restrictions.eq(\"userName\", userName)); List&lt;Users&gt; list = criteria.list(); if (list.size()&gt;0) { users=list.get(0); Timestamp outDate = (Timestamp) users.getOutDate(); System.out.println(\"outDate&gt;&gt;&gt;\"+outDate); if(outDate.getTime() &lt;= System.currentTimeMillis()){ //表示已经过期 this.getRequest().setAttribute(\"mesg\", \"链接已经过期,请重新申请找回密码.\"); System.out.println(\"时间 超时\"); return \"error\"; } String key = users.getUserName()+\"$\"+outDate.getTime()/1000*1000+\"$\"+users.getValidataCode();//数字签名 System.out.println(\"key link》》\"+key); String digitalSignature = Md5.md5(key);// 数字签名 System.out.println(\"digitalSignature&gt;&gt;&gt;&gt;\"+digitalSignature); if(!digitalSignature.equals(sid)) { this.getRequest().setAttribute(\"mesg\", \"链接不正确,是否已经过期了?重新申请吧.\"); System.out.println(\"标示不正确\"); return \"error\"; }else { //链接验证通过 转到修改密码页面 this.getRequest().setAttribute(\"user\", users); return \"success\"; } }else { this.getRequest().setAttribute(\"mesg\", \"链接错误,无法找到匹配用户,请重新申请找回密码.\"); System.out.println(\"用户不存在\"); return \"error\"; } } public Users getUsers() { return users; } public void setUsers(Users users) { this.users = users; } public UserHander getUserHander() { return userHander; } public void setUserHander(UserHander userHander) { this.userHander = userHander; } public String getEmail() { return email; } public void setEmail(String email) { this.email = email; } public String getSid() { return sid; } public void setSid(String sid) { this.sid = sid; } public String getUserName() { return userName; } public void setUserName(String userName) { this.userName = userName; }}","link":"/2020/03/20/邮箱找回密码/"},{"title":"","text":"[TOC] 需求1：通过邮箱找回密码需求描述：1.实现当用户忘记密码的时候可以通过绑定的邮箱重置密码 出现场景： 主动重置：知道自己忘记了直接点击登录页的忘记密码 流程： 12345678910111213141516171819202122232425st=&gt;start: 忘记密码op=&gt;operation: 输入账号页面输入邮箱账号cond=&gt;condition: 密保邮箱存在 cond2=&gt;condition: 成功发送cond3=&gt;condition: 点击链接&amp;&amp;链接有效cond4=&gt;condition: 重置成功cond5=&gt;condition: 绑定成功op1=&gt;operation: 发送邮件op2=&gt;operation: 邮件发送成功，请至邮箱查看链接重置密码op3=&gt;operation: 跳转到重置密码页面op4=&gt;operation: 异常错误处理op5=&gt;operation: 输入信息op6=&gt;operation: 密码重置成功op7=&gt;operation: 绑定邮箱e=&gt;end: endst-&gt;op-&gt;condcond(yes)-&gt;op1cond(no)-&gt;op7op7-&gt;cond5(yes)-&gt;opcond5(no)-&gt;eop1-&gt;cond2(yes)-&gt;op2-&gt;cond3(yes)-&gt;op3-&gt;op5-&gt;cond4(yes)-&gt;op6cond2(no)-&gt;op4cond3(no)-&gt;op4cond4(no)-&gt;op4 实现方案：方案1 邮箱信息作为配置文件存到etcd，后续更改走配置中心 放开api/passwdReset/**不拦截 用户忘记密码，跳转到申请重置密码页面（需要申请重置密码的页面输入账号和邮箱） 用户填写邮箱账号，验证密保邮箱是否存在存在则跳转确认发送邮件页（需要发送邮件页） 校验账户邮箱 邮箱与账户不匹配报错 账户不存在报错 邮箱不存在则跳转绑定邮箱页（需要绑定邮箱页） 确认发送邮件 生成重置密码的链接 设置链接密钥 设置链接时效 将加密数字签名存到redis 校验请求链接的合法性 校验密钥 校验时效 校验成功跳转到更改密码页面（需要更改密码页面） 更改密码后让密钥失效清除redis中的信息 方案2 配置邮箱信息（需要一个邮箱配置页面） 配置自定义邮箱信息 测试邮箱配置 配置重置密码链接时效 放开api/passwdReset/**不拦截 用户忘记密码，跳转到申请重置密码页面（需要申请重置密码的页面输入账号和邮箱） 用户填写邮箱账号，验证密保邮箱是否存在存在则跳转确认发送邮件页（需要发送邮件页） 校验账户邮箱 邮箱与账户不匹配报错 账户不存在报错 邮箱不存在则跳转绑定邮箱页（需要绑定邮箱页） 确认发送邮件 生成重置密码的链接 ​ 设置链接密钥 ​ 设置链接时效 校验请求链接合法性 ​ 校验密钥 ​ 校验时效 校验成功跳转到更改密码页面（需要更改密码页面） 更改密码后让密钥失效保证每个url只能用一次 方案二数据库： tb_apps_user_info表中需要增加两个字段outdate、validatacode 字段 类型 outdate datetime vaildatacode varchar(36) 增加一张tb_apps_mail_conf表记录 字段 类型 protocol varchar host varchar port varchar auth varchar 需求2：通过邮箱进行二次校验需求描述：2.实现用户二次校验时可通过提供绑定的邮箱账号接收校验码 出现场景： 当用户登录错误次数达到上限触发二次校验规则时进行二次校验 流程： 123456789101112131415161718192021st=&gt;start: 二次校验op=&gt;operation: 输入账号页面输入邮箱账号cond=&gt;condition: 密保邮箱存在 cond2=&gt;condition: 成功发送邮件cond4=&gt;condition: 验证成功op1=&gt;operation: 发送验证码邮件op2=&gt;operation: 邮件发送成功，请至邮箱查看验证码op3=&gt;operation: 用户查看验证码输入验证码op4=&gt;operation: 异常错误处理op5=&gt;operation: 验证码op6=&gt;operation: 登录成功op7=&gt;operation: 登录失败e=&gt;end: endst-&gt;op-&gt;cond(yes)-&gt;op1cond(no)-&gt;opop1-&gt;cond2cond2(yes)-&gt;op2-&gt;op3-&gt;cond4(yes)-&gt;op6cond2(no)-&gt;op4cond4(no)-&gt;op7 实现方案：1.用户登录错误次数达上线触发二次验证跳转至二次验证页面（需要二次验证页面） 2.提供手机或邮箱两种验证方式两者可以同一个页面 通过输入的内容来区分 3.用户填邮箱则选择通过邮件方式接收验证码 生成验证码并设置时效存到redis 发送验证码邮件 4.用户输入验证码，校验验证码，验证码通过，则验证成功","link":"/2020/05/30/邮箱找回密码与二次认证/"}],"tags":[{"name":"IT","slug":"IT","link":"/tags/IT/"},{"name":"rabbitMq","slug":"rabbitMq","link":"/tags/rabbitMq/"},{"name":"MQ","slug":"MQ","link":"/tags/MQ/"},{"name":"SQLAlert","slug":"SQLAlert","link":"/tags/SQLAlert/"},{"name":"JAVAEE","slug":"JAVAEE","link":"/tags/JAVAEE/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"springboot","slug":"springboot","link":"/tags/springboot/"},{"name":"lombok","slug":"lombok","link":"/tags/lombok/"},{"name":"annotation","slug":"annotation","link":"/tags/annotation/"},{"name":"CI/CD","slug":"CI-CD","link":"/tags/CI-CD/"},{"name":"ONOS","slug":"ONOS","link":"/tags/ONOS/"},{"name":"mybatis","slug":"mybatis","link":"/tags/mybatis/"},{"name":"elasticsearch","slug":"elasticsearch","link":"/tags/elasticsearch/"},{"name":"interceptor","slug":"interceptor","link":"/tags/interceptor/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"MAC","slug":"MAC","link":"/tags/MAC/"},{"name":"PROTOCAL","slug":"PROTOCAL","link":"/tags/PROTOCAL/"},{"name":"mac","slug":"mac","link":"/tags/mac/"},{"name":"karaf","slug":"karaf","link":"/tags/karaf/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"OSGI","slug":"OSGI","link":"/tags/OSGI/"},{"name":"oauth2","slug":"oauth2","link":"/tags/oauth2/"},{"name":"network","slug":"network","link":"/tags/network/"},{"name":"OSI","slug":"OSI","link":"/tags/OSI/"},{"name":"yaml","slug":"yaml","link":"/tags/yaml/"},{"name":"restTemplate","slug":"restTemplate","link":"/tags/restTemplate/"},{"name":"webClient","slug":"webClient","link":"/tags/webClient/"},{"name":"login","slug":"login","link":"/tags/login/"},{"name":"文件服务器","slug":"文件服务器","link":"/tags/文件服务器/"},{"name":"logback","slug":"logback","link":"/tags/logback/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"redisTemplate","slug":"redisTemplate","link":"/tags/redisTemplate/"},{"name":"StringRedisTemplate","slug":"StringRedisTemplate","link":"/tags/StringRedisTemplate/"},{"name":"全局异常","slug":"全局异常","link":"/tags/全局异常/"},{"name":"TCP","slug":"TCP","link":"/tags/TCP/"},{"name":"UDP","slug":"UDP","link":"/tags/UDP/"},{"name":"JAVASE","slug":"JAVASE","link":"/tags/JAVASE/"},{"name":"reflection","slug":"reflection","link":"/tags/reflection/"},{"name":"bazel","slug":"bazel","link":"/tags/bazel/"},{"name":"beanutils","slug":"beanutils","link":"/tags/beanutils/"},{"name":"enlink","slug":"enlink","link":"/tags/enlink/"},{"name":"linux-shell","slug":"linux-shell","link":"/tags/linux-shell/"},{"name":"computer","slug":"computer","link":"/tags/computer/"},{"name":"debug","slug":"debug","link":"/tags/debug/"},{"name":"idea","slug":"idea","link":"/tags/idea/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"gitlab","slug":"gitlab","link":"/tags/gitlab/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"postman","slug":"postman","link":"/tags/postman/"},{"name":"ELASTICSEARCH","slug":"ELASTICSEARCH","link":"/tags/ELASTICSEARCH/"},{"name":"EXCEL","slug":"EXCEL","link":"/tags/EXCEL/"},{"name":"GO","slug":"GO","link":"/tags/GO/"},{"name":"go","slug":"go","link":"/tags/go/"},{"name":"Eureka","slug":"Eureka","link":"/tags/Eureka/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"IDEA","slug":"IDEA","link":"/tags/IDEA/"},{"name":"listener","slug":"listener","link":"/tags/listener/"},{"name":"jetty","slug":"jetty","link":"/tags/jetty/"},{"name":"servlet","slug":"servlet","link":"/tags/servlet/"},{"name":"JAVASCRIPT","slug":"JAVASCRIPT","link":"/tags/JAVASCRIPT/"},{"name":"keepalived","slug":"keepalived","link":"/tags/keepalived/"},{"name":"高可用","slug":"高可用","link":"/tags/高可用/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"mycat","slug":"mycat","link":"/tags/mycat/"},{"name":"PYTHON","slug":"PYTHON","link":"/tags/PYTHON/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"Ribbon","slug":"Ribbon","link":"/tags/Ribbon/"},{"name":"ribbon","slug":"ribbon","link":"/tags/ribbon/"},{"name":"loadbalanced","slug":"loadbalanced","link":"/tags/loadbalanced/"},{"name":"LINUX","slug":"LINUX","link":"/tags/LINUX/"},{"name":"RPM","slug":"RPM","link":"/tags/RPM/"},{"name":"serialVersionUID","slug":"serialVersionUID","link":"/tags/serialVersionUID/"},{"name":"SPRING-BOOT","slug":"SPRING-BOOT","link":"/tags/SPRING-BOOT/"},{"name":"SECURITY","slug":"SECURITY","link":"/tags/SECURITY/"},{"name":"spring task","slug":"spring-task","link":"/tags/spring-task/"},{"name":"cron","slug":"cron","link":"/tags/cron/"},{"name":"session","slug":"session","link":"/tags/session/"},{"name":"pagehelper","slug":"pagehelper","link":"/tags/pagehelper/"},{"name":"jpa","slug":"jpa","link":"/tags/jpa/"},{"name":"AOP","slug":"AOP","link":"/tags/AOP/"},{"name":"日志","slug":"日志","link":"/tags/日志/"},{"name":"Quartz","slug":"Quartz","link":"/tags/Quartz/"},{"name":"SSO","slug":"SSO","link":"/tags/SSO/"},{"name":"fastjson","slug":"fastjson","link":"/tags/fastjson/"},{"name":"json","slug":"json","link":"/tags/json/"},{"name":"registry center","slug":"registry-center","link":"/tags/registry-center/"},{"name":"druid","slug":"druid","link":"/tags/druid/"},{"name":"swagger","slug":"swagger","link":"/tags/swagger/"},{"name":"TK.MYBATIS","slug":"TK-MYBATIS","link":"/tags/TK-MYBATIS/"},{"name":"时间转化","slug":"时间转化","link":"/tags/时间转化/"},{"name":"标签1","slug":"标签1","link":"/tags/标签1/"},{"name":"消息队列","slug":"消息队列","link":"/tags/消息队列/"},{"name":"事物","slug":"事物","link":"/tags/事物/"},{"name":"分布式","slug":"分布式","link":"/tags/分布式/"},{"name":"抖音","slug":"抖音","link":"/tags/抖音/"},{"name":"centos","slug":"centos","link":"/tags/centos/"},{"name":"淘宝","slug":"淘宝","link":"/tags/淘宝/"},{"name":"classloader","slug":"classloader","link":"/tags/classloader/"},{"name":"设计模式","slug":"设计模式","link":"/tags/设计模式/"}],"categories":[{"name":"GIT","slug":"GIT","link":"/categories/GIT/"},{"name":"IT","slug":"IT","link":"/categories/IT/"},{"name":"JAVAEE","slug":"JAVAEE","link":"/categories/JAVAEE/"},{"name":"MYBATIS","slug":"JAVAEE/MYBATIS","link":"/categories/JAVAEE/MYBATIS/"},{"name":"ES","slug":"ES","link":"/categories/ES/"},{"name":"SYSTEM","slug":"SYSTEM","link":"/categories/SYSTEM/"},{"name":"office","slug":"office","link":"/categories/office/"},{"name":"MYSQL","slug":"JAVAEE/MYSQL","link":"/categories/JAVAEE/MYSQL/"},{"name":"SPRINGBOOT","slug":"SPRINGBOOT","link":"/categories/SPRINGBOOT/"},{"name":"SECURITY","slug":"SPRINGBOOT/SECURITY","link":"/categories/SPRINGBOOT/SECURITY/"},{"name":"VIM","slug":"VIM","link":"/categories/VIM/"},{"name":"分类1","slug":"分类1","link":"/categories/分类1/"},{"name":"电商","slug":"电商","link":"/categories/电商/"},{"name":"DESIGN PATTERNS","slug":"JAVAEE/DESIGN-PATTERNS","link":"/categories/JAVAEE/DESIGN-PATTERNS/"}]}